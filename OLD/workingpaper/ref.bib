@article{Cozzo2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.1759v4},
author = {Cozzo, Emanuele and Moreno, Yamir},
eprint = {arXiv:1311.1759v4},
file = {:home/melian/Downloads/1311.1759.pdf:pdf},
title = {{Dimensionality reduction and spectral properties of multilayer networks}},
year = {2018}
}
@article{Vats2015,
abstract = {Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations with respect to a target distribution. A fundamental question is when should sampling stop so that we have good estimates of the desired quantities? The key to answering this question lies in assessing the Monte Carlo error through a multivariate Markov chain central limit theorem (CLT). The multivariate nature of this Monte Carlo error largely has been ignored in the MCMC literature. We present a multivariate framework for terminating simulation in MCMC. We define a multivariate effective sample size, estimating which requires strongly consistent estimators of the covariance matrix in the Markov chain CLT; a property we show for the multivariate batch means estimator. We then provide a lower bound on the number of minimum effective samples required for a desired level of precision. This lower bound depends on the problem only in the dimension of the expectation being estimated, and not on the underlying stochastic process. This result is obtained by drawing a connection between terminating simulation via effective sample size and terminating simulation using a relative standard deviation fixed-volume sequential stopping rule; which we demonstrate is an asymptotically valid procedure. The finite sample properties of the proposed method are demonstrated in a variety of examples.},
archivePrefix = {arXiv},
arxivId = {1512.07713},
author = {Vats, Dootika and Flegal, James M. and Jones, Galin L.},
doi = {10.1016/j.jaad.2011.01.035},
eprint = {1512.07713},
file = {::},
isbn = {1512.07713},
issn = {0190-9622},
title = {{Multivariate Output Analysis for Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1512.07713},
year = {2015}
}
@article{DeDomenico:2014,
author = {{De Domenico M.} and {Porter M. A.} and Arenas, A},
journal = {Journal of Complex Networks},
pages = {159--176},
title = {{MuxViz: a tool for multilayer analysis and visualization of networks}},
volume = {3},
year = {2014}
}
@article{Valera2015,
abstract = {We propose the infinite factorial dynamic model (iFDM), a general Bayesian nonparametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.},
author = {Valera, Isabel and Ruiz, Francisco and Svensson, Lennart and Perez-Cruz, Fernando},
file = {::},
journal = {Advances in Neural Information Processing Systems 28},
pages = {1666--1674},
title = {{Infinite Factorial Dynamical Model}},
url = {http://papers.nips.cc/paper/5667-infinite-factorial-dynamical-model.pdf},
year = {2015}
}
@article{Gaoetal:2012,
author = {{Gao J.} and {Buldyrev S. V.} and {Stanley H. E.} and Havlin, S},
journal = {Nature physics},
pages = {40--48},
title = {{Networks formed from interdependent networks}},
volume = {8},
year = {2012}
}
@article{Grelaudetal:2009,
author = {{Grelaud A.} and {Robert C. P.} and {Marin J-M.} and {Rodolphe F.} and Taly, J-F.},
journal = {Bayesian Analysis},
pages = {317--336},
title = {{{\{}ABC{\}} likelihood-free methods for model choice in {\{}G{\}}ibbs random fields}},
volume = {4},
year = {2009}
}
@article{OHare2015,
abstract = {Model parameter inference has become increasingly popular in recent years in the field of computational epidemiology, especially for models with a large number of parameters. Techniques such as Approximate Bayesian Computation (ABC) or maximum/partial likelihoods are commonly used to infer parameters in phenomenological models that best describe some set of data. These techniques rely on efficient exploration of the underlying parameter space, which is difficult in high dimensions, especially if there are correlations between the parameters in the model that may not be known a priori. The aim of this article is to demonstrate the use of the recently invented Adaptive Metropolis algorithm for exploring parameter space in a practical way through the use of a simple epidemiological model.},
author = {O'Hare, Anthony},
doi = {10.1089/cmb.2015.0086},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {1999,also,antigen,bola,bola-drb3,bovine,called the bovine leucocyte,complex,genotyping,includes many immune-related genes,lewin et al,mhc,of cattle,the major histocompatibility complex},
number = {11},
pages = {997--1004},
pmid = {26176624},
title = {{Inference in High-Dimensional Parameter Space}},
url = {http://online.liebertpub.com/doi/10.1089/cmb.2015.0086},
volume = {22},
year = {2015}
}
@article{Cristian1989,
abstract = {Abstract A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock ... $\backslash$n},
author = {Cristian, Flaviu},
doi = {10.1007/BF01784024},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Clock synchronization,Communication,Distributed system,Fault-tolerance,Time service},
number = {3},
pages = {146--158},
title = {{Probabilistic clock synchronization}},
volume = {3},
year = {1989}
}
@article{Kivelaetal:2014,
author = {{Kivela M.} and {Arenas A.} and {Barthelemy M.} and {Gleeson J. P.} and {Moreno Y.} and Porter, M A},
journal = {Journal of Complex Networks},
pages = {203--271},
title = {{Multilayer networks}},
volume = {3},
year = {2014}
}
@article{Hensman2015,
abstract = {In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.1605v2},
author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
doi = {10.1109/TPAMI.2014.2318711},
eprint = {arXiv:1401.1605v2},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {383--393},
title = {{Fast nonparametric clustering of structured time-series}},
volume = {37},
year = {2015}
}
@article{Walters2011,
abstract = {In this paper we survey the literature on the Black-Litterman model. This survey is provided both as a chronology and a taxonomy as there are many claims on the model in the literature. We provide a complete description of the canonical model including full derivations from the underlying principles using both Theil's Mixed Estimation model and Bayes Theory. The various parameters of the model are considered, along with information on their computation or calibration. Further consideration is given to several of the key papers, with worked examples illustrating the concepts.},
author = {Walters, Jay},
doi = {10.2139/ssrn.1314585},
isbn = {1556-5068},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{The Black-Litterman Model in Detail}},
url = {http://www.ssrn.com/abstract=1314585},
year = {2011}
}
@article{Reichstein,
abstract = {Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning. H umans have always striven to predict and understand the world, and the ability to make better predictions has given competitive advantages in diverse contexts (such as weather, diseases or financial markets). Yet the tools for prediction have substantially changed over time, from ancient Greek philosophical reasoning to non-scientific medieval methods such as soothsaying, towards modern scientific discourse , which has come to include hypothesis testing, theory development and computer modelling underpinned by statistical and physical relationships, that is, laws 1. A success story in the geosciences is weather prediction, which has greatly improved through the integration of better theory, increased computational power, and established observational systems, which allow for the assimilation of large amounts of data into the modelling system 2. Nevertheless, we can accurately predict the evolution of the weather on a timescale of days, not months. Seasonal meteorological predictions, forecasting extreme events such as flooding or fire, and long-term climate projections are still major challenges. This is especially true for predicting dynamics in the biosphere, which is dominated by biologically mediated processes such as growth or reproduction, and is strongly controlled by seemingly stochastic disturbances such as fires and landslides. Such predictive problems have not seen much progress in the past few decades 3. At the same time, a deluge of Earth system data has become available, with storage volumes already well beyond dozens of petabytes and rapidly increasing transmission rates exceeding hundreds of terabytes per day 4. These data come from a plethora of sensors measuring states, fluxes and intensive or time/space-integrated variables, representing fifteen or more orders of temporal and spatial magnitude. They include remote sensing from a few metres to hundreds of kilometres above Earth as well as in situ observations (increasingly from autonomous sensors) at and below the surface and in the atmosphere, many of which are further being complemented by citizen science observations. Model simulation output adds to this deluge; the CMIP-5 dataset of the Climate Model Intercomparison Project, used extensively for scientific groundwork towards periodic climate assessments, is over 3 petabytes in size, and the next generation, CMIP-6, is estimated to reach up to 30 petabytes 5. The data from models share many of the challenges and statistical properties of observational data, including many forms of uncertainty. In summary, Earth system data are exemplary of all four of the 'four Vs' of 'big data': volume, velocity, variety and veracity (see Fig. 1). One key challenge is to extract interpret-able information and knowledge from this big data, possibly almost in real time and integrating between disciplines. Taken together, our ability to collect and create data far outpaces our ability to sensibly assimilate it, let alone understand it. Predictive ability in the last few decades has not increased apace with data availability. To get the most out of the explosive growth and diversity of Earth system data, we face two major tasks in the coming years: (1) extracting knowledge from the data deluge, and (2) deriving models that learn much more from data than traditional data assimilation approaches can, while still respecting our evolving understanding of nature's laws. The combination of unprecedented data sources, increased computational power, and the recent advances in statistical modelling and machine learning offer exciting new opportunities for expanding our knowledge about the Earth system from data. In particular, many tools are available from the fields of machine learning and artificial intelligence, but they need to be further developed and adapted to geo-scientific analysis. Earth system science offers new opportunities, challenges and methodological demands, in particular for recent research lines focusing on spatio-temporal context and uncertainties (Box 1; see https://developers. google.com/machine-learning/glossary/ and http://www.wildml.com/ deep-learning-glossary/ for more complete glossaries). In the following sections we review the development of machine learning in the geoscientific context, and highlight how deep learning-that is, the automatic extraction of abstract (spatio-temporal) features-has the potential to overcome many of the limitations that have, until now, hindered a more widespread adoption of machine learning. We further lay out the most promising but also challenging approaches in combining machine learning with physical modelling. State-of-the-art geoscientific machine learning Machine learning is now a successful part of several research-driven and operational geoscientific processing schemes, addressing the atmosphere, the land surface and the ocean, and has co-evolved with data availability over the past decade. Early landmarks in classification of land cover and clouds emerged almost 30 years ago through the coincidence of high-resolution satellite data and the first revival of neural networks 6,7. Most major machine learning methodological},
author = {Reichstein, Markus and Camps-Valls, Gustau and Stevens, Bjorn and Jung, Martin and Denzler, Joachim and Carvalhais, Nuno and Prabhat, {\&}},
doi = {10.1038/s41586-019-0912-1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/s41586-019-0912-1.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
publisher = {Springer US},
title = {{Deep learning and process understanding for data-driven Earth system science}},
url = {www.nature.com/nature}
}
@article{Kivela:2013,
author = {Kivela, M},
journal = {http://www.plexmath.eu/?page{\_}id=327},
title = {{Multilayer networks library}},
year = {2013}
}
@unpublished{Bailey2013,
abstract = {We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.},
author = {Bailey, David H. and Borwein, Jonathan and {Lopez de Prado}, Marcos and Zhu, Qiji Jim},
booktitle = {SSRN},
doi = {10.2139/ssrn.2308659},
issn = {0002-9920},
keywords = {E44,G0,G1,G15,G2,G24,Sharpe ratio,backtest,historical simulation,investment strategy,minimum backtest length,optimization,performance degradation,probability of backtest over-fitting},
title = {{Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance}},
year = {2013}
}
@book{MarkowitzBook,
author = {Markowitz, H},
publisher = {Blackwell Publishing, MA},
title = {{Portfolio selection}},
year = {1991}
}
@article{Beaumont:2010,
author = {Beaumont, M A},
journal = {Annual Review Ecology, Evolution and Systematics},
pages = {379--406},
title = {{Approximate {\{}B{\}}ayesian Computation in Evolution and Ecology}},
volume = {41},
year = {2010}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P a},
doi = {10.1371/journal.pmed.0020124},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16060722},
volume = {2},
year = {2005}
}
@article{Gael2018,
abstract = {We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.},
author = {Gael, Jurgen Van},
doi = {10.7551/mitpress/1120.003.0079},
file = {::},
isbn = {0-262-04208-8},
journal = {Advances in Neural Information Processing Systems 14},
pages = {1--8},
title = {{The Infinite Hidden Markov Model}},
year = {2018}
}
@article{Tarantola:2006,
author = {Tarantola, A},
journal = {Nature physics},
pages = {492--494},
title = {{Popper, {\{}B{\}}ayes and the inverse problem}},
volume = {2},
year = {2006}
}
@article{Uzzi2018,
author = {Uzzi, Brian and Vespignani, Alessandro and B{\"{o}}rner, Katy and Radicchi, Filippo and Sinatra, Roberta and Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Waltman, Ludo and Bergstrom, Carl T. and Milojevi{\'{c}}, Sta{\v{s}}a and Helbing, Dirk and Petersen, Alexander M. and Fortunato, Santo and Wang, Dashun and Evans, James A.},
doi = {10.1126/science.aao0185},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/eaao0185.full.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6379},
pages = {eaao0185},
title = {{Science of science}},
volume = {359},
year = {2018}
}
@book{Alex2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Alex, Graves},
booktitle = {Studies in computational intelligence},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
file = {::},
isbn = {9783642229091 9783642229091 3642229093 9783642229107},
issn = {18792782},
keywords = {Bayes-Netz,Bootstrap-Aggregation,Computational Intelligence,Ensembles in Machine Learning Applications,Fehlerkorrekturcode,Hardback,Klassifikator,Merkmalsextraktion,Research,Set theory,Soft Computing,Un{\"{u}}berwachtes Lernen,machine learning,{\"{U}}berwachtes Lernen},
pages = {252},
pmid = {23459267},
title = {{Supervised Sequence Labelling with RNN}},
url = {files/1074/bok{\%}3A978-3-642-24797-2.pdf},
year = {2011}
}
@article{DeDomenicoetal:2015,
author = {{De Domenico M.} and {Nicosia V.} and Latora, V},
journal = {Nature Communications},
pages = {6864},
title = {{Structural reducibility of multilayer networks}},
volume = {6},
year = {2015}
}
@article{Roberts2013,
abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
doi = {10.1098/rsta.2011.0550},
file = {::},
isbn = {1364-503X (Print) 1364-503X (Linking)},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Bayesian modelling,Gaussian processes,Time-series analysis},
number = {1984},
pages = {1--27},
pmid = {23277607},
title = {{Gaussian processes for time-series modelling}},
volume = {371},
year = {2013}
}
@article{Voelkl2018,
abstract = {Single-laboratory studies conducted under highly standardized conditions are the gold standard in preclinical animal research. Using simulations based on 440 preclinical studies across 13 different interventions in animal models of stroke, myocardial infarction, and breast cancer, we compared the accuracy of effect size estimates between single-laboratory and multi-laboratory study designs. Single-laboratory studies generally failed to predict effect size accurately, and larger sample sizes rendered effect size estimates even less accurate. By contrast, multi-laboratory designs including as few as 2 to 4 laboratories increased coverage probability by up to 42 percentage points without a need for larger sample sizes. These findings demonstrate that within-study standardization is a major cause of poor reproducibility. More representative study samples are required to improve the external validity and reproducibility of preclinical animal research and to prevent wasting animals and resources for inconclusive research.},
author = {Voelkl, Bernhard and Vogt, Lucile and Sena, Emily S. and W{\"{u}}rbel, Hanno},
doi = {10.1371/journal.pbio.2003693},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Voelkl{\_}PLoSBiol2018{\_}All.pdf:pdf},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
number = {2},
pmid = {29470495},
title = {{Reproducibility of preclinical animal research improves with heterogeneity of study samples}},
volume = {16},
year = {2018}
}
@article{Gunther2018,
abstract = {Technology evolves faster than ever, with the pace picking up with every passing year. Unprecedented in history, we have the greatest and brightest minds driving unparalleled change. Progress isn't a trend that naturally happens on its own - it requires an ever growing number of researchers and experts. The scientific community of today doesn't get what it deserves, constantly struggling with obtaining funding and work- ing endless hours with little to no reimbursement. It is our stated mission objective to empower those who empower us all by establishing Science- root, the first blockchain-based scientific ecosystem to integrate a social media scientific network, a funding platform and a decentralized publish- ing framework for journals.},
author = {G{\"{u}}nther, Vlad and Alexandru Chirita},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/whitepaperScienceRoot.pdf:pdf},
title = {{" Scienceroot " Whitepaper}},
url = {https://www.scienceroot.com/},
year = {2018}
}

@article{Voelkl2018,
abstract = {Single-laboratory studies conducted under highly standardized conditions are the gold standard in preclinical animal research. Using simulations based on 440 preclinical studies across 13 different interventions in animal models of stroke, myocardial infarction, and breast cancer, we compared the accuracy of effect size estimates between single-laboratory and multi-laboratory study designs. Single-laboratory studies generally failed to predict effect size accurately, and larger sample sizes rendered effect size estimates even less accurate. By contrast, multi-laboratory designs including as few as 2 to 4 laboratories increased coverage probability by up to 42 percentage points without a need for larger sample sizes. These findings demonstrate that within-study standardization is a major cause of poor reproducibility. More representative study samples are required to improve the external validity and reproducibility of preclinical animal research and to prevent wasting animals and resources for inconclusive research.},
author = {Voelkl, Bernhard and Vogt, Lucile and Sena, Emily S. and W{\"{u}}rbel, Hanno},
doi = {10.1371/journal.pbio.2003693},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
number = {2},
pmid = {29470495},
title = {{Reproducibility of preclinical animal research improves with heterogeneity of study samples}},
volume = {16},
year = {2018}
}
@article{OHare2015,
abstract = {Model parameter inference has become increasingly popular in recent years in the field of computational epidemiology, especially for models with a large number of parameters. Techniques such as Approximate Bayesian Computation (ABC) or maximum/partial likelihoods are commonly used to infer parameters in phenomenological models that best describe some set of data. These techniques rely on efficient exploration of the underlying parameter space, which is difficult in high dimensions, especially if there are correlations between the parameters in the model that may not be known a priori. The aim of this article is to demonstrate the use of the recently invented Adaptive Metropolis algorithm for exploring parameter space in a practical way through the use of a simple epidemiological model.},
author = {O'Hare, Anthony},
doi = {10.1089/cmb.2015.0086},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {1999,also,antigen,bola,bola-drb3,bovine,called the bovine leucocyte,complex,genotyping,includes many immune-related genes,lewin et al,mhc,of cattle,the major histocompatibility complex},
number = {11},
pages = {997--1004},
pmid = {26176624},
title = {{Inference in High-Dimensional Parameter Space}},
url = {http://online.liebertpub.com/doi/10.1089/cmb.2015.0086},
volume = {22},
year = {2015}
}

@article{Reichsteietal2019,
abstract = {Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning.},
author = {{Reichstein, M.} and {Camps-Valls, G.} and {Stevens, B.} and {Jung, M.} and {Denzler, J.} and {Carvalhais, N.} and Prabhat},
doi = {10.1038/s41586-019-0912-1},
journal = {Nature},
pages = {195--2024},
title = { Deep learning and process understanding for data-driven Earth system science},
volume = {566},
year = {2019}
}

@article{Cozzo2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.1759v4},
author = {Cozzo, Emanuele and Moreno, Yamir},
eprint = {arXiv:1311.1759v4},
file = {:home/melian/Downloads/1311.1759.pdf:pdf},
title = {{Dimensionality reduction and spectral properties of multilayer networks}},
year = {2018}
}
@book{MarkowitzBook,
author = {Markowitz, H},
publisher = {Blackwell Publishing, MA},
title = {{Portfolio selection}},
year = {1991}
}
@article{Walters2011,
abstract = {In this paper we survey the literature on the Black-Litterman model. This survey is provided both as a chronology and a taxonomy as there are many claims on the model in the literature. We provide a complete description of the canonical model including full derivations from the underlying principles using both Theil's Mixed Estimation model and Bayes Theory. The various parameters of the model are considered, along with information on their computation or calibration. Further consideration is given to several of the key papers, with worked examples illustrating the concepts.},
author = {Walters, Jay},
doi = {10.2139/ssrn.1314585},
isbn = {1556-5068},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{The Black-Litterman Model in Detail}},
url = {http://www.ssrn.com/abstract=1314585},
year = {2011}
}
@article{Gaoetal:2012,
author = {{Gao J.} and {Buldyrev S. V.} and {Stanley H. E.} and Havlin, S},
journal = {Nature physics},
pages = {40--48},
title = {{Networks formed from interdependent networks}},
volume = {8},
year = {2012}
}
@article{Tarantola:2006,
author = {Tarantola, A},
journal = {Nature physics},
pages = {492--494},
title = {{Popper, {\{}B{\}}ayes and the inverse problem}},
volume = {2},
year = {2006}
}
@article{Grelaudetal:2009,
author = {{Grelaud A.} and {Robert C. P.} and {Marin J-M.} and {Rodolphe F.} and Taly, J-F.},
journal = {Bayesian Analysis},
pages = {317--336},
title = {{{\{}ABC{\}} likelihood-free methods for model choice in {\{}G{\}}ibbs random fields}},
volume = {4},
year = {2009}
}
@article{Kivela:2013,
author = {Kivela, M},
journal = {http://www.plexmath.eu/?page{\_}id=327},
title = {{Multilayer networks library}},
year = {2013}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, J. P. A.},
doi = {10.1371/journal.pmed.0020124},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16060722},
volume = {2},
year = {2005}
}
@article{DeDomenico:2014,
author = {{De Domenico M.} and {Porter M. A.} and Arenas, A},
journal = {Journal of Complex Networks},
pages = {159--176},
title = {{MuxViz: a tool for multilayer analysis and visualization of networks}},
volume = {3},
year = {2014}
}
@unpublished{Bailey2013,
abstract = {We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.},
author = {Bailey, David H. and Borwein, Jonathan and {Lopez de Prado}, Marcos and Zhu, Qiji Jim},
booktitle = {SSRN},
doi = {10.2139/ssrn.2308659},
issn = {0002-9920},
keywords = {E44,G0,G1,G15,G2,G24,Sharpe ratio,backtest,historical simulation,investment strategy,minimum backtest length,optimization,performance degradation,probability of backtest over-fitting},
title = {{Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance}},
year = {2013}
}
@article{Kivelaetal:2014,
author = {{Kivela M.} and {Arenas A.} and {Barthelemy M.} and {Gleeson J. P.} and {Moreno Y.} and Porter, M A},
journal = {Journal of Complex Networks},
pages = {203--271},
title = {{Multilayer networks}},
volume = {3},
year = {2014}
}
@article{Cristian1989,
abstract = {Abstract A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock ... $\backslash$n},
author = {Cristian, Flaviu},
doi = {10.1007/BF01784024},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Clock synchronization,Communication,Distributed system,Fault-tolerance,Time service},
number = {3},
pages = {146--158},
title = {{Probabilistic clock synchronization}},
volume = {3},
year = {1989}
}
@article{Beaumont:2010,
author = {Beaumont, M A},
journal = {Annual Review Ecology, Evolution and Systematics},
pages = {379--406},
title = {{Approximate {\{}B{\}}ayesian Computation in Evolution and Ecology}},
volume = {41},
year = {2010}
}
@article{DeDomenicoetal:2015,
author = {{De Domenico M.} and {Nicosia V.} and Latora, V},
journal = {Nature Communications},
pages = {6864},
title = {{Structural reducibility of multilayer networks}},
volume = {6},
year = {2015}
}
@article{Vats2015,
abstract = {Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations with respect to a target distribution. A fundamental question is when should sampling stop so that we have good estimates of the desired quantities? The key to answering this question lies in assessing the Monte Carlo error through a multivariate Markov chain central limit theorem (CLT). The multivariate nature of this Monte Carlo error largely has been ignored in the MCMC literature. We present a multivariate framework for terminating simulation in MCMC. We define a multivariate effective sample size, estimating which requires strongly consistent estimators of the covariance matrix in the Markov chain CLT; a property we show for the multivariate batch means estimator. We then provide a lower bound on the number of minimum effective samples required for a desired level of precision. This lower bound depends on the problem only in the dimension of the expectation being estimated, and not on the underlying stochastic process. This result is obtained by drawing a connection between terminating simulation via effective sample size and terminating simulation using a relative standard deviation fixed-volume sequential stopping rule; which we demonstrate is an asymptotically valid procedure. The finite sample properties of the proposed method are demonstrated in a variety of examples.},
archivePrefix = {arXiv},
arxivId = {1512.07713},
author = {Vats, Dootika and Flegal, James M. and Jones, Galin L.},
doi = {10.1016/j.jaad.2011.01.035},
eprint = {1512.07713},
file = {::},
isbn = {1512.07713},
issn = {0190-9622},
title = {{Multivariate Output Analysis for Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1512.07713},
year = {2015}
}

 

