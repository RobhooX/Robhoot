<?xml version="1.0" encoding="UTF-8"?><xml><records><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Li, Tian</author><author>Zhong, Jie</author><author>Liu, Ji</author><author>Wu, Wentao</author><author>Zhang, Ce</author></authors></contributors><titles><title>Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning Workloads</title></titles><periodical/><pages>607-620</pages><volume>11</volume><issue>5</issue><keywords/><dates><year>2017</year></dates><urls><pdf-urls><url>internal-pdf://LietalEaseML.pdf</url></pdf-urls><web-urls><url>http://arxiv.org/abs/1708.07308</url></web-urls></urls><abstract>We present ease.ml, a declarative machine learning service platform we built to support more than ten research groups outside the computer science departments at ETH Zurich for their machine learning needs. With ease.ml, a user defines the high-level schema of a machine learning application and submits the task via a Web interface. The system automatically deals with the rest, such as model selection and data movement. In this paper, we describe the ease.ml architecture and focus on a novel technical problem introduced by ease.ml regarding resource allocation. We ask, as a &quot;service provider&quot; that manages a shared cluster of machines among all our users running machine learning workloads, what is the resource allocation strategy that maximizes the global satisfaction of all our users? Resource allocation is a critical yet subtle issue in this multi-tenant scenario, as we have to balance between efficiency and fairness. We first formalize the problem that we call multi-tenant model selection, aiming for minimizing the total regret of all users running automatic model selection tasks. We then develop a novel algorithm that combines multi-armed bandits with Bayesian optimization and prove a regret bound under the multi-tenant setting. Finally, we report our evaluation of ease.ml on synthetic data and on one service we are providing to our users, namely, image classification with deep neural networks. Our experimental evaluation results show that our proposed solution can be up to 9.8x faster in achieving the same global quality for all users as the two popular heuristics used by our users before ease.ml.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Günther, Vlad and Alexandru Chirita</author></authors></contributors><titles><title>&quot; Scienceroot &quot; Whitepaper</title></titles><periodical/><keywords/><dates><year>2018</year></dates><urls><pdf-urls><url>internal-pdf://whitepaperScienceRoot.pdf</url></pdf-urls><web-urls><url>https://www.scienceroot.com/</url></web-urls></urls><abstract>Technology evolves faster than ever, with the pace picking up with every passing year. Unprecedented in history, we have the greatest and brightest minds driving unparalleled change. Progress isn’t a trend that naturally happens on its own - it requires an ever growing number of researchers and experts. The scientific community of today doesn’t get what it deserves, constantly struggling with obtaining funding and work- ing endless hours with little to no reimbursement. It is our stated mission objective to empower those who empower us all by establishing Science- root, the first blockchain-based scientific ecosystem to integrate a social media scientific network, a funding platform and a decentralized publish- ing framework for journals.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Roberts, S.</author><author>Osborne, M.</author><author>Ebden, M.</author><author>Reece, S.</author><author>Gibson, N.</author><author>Aigrain, S.</author></authors></contributors><titles><title>Gaussian processes for time-series modelling</title><secondary-title>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</secondary-title></titles><periodical><full-title>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</full-title></periodical><pages>1-27</pages><volume>371</volume><issue>1984</issue><keywords><keyword>Bayesian modelling</keyword><keyword>Gaussian processes</keyword><keyword>Time-series analysis</keyword></keywords><dates><year>2013</year></dates><isbn>1364-503X (Print) 1364-503X (Linking)</isbn><accession-num>23277607</accession-num><electronic-resource-num>10.1098/rsta.2011.0550</electronic-resource-num><urls><pdf-urls><url>internal-pdf://</url></pdf-urls></urls><abstract>In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Valera, Isabel</author><author>Ruiz, Francisco</author><author>Svensson, Lennart</author><author>Perez-Cruz, Fernando</author></authors></contributors><titles><title>Infinite Factorial Dynamical Model</title><secondary-title>Advances in Neural Information Processing Systems 28</secondary-title></titles><periodical><full-title>Advances in Neural Information Processing Systems 28</full-title></periodical><pages>1666-1674</pages><keywords/><dates><year>2015</year></dates><urls><pdf-urls><url>internal-pdf://</url></pdf-urls><web-urls><url>http://papers.nips.cc/paper/5667-infinite-factorial-dynamical-model.pdf</url></web-urls></urls><abstract>We propose the infinite factorial dynamic model (iFDM), a general Bayesian nonparametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Hensman, James</author><author>Rattray, Magnus</author><author>Lawrence, Neil D.</author></authors></contributors><titles><title>Fast nonparametric clustering of structured time-series</title><secondary-title>IEEE Transactions on Pattern Analysis and Machine Intelligence</secondary-title></titles><periodical><full-title>IEEE Transactions on Pattern Analysis and Machine Intelligence</full-title></periodical><pages>383-393</pages><volume>37</volume><issue>2</issue><keywords/><dates><year>2015</year></dates><electronic-resource-num>10.1109/TPAMI.2014.2318711</electronic-resource-num><urls><pdf-urls><url>internal-pdf://</url></pdf-urls></urls><abstract>In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Gael, Jurgen Van</author></authors></contributors><titles><title>The Infinite Hidden Markov Model</title><secondary-title>Advances in Neural Information Processing Systems 14</secondary-title></titles><periodical><full-title>Advances in Neural Information Processing Systems 14</full-title></periodical><pages>1-8</pages><keywords/><dates><year>2018</year></dates><isbn>0-262-04208-8</isbn><electronic-resource-num>10.7551/mitpress/1120.003.0079</electronic-resource-num><urls><pdf-urls><url>internal-pdf://</url></pdf-urls></urls><abstract>We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Book">1</ref-type><contributors><authors><author>Alex, Graves</author></authors></contributors><titles><title>Supervised Sequence Labelling with RNN</title><secondary-title>Studies in computational intelligence</secondary-title></titles><periodical><full-title>Studies in computational intelligence</full-title></periodical><pages>252</pages><keywords><keyword>Bayes-Netz</keyword><keyword>Bootstrap-Aggregation</keyword><keyword>Computational Intelligence</keyword><keyword>Ensembles in Machine Learning Applications</keyword><keyword>Fehlerkorrekturcode</keyword><keyword>Hardback</keyword><keyword>Klassifikator</keyword><keyword>Merkmalsextraktion</keyword><keyword>Research</keyword><keyword>Set theory</keyword><keyword>Soft Computing</keyword><keyword>Unüberwachtes Lernen</keyword><keyword>machine learning</keyword><keyword>Überwachtes Lernen</keyword></keywords><dates><year>2011</year></dates><isbn>9783642229091 9783642229091 3642229093 9783642229107</isbn><accession-num>23459267</accession-num><electronic-resource-num>10.1007/978-3-642-24797-2</electronic-resource-num><urls><pdf-urls><url>internal-pdf://</url></pdf-urls><web-urls><url>files/1074/bok%3A978-3-642-24797-2.pdf</url></web-urls></urls></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Reichstein, Markus</author><author>Camps-Valls, Gustau</author><author>Stevens, Bjorn</author><author>Jung, Martin</author><author>Denzler, Joachim</author><author>Carvalhais, Nuno</author><author>Prabhat, &amp;</author></authors></contributors><titles><title>Deep learning and process understanding for data-driven Earth system science</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title></periodical><keywords/><publisher>Springer US</publisher><electronic-resource-num>10.1038/s41586-019-0912-1</electronic-resource-num><urls><pdf-urls><url>internal-pdf://s41586-019-0912-1.pdf</url></pdf-urls><web-urls><url>www.nature.com/nature</url></web-urls></urls><abstract>Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning. H umans have always striven to predict and understand the world, and the ability to make better predictions has given competitive advantages in diverse contexts (such as weather, diseases or financial markets). Yet the tools for prediction have substantially changed over time, from ancient Greek philosophical reasoning to non-scientific medieval methods such as soothsaying, towards modern scientific discourse , which has come to include hypothesis testing, theory development and computer modelling underpinned by statistical and physical relationships, that is, laws 1. A success story in the geosciences is weather prediction, which has greatly improved through the integration of better theory, increased computational power, and established observational systems, which allow for the assimilation of large amounts of data into the modelling system 2. Nevertheless, we can accurately predict the evolution of the weather on a timescale of days, not months. Seasonal meteorological predictions, forecasting extreme events such as flooding or fire, and long-term climate projections are still major challenges. This is especially true for predicting dynamics in the biosphere, which is dominated by biologically mediated processes such as growth or reproduction, and is strongly controlled by seemingly stochastic disturbances such as fires and landslides. Such predictive problems have not seen much progress in the past few decades 3. At the same time, a deluge of Earth system data has become available, with storage volumes already well beyond dozens of petabytes and rapidly increasing transmission rates exceeding hundreds of terabytes per day 4. These data come from a plethora of sensors measuring states, fluxes and intensive or time/space-integrated variables, representing fifteen or more orders of temporal and spatial magnitude. They include remote sensing from a few metres to hundreds of kilometres above Earth as well as in situ observations (increasingly from autonomous sensors) at and below the surface and in the atmosphere, many of which are further being complemented by citizen science observations. Model simulation output adds to this deluge; the CMIP-5 dataset of the Climate Model Intercomparison Project, used extensively for scientific groundwork towards periodic climate assessments, is over 3 petabytes in size, and the next generation, CMIP-6, is estimated to reach up to 30 petabytes 5. The data from models share many of the challenges and statistical properties of observational data, including many forms of uncertainty. In summary, Earth system data are exemplary of all four of the 'four Vs' of 'big data': volume, velocity, variety and veracity (see Fig. 1). One key challenge is to extract interpret-able information and knowledge from this big data, possibly almost in real time and integrating between disciplines. Taken together, our ability to collect and create data far outpaces our ability to sensibly assimilate it, let alone understand it. Predictive ability in the last few decades has not increased apace with data availability. To get the most out of the explosive growth and diversity of Earth system data, we face two major tasks in the coming years: (1) extracting knowledge from the data deluge, and (2) deriving models that learn much more from data than traditional data assimilation approaches can, while still respecting our evolving understanding of nature's laws. The combination of unprecedented data sources, increased computational power, and the recent advances in statistical modelling and machine learning offer exciting new opportunities for expanding our knowledge about the Earth system from data. In particular, many tools are available from the fields of machine learning and artificial intelligence, but they need to be further developed and adapted to geo-scientific analysis. Earth system science offers new opportunities, challenges and methodological demands, in particular for recent research lines focusing on spatio-temporal context and uncertainties (Box 1; see https://developers. google.com/machine-learning/glossary/ and http://www.wildml.com/ deep-learning-glossary/ for more complete glossaries). In the following sections we review the development of machine learning in the geoscientific context, and highlight how deep learning-that is, the automatic extraction of abstract (spatio-temporal) features-has the potential to overcome many of the limitations that have, until now, hindered a more widespread adoption of machine learning. We further lay out the most promising but also challenging approaches in combining machine learning with physical modelling. State-of-the-art geoscientific machine learning Machine learning is now a successful part of several research-driven and operational geoscientific processing schemes, addressing the atmosphere, the land surface and the ocean, and has co-evolved with data availability over the past decade. Early landmarks in classification of land cover and clouds emerged almost 30 years ago through the coincidence of high-resolution satellite data and the first revival of neural networks 6,7. Most major machine learning methodological</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Uzzi, Brian</author><author>Vespignani, Alessandro</author><author>Börner, Katy</author><author>Radicchi, Filippo</author><author>Sinatra, Roberta</author><author>Barabási, Albert-László</author><author>Waltman, Ludo</author><author>Bergstrom, Carl T.</author><author>Milojević, Staša</author><author>Helbing, Dirk</author><author>Petersen, Alexander M.</author><author>Fortunato, Santo</author><author>Wang, Dashun</author><author>Evans, James A.</author></authors></contributors><titles><title>Science of science</title><secondary-title>Science</secondary-title></titles><periodical><full-title>Science</full-title></periodical><pages>eaao0185</pages><volume>359</volume><issue>6379</issue><keywords/><dates><year>2018</year></dates><electronic-resource-num>10.1126/science.aao0185</electronic-resource-num><urls><pdf-urls><url>internal-pdf://eaao0185.full.pdf</url></pdf-urls></urls></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Voelkl, Bernhard</author><author>Vogt, Lucile</author><author>Sena, Emily S.</author><author>Würbel, Hanno</author></authors></contributors><titles><title>Reproducibility of preclinical animal research improves with heterogeneity of study samples</title><secondary-title>PLoS Biology</secondary-title></titles><periodical><full-title>PLoS Biology</full-title></periodical><volume>16</volume><issue>2</issue><keywords/><dates><year>2018</year></dates><isbn>1111111111</isbn><accession-num>29470495</accession-num><electronic-resource-num>10.1371/journal.pbio.2003693</electronic-resource-num><urls><pdf-urls><url>internal-pdf://Voelkl_PLoSBiol2018_All.pdf</url></pdf-urls></urls><abstract>Single-laboratory studies conducted under highly standardized conditions are the gold standard in preclinical animal research. Using simulations based on 440 preclinical studies across 13 different interventions in animal models of stroke, myocardial infarction, and breast cancer, we compared the accuracy of effect size estimates between single-laboratory and multi-laboratory study designs. Single-laboratory studies generally failed to predict effect size accurately, and larger sample sizes rendered effect size estimates even less accurate. By contrast, multi-laboratory designs including as few as 2 to 4 laboratories increased coverage probability by up to 42 percentage points without a need for larger sample sizes. These findings demonstrate that within-study standardization is a major cause of poor reproducibility. More representative study samples are required to improve the external validity and reproducibility of preclinical animal research and to prevent wasting animals and resources for inconclusive research.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>O'Hare, Anthony</author></authors></contributors><titles><title>Inference in High-Dimensional Parameter Space</title><secondary-title>Journal of Computational Biology</secondary-title></titles><periodical><full-title>Journal of Computational Biology</full-title></periodical><pages>997-1004</pages><volume>22</volume><issue>11</issue><keywords><keyword>1999</keyword><keyword>also</keyword><keyword>antigen</keyword><keyword>bola</keyword><keyword>bola-drb3</keyword><keyword>bovine</keyword><keyword>called the bovine leucocyte</keyword><keyword>complex</keyword><keyword>genotyping</keyword><keyword>includes many immune-related genes</keyword><keyword>lewin et al</keyword><keyword>mhc</keyword><keyword>of cattle</keyword><keyword>the major histocompatibility complex</keyword></keywords><dates><year>2015</year></dates><accession-num>26176624</accession-num><electronic-resource-num>10.1089/cmb.2015.0086</electronic-resource-num><urls><web-urls><url>http://online.liebertpub.com/doi/10.1089/cmb.2015.0086</url></web-urls></urls><abstract>Model parameter inference has become increasingly popular in recent years in the field of computational epidemiology, especially for models with a large number of parameters. Techniques such as Approximate Bayesian Computation (ABC) or maximum/partial likelihoods are commonly used to infer parameters in phenomenological models that best describe some set of data. These techniques rely on efficient exploration of the underlying parameter space, which is difficult in high dimensions, especially if there are correlations between the parameters in the model that may not be known a priori. The aim of this article is to demonstrate the use of the recently invented Adaptive Metropolis algorithm for exploring parameter space in a practical way through the use of a simple epidemiological model.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Cozzo, Emanuele</author><author>Moreno, Yamir</author></authors></contributors><titles><title>Dimensionality reduction and spectral properties of multilayer networks</title></titles><periodical/><keywords/><dates><year>2018</year></dates><urls><pdf-urls><url>internal-pdf://1311.1759.pdf</url></pdf-urls></urls></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Book">1</ref-type><contributors><authors><author>Markowitz, H</author></authors></contributors><titles><title>Portfolio selection</title></titles><periodical/><keywords/><dates><year>1991</year></dates><publisher>Blackwell Publishing, MA</publisher><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Walters, Jay</author></authors></contributors><titles><title>The Black-Litterman Model in Detail</title><secondary-title>SSRN Electronic Journal</secondary-title></titles><periodical><full-title>SSRN Electronic Journal</full-title></periodical><keywords/><dates><year>2011</year></dates><isbn>1556-5068</isbn><electronic-resource-num>10.2139/ssrn.1314585</electronic-resource-num><urls><web-urls><url>http://www.ssrn.com/abstract=1314585</url></web-urls></urls><abstract>In this paper we survey the literature on the Black-Litterman model. This survey is provided both as a chronology and a taxonomy as there are many claims on the model in the literature. We provide a complete description of the canonical model including full derivations from the underlying principles using both Theil's Mixed Estimation model and Bayes Theory. The various parameters of the model are considered, along with information on their computation or calibration. Further consideration is given to several of the key papers, with worked examples illustrating the concepts.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Gao J.</author><author>Buldyrev S. V.</author><author>Stanley H. E.</author><author>Havlin, S</author></authors></contributors><titles><title>Networks formed from interdependent networks</title><secondary-title>Nature physics</secondary-title></titles><periodical><full-title>Nature physics</full-title></periodical><pages>40-48</pages><volume>8</volume><keywords/><dates><year>2012</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Grelaud A.</author><author>Robert C. P.</author><author>Marin J-M.</author><author>Rodolphe F.</author><author>Taly, J-F.</author></authors></contributors><titles><title>{ABC} likelihood-free methods for model choice in {G}ibbs random fields</title><secondary-title>Bayesian Analysis</secondary-title></titles><periodical><full-title>Bayesian Analysis</full-title></periodical><pages>317-336</pages><volume>4</volume><keywords/><dates><year>2009</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Tarantola, A</author></authors></contributors><titles><title>Popper, {B}ayes and the inverse problem</title><secondary-title>Nature physics</secondary-title></titles><periodical><full-title>Nature physics</full-title></periodical><pages>492-494</pages><volume>2</volume><keywords/><dates><year>2006</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Kivela, M</author></authors></contributors><titles><title>Multilayer networks library</title><secondary-title>http://www.plexmath.eu/?page_id=327</secondary-title></titles><periodical><full-title>http://www.plexmath.eu/?page_id=327</full-title></periodical><keywords/><dates><year>2013</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Ioannidis, John P a</author></authors></contributors><titles><title>Why most published research findings are false.</title><secondary-title>PLoS medicine</secondary-title></titles><periodical><full-title>PLoS medicine</full-title></periodical><pages>e124</pages><volume>2</volume><issue>8</issue><keywords><keyword>Bias (Epidemiology)</keyword><keyword>Data Interpretation</keyword><keyword>Likelihood Functions</keyword><keyword>Meta-Analysis as Topic</keyword><keyword>Odds Ratio</keyword><keyword>Publishing</keyword><keyword>Reproducibility of Results</keyword><keyword>Research Design</keyword><keyword>Sample Size</keyword><keyword>Statistical</keyword></keywords><dates><year>2005</year></dates><accession-num>16060722</accession-num><electronic-resource-num>10.1371/journal.pmed.0020124</electronic-resource-num><urls><web-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/16060722</url></web-urls></urls><abstract>There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>De Domenico M.</author><author>Porter M. A.</author><author>Arenas, A</author></authors></contributors><titles><title>MuxViz: a tool for multilayer analysis and visualization of networks</title><secondary-title>Journal of Complex Networks</secondary-title></titles><periodical><full-title>Journal of Complex Networks</full-title></periodical><pages>159-176</pages><volume>3</volume><keywords/><dates><year>2014</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Unpublished Work">37</ref-type><contributors><authors><author>Bailey, David H.</author><author>Borwein, Jonathan</author><author>Lopez de Prado, Marcos</author><author>Zhu, Qiji Jim</author></authors></contributors><titles><title>Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance</title><secondary-title>SSRN</secondary-title></titles><periodical><full-title>SSRN</full-title></periodical><keywords><keyword>E44</keyword><keyword>G0</keyword><keyword>G1</keyword><keyword>G15</keyword><keyword>G2</keyword><keyword>G24</keyword><keyword>Sharpe ratio</keyword><keyword>backtest</keyword><keyword>historical simulation</keyword><keyword>investment strategy</keyword><keyword>minimum backtest length</keyword><keyword>optimization</keyword><keyword>performance degradation</keyword><keyword>probability of backtest over-fitting</keyword></keywords><dates><year>2013</year></dates><electronic-resource-num>10.2139/ssrn.2308659</electronic-resource-num><urls/><abstract>We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Kivela M.</author><author>Arenas A.</author><author>Barthelemy M.</author><author>Gleeson J. P.</author><author>Moreno Y.</author><author>Porter, M A</author></authors></contributors><titles><title>Multilayer networks</title><secondary-title>Journal of Complex Networks</secondary-title></titles><periodical><full-title>Journal of Complex Networks</full-title></periodical><pages>203-271</pages><volume>3</volume><keywords/><dates><year>2014</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Cristian, Flaviu</author></authors></contributors><titles><title>Probabilistic clock synchronization</title><secondary-title>Distributed Computing</secondary-title></titles><periodical><full-title>Distributed Computing</full-title></periodical><pages>146-158</pages><volume>3</volume><issue>3</issue><keywords><keyword>Clock synchronization</keyword><keyword>Communication</keyword><keyword>Distributed system</keyword><keyword>Fault-tolerance</keyword><keyword>Time service</keyword></keywords><dates><year>1989</year></dates><electronic-resource-num>10.1007/BF01784024</electronic-resource-num><urls/><abstract>Abstract A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock ... \n</abstract></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Beaumont, M A</author></authors></contributors><titles><title>Approximate {B}ayesian Computation in Evolution and Ecology</title><secondary-title>Annual Review Ecology, Evolution and Systematics</secondary-title></titles><periodical><full-title>Annual Review Ecology, Evolution and Systematics</full-title></periodical><pages>379-406</pages><volume>41</volume><keywords/><dates><year>2010</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>De Domenico M.</author><author>Nicosia V.</author><author>Latora, V</author></authors></contributors><titles><title>Structural reducibility of multilayer networks</title><secondary-title>Nature Communications</secondary-title></titles><periodical><full-title>Nature Communications</full-title></periodical><pages>6864</pages><volume>6</volume><keywords/><dates><year>2015</year></dates><urls/></record><record><database name="Robhoot.enl" path="Robhoot.enl">Robhoot.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Vats, Dootika</author><author>Flegal, James M.</author><author>Jones, Galin L.</author></authors></contributors><titles><title>Multivariate Output Analysis for Markov chain Monte Carlo</title></titles><periodical/><keywords/><dates><year>2015</year></dates><isbn>1512.07713</isbn><electronic-resource-num>10.1016/j.jaad.2011.01.035</electronic-resource-num><urls><pdf-urls><url>internal-pdf://</url></pdf-urls><web-urls><url>http://arxiv.org/abs/1512.07713</url></web-urls></urls><abstract>Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations with respect to a target distribution. A fundamental question is when should sampling stop so that we have good estimates of the desired quantities? The key to answering this question lies in assessing the Monte Carlo error through a multivariate Markov chain central limit theorem (CLT). The multivariate nature of this Monte Carlo error largely has been ignored in the MCMC literature. We present a multivariate framework for terminating simulation in MCMC. We define a multivariate effective sample size, estimating which requires strongly consistent estimators of the covariance matrix in the Markov chain CLT; a property we show for the multivariate batch means estimator. We then provide a lower bound on the number of minimum effective samples required for a desired level of precision. This lower bound depends on the problem only in the dimension of the expectation being estimated, and not on the underlying stochastic process. This result is obtained by drawing a connection between terminating simulation via effective sample size and terminating simulation using a relative standard deviation fixed-volume sequential stopping rule; which we demonstrate is an asymptotically valid procedure. The finite sample properties of the proposed method are demonstrated in a variety of examples.</abstract></record></records></xml>
