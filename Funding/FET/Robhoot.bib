@misc{Casadevall2020,
author = {Casadevall, Arturo and Pirofski, Liise-anne},
booktitle = {Bloomberg},
file = {::},
pages = {6--9},
title = {{It ' s Still Hard to Predict Who Will Die From}},
url = {https://www.bloomberg.com/opinion/articles/2020-04-06/it-s-still-hard-to-predict-who-will-die-from-covid-19},
year = {2020}
}
@manual{IrisAI,
title = {{Iris}},
url = {https://iris.ai}
}
@incollection{Steinruecken,
abstract = {The Automatic Statistician project aims to automate data science, producing predictions and human-readable reports from raw datasets with minimal human intervention. Alongside basic graphs and statistics, the generated reports contain a curation of high-level insights about the dataset, that are obtained from (1) an automated construction of models for the dataset, (2) a comparison of these models, and (3) a software component that turns these results into natural language descriptions. This chapter describes the common architecture of such Automatic Statistician systems, and discusses some of the design decisions and technical challenges.},
author = {Steinruecken, Christian and Smith, Emma and Janz, David and Lloyd, James and Ghahramani, Zoubin},
doi = {10.1007/978-3-030-05318-5_9},
editor = {{F. Hutter et al. (eds.), Automated Machine Learning, The Springer Series on Challenges in Machine Learning}},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Steinruecken2019{\_}Chapter{\_}TheAutomaticStatistician.pdf:pdf},
isbn = {9783030053185},
pages = {161--173},
title = {{The Automatic Statistician}},
year = {2019}
}
@article{OHare2015,
abstract = {Model parameter inference has become increasingly popular in recent years in the field of computational epidemiology, especially for models with a large number of parameters. Techniques such as Approximate Bayesian Computation (ABC) or maximum/partial likelihoods are commonly used to infer parameters in phenomenological models that best describe some set of data. These techniques rely on efficient exploration of the underlying parameter space, which is difficult in high dimensions, especially if there are correlations between the parameters in the model that may not be known a priori. The aim of this article is to demonstrate the use of the recently invented Adaptive Metropolis algorithm for exploring parameter space in a practical way through the use of a simple epidemiological model.},
author = {O'Hare, Anthony},
doi = {10.1089/cmb.2015.0086},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {1999,also,antigen,bola,bola-drb3,bovine,called the bovine leucocyte,complex,genotyping,includes many immune-related genes,lewin et al,mhc,of cattle,the major histocompatibility complex},
number = {11},
pages = {997--1004},
pmid = {26176624},
title = {{Inference in High-Dimensional Parameter Space}},
url = {http://online.liebertpub.com/doi/10.1089/cmb.2015.0086},
volume = {22},
year = {2015}
}
@article{Cozzo2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.1759v4},
author = {Cozzo, Emanuele and Moreno, Yamir},
eprint = {arXiv:1311.1759v4},
file = {:home/melian/Downloads/1311.1759.pdf:pdf},
title = {{Dimensionality reduction and spectral properties of multilayer networks}},
year = {2018}
}
@book{MarkowitzBook,
author = {Markowitz, H},
publisher = {Blackwell Publishing, MA},
title = {{Portfolio selection}},
year = {1991}
}
@article{Kivela:2013,
author = {Kivela, M},
journal = {http://www.plexmath.eu/?page{\_}id=327},
title = {{Multilayer networks library}},
year = {2013}
}
@article{Vats2015,
abstract = {Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations with respect to a target distribution. A fundamental question is when should sampling stop so that we have good estimates of the desired quantities? The key to answering this question lies in assessing the Monte Carlo error through a multivariate Markov chain central limit theorem (CLT). The multivariate nature of this Monte Carlo error largely has been ignored in the MCMC literature. We present a multivariate framework for terminating simulation in MCMC. We define a multivariate effective sample size, estimating which requires strongly consistent estimators of the covariance matrix in the Markov chain CLT; a property we show for the multivariate batch means estimator. We then provide a lower bound on the number of minimum effective samples required for a desired level of precision. This lower bound depends on the problem only in the dimension of the expectation being estimated, and not on the underlying stochastic process. This result is obtained by drawing a connection between terminating simulation via effective sample size and terminating simulation using a relative standard deviation fixed-volume sequential stopping rule; which we demonstrate is an asymptotically valid procedure. The finite sample properties of the proposed method are demonstrated in a variety of examples.},
archivePrefix = {arXiv},
arxivId = {1512.07713},
author = {Vats, Dootika and Flegal, James M. and Jones, Galin L.},
doi = {10.1016/j.jaad.2011.01.035},
eprint = {1512.07713},
file = {::},
isbn = {1512.07713},
issn = {0190-9622},
title = {{Multivariate Output Analysis for Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1512.07713},
year = {2015}
}
@manual{GoogleAI,
title = {{Google AI}},
url = {https://ai.google/}
}
@article{Andersen2020,
abstract = {Since the first reports of a novel pneumonia (COVID-19) in Wuhan city, Hubei province, China there has been considerable discussion and uncertainty over the origin of the causative virus, SARS-CoV-2. Infections with SARS-CoV-2 are now widespread in China, with cases in every province. As of 14 February 2020, 64,473 such cases have been confirmed, with 1,384 deaths attributed to the virus. These official case numbers are likely an underestimate because of limited reporting of mild and asymptomatic cases, and the virus is clearly capable of efficient human-to-human transmission. Based on the possibility of spread to countries with weaker healthcare systems, the World Health Organization has declared the COVID-19 outbreak a Public Health Emergency of International Concern (PHEIC). There are currently neither vaccines nor specific treatments for this disease.},
author = {Andersen, Kristian G. and Rambaut, Andrew and Lipkin, W. Ian and Holmes, Edward C. and Garry, Robert F.},
doi = {10.2106/JBJS.F.00094},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Andersen{\_}et{\_}al{\_}2020{\_}NatureMed{\_}The{\_}proximate{\_}corigin{\_}of{\_}SARS{\_}CoV.pdf:pdf},
issn = {0021-9355},
journal = {Virological},
number = {ii},
pages = {1--7},
title = {{The Proximal Origin of SARS-CoV-2}},
url = {http://virological.org/t/the-proximal-origin-of-sars-cov-2/398},
year = {2020}
}
@article{Uzzi2018,
author = {Uzzi, Brian and Vespignani, Alessandro and B{\"{o}}rner, Katy and Radicchi, Filippo and Sinatra, Roberta and Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Waltman, Ludo and Bergstrom, Carl T. and Milojevi{\'{c}}, Sta{\v{s}}a and Helbing, Dirk and Petersen, Alexander M. and Fortunato, Santo and Wang, Dashun and Evans, James A.},
doi = {10.1126/science.aao0185},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/eaao0185.full.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6379},
pages = {eaao0185},
title = {{Science of science}},
volume = {359},
year = {2018}
}
@book{Butler1999,
author = {Rummo, Paul-Eerik and Soosaar, Enn},
booktitle = {World Literature Today},
doi = {10.2307/40153772},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/GPMB{\_}Annual{\_}Report{\_}Exec{\_}Summary{\_}Foreword{\_}and{\_}About{\_}English.pdf:pdf},
issn = {01963570},
number = {2},
pages = {330},
title = {{A World}},
url = {https://www.jstor.org/stable/10.2307/40153772?origin=crossref},
volume = {72},
year = {1998}
}
@article{Bin2020,
abstract = {The initial UK government strategy of a timed intervention as a means of combatting Covid-19 is in stark contrast to policies adopted in many other countries which have embraced more severe social distancing policies. Our objective in this note is to enunciate the differences between these policies and to suggest modified policies, for post lock-down, that may allow management of Covid-19, while at the same time enabling some degree of reduced social and economic activity.},
archivePrefix = {arXiv},
arxivId = {2003.09930},
author = {Bin, Michelangelo and Cheung, Peter and Crisostomi, Emanuele and Ferraro, Pietro and Myant, Connor and Parisini, Thomas and Shorten, Robert},
eprint = {2003.09930},
file = {::},
month = {mar},
title = {{On Fast Multi-Shot Epidemic Interventions for Post Lock-Down Mitigation: Implications for Simple Covid-19 Models}},
url = {http://arxiv.org/abs/2003.09930},
year = {2020}
}
@manual{HOT,
title = {{HOT}},
url = {https://www.hotosm.org/}
}
@article{Cardoso2016,
abstract = {Ecological systems are the quintessential complex systems, involving numerous high-order interactions and non-linear relationships. The most commonly used statistical modelling techniques can hardly reflect the complexity of ecological patterns and processes. Finding hidden relationships in complex data is now possible through the use of massive computational power, particularly by means of Artificial Intelligence (AI) methods, such as evolutionary computation. Here we use symbolic regression (SR), which searches for both the formal structure of equations and the fitting parameters simultaneously, hence providing the required flexibility to characterize complex ecological systems. First, we demonstrate how SR can deal with complex datasets for: 1) modelling species richness; and 2) modelling species spatial distributions. Second, we illustrate how SR can be used to find general models in ecology, by using it to: 3) develop new models for the interspecific abundance-occupancy relationship; 4) develop species richness estimators; and 5) develop the species-area relationship and the general dynamic model of oceanic island biogeography. All the examples suggest that evolving free-form equations purely from data, often without prior human inference or hypotheses, may represent a very powerful tool for ecologists and biogeographers to become aware of hidden relationships and suggest general theoretical principles.},
author = {Cardoso, Pedro and Borges, Paulo AV and Carvalho, Jose C and Rigal, Francois and Gabriel, Rosalina and Cascalho, Jose and Correia, Luis},
doi = {10.1101/027839},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/027839.full.pdf:pdf},
journal = {bioRxiv},
pages = {027839},
title = {{Automated discovery of relationships, models and principles in ecology}},
url = {http://biorxiv.org/lookup/doi/10.1101/027839},
year = {2016}
}
@article{Fang2011,
abstract = {Articles may be retracted when their findings are no longer considered trustworthy due to scientific misconduct or error, they plagiarize previously published work, or they are found to violate ethical guidelines. Using a novel measure that we call the “retraction index,” we found that the frequency of retraction varies among journals and shows a strong correlation with the journal impact factor. Although retractions are relatively rare, the retraction process is essential for correcting the literature and maintaining trust in the scientific process.},
author = {Fang, Ferric C and Casadevall, Arturo},
doi = {10.1128/IAI.05661-11},
editor = {Morrison, R P},
journal = {Infection and Immunity},
month = {oct},
number = {10},
pages = {3855 LP  -- 3859},
title = {{Retracted Science and the Retraction Index}},
url = {http://iai.asm.org/content/79/10/3855.abstract},
volume = {79},
year = {2011}
}




@article{DeDomenico2015a,
abstract = {Many complex systems can be represented as networks consisting of distinct types of interactions, which can be categorized as links belonging to different layers. For example, a good description of the full protein-protein interactome requires, for some organisms, up to seven distinct network layers, accounting for different genetic and physical interactions, each containing thousands of protein-protein relationships. A fundamental open question is then how many layers are indeed necessary to accurately represent the structure of a multilayered complex system. Here we introduce a method based on quantum theory to reduce the number of layers to a minimum while maximizing the distinguishability between the multilayer network and the corresponding aggregated graph. We validate our approach on synthetic benchmarks and we show that the number of informative layers in some real multilayer networks of protein-genetic interactions, social, economical and transportation systems can be reduced by up to 75{\%}.},
archivePrefix = {arXiv},
arxivId = {1405.0425},
author = {{De Domenico}, Manlio and Nicosia, Vincenzo and Arenas, Alexandre and Latora, Vito},
doi = {10.1038/ncomms7864},
eprint = {1405.0425},
file = {::},
isbn = {2041-1723 (Electronic)$\backslash$r2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pages = {1--9},
pmid = {25904309},
publisher = {Nature Publishing Group},
title = {{Structural reducibility of multilayer networks}},
url = {http://dx.doi.org/10.1038/ncomms7864},
volume = {6},
year = {2015}
}
@article{Fan2012,
abstract = {Access to a large amount of knowledge is critical for success at answering open-domain questions for DeepQA systems such as IBM Watson™. Formal representation of knowledge has the advantage of being easy to reason with, but acquisition of structured knowledge in open domains from unstructured data is often difficult and expensive. Our central hypothesis is that shallow syntactic knowledge and its implied semantics can be easily acquired and can be used in many areas of a question-answering system. We take a two-stage approach to extract the syntactic knowledge and implied semantics. First, shallow knowledge from large collections of documents is automatically extracted. Second, additional semantics are inferred from aggregate statistics of the automatically extracted shallow knowledge. In this paper, we describe in detail what kind of shallow knowledge is extracted, how it is automatically done from a large corpus, and how additional semantics are inferred from aggregate statistics. We also briefly discuss the various ways extracted knowledge is used throughout the IBM DeepQA system. {\textcopyright} 1957-2012 IBM.},
author = {Fan, J. and Kalyanpur, A. and Gondek, D. C. and Ferrucci, D. A.},
doi = {10.1147/JRD.2012.2186519},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/05 - Automatic knowledge extraction from documents.pdf:pdf},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = {may},
number = {3.4},
pages = {5:1--5:10},
title = {{Automatic knowledge extraction from documents}},
url = {http://ieeexplore.ieee.org/document/6177723/},
volume = {56},
year = {2012}
}
@article{Sahasranaman2020,
abstract = {We characterize the network of COVID-19 spread in India and find that the transmission rate is 0.43, with daily case growth driven by individuals who contracted the virus abroad. We explore the question of whether this represents exponentially decaying dynamics or is simply an artefact of India's testing strategy. Testing has largely been limited to individuals travelling from high-risk countries and their immediate contacts, meaning that the network reflects positive identifications from a biased testing sample. Given generally low levels of testing and an almost complete absence of testing for community spread, there is significant risk that we may be missing out on the actual nature of outbreak. India still has an apparently low current caseload, with possibly a small window of time to act, and should therefore aggressively and systematically expand random testing for community spread, including for asymptomatic cases. This will help understand true transmission characteristics and plan appropriately for the immediate future.},
archivePrefix = {arXiv},
arxivId = {2003.09715},
author = {Sahasranaman, Anand and Kumar, Nishanth},
eprint = {2003.09715},
file = {::},
month = {mar},
title = {{Network structure of COVID-19 spread and the lacuna in India's testing strategy}},
url = {http://arxiv.org/abs/2003.09715},
year = {2020}
}
@article{Stier2020,
abstract = {The current outbreak of novel coronavirus disease 2019 (COVID-19) poses an unprecedented global health and economic threat to interconnected human societies. Until a vaccine is developed, strategies for controlling the outbreak rely on aggressive social distancing. These measures largely disconnect the social network fabric of human societies, especially in urban areas. Here, we estimate the growth rates and reproductive numbers of COVID-19 in US cities from March 14th through March 19th to reveal a power-law scaling relationship to city population size. This means that COVID-19 is spreading faster on average in larger cities with the additional implication that, in an uncontrolled outbreak, larger fractions of the population are expected to become infected in more populous urban areas. We discuss the implications of these observations for controlling the COVID-19 outbreak, emphasizing the need to implement more aggressive distancing policies in larger cities while also with preserving socioeconomic activity.},
archivePrefix = {arXiv},
arxivId = {2003.10376},
author = {Stier, Andrew J and Berman, Marc G and Bettencourt, Luis M A},
eprint = {2003.10376},
file = {::},
month = {mar},
title = {{COVID-19 attack rate increases with city size}},
url = {http://arxiv.org/abs/2003.10376},
year = {2020}
}
@article{Leung2020,
author = {Leung, Kathy and Wu, Joseph T and Liu, Di and Leung, Gabriel M},
doi = {10.1016/S0140-6736(20)30746-7},
file = {::},
issn = {01406736},
journal = {The Lancet},
month = {apr},
number = {20},
title = {{First-wave COVID-19 transmissibility and severity in China outside Hubei after control measures, and second-wave scenario planning: a modelling impact assessment}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673620307467},
volume = {6736},
year = {2020}
}
@article{Grazzini2017,
author = {Grazzini, Jakob and Richiardi, Matteo G. and Tsionas, Mike},
doi = {10.1016/j.jedc.2017.01.014},
issn = {01651889},
journal = {Journal of Economic Dynamics and Control},
month = {apr},
pages = {26--47},
title = {{Bayesian estimation of agent-based models}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0165188917300222},
volume = {77},
year = {2017}
}
@article{Kivelaetal:2014,
author = {{Kivela M.} and {Arenas A.} and {Barthelemy M.} and {Gleeson J. P.} and {Moreno Y.} and Porter, M A},
journal = {Journal of Complex Networks},
pages = {203--271},
title = {{Multilayer networks}},
volume = {3},
year = {2014}
}
@article{Faggian2020,
abstract = {We present a mobile app solution to help the containment of an epidemic outbreak by keeping track of possible infections in the incubation period. We consider the particular case of an infection which primarily spreads among people through proximal contact, via respiratory droplets. This smartphone application will work offline and will be able to detect other devices in close proximity and list all the interactions in an anonymous and encrypted way. If an app user is tested positive and so is certified as infected, the application notifies immediately the potential contagion to the devices in the list and suggests to start a voluntary quarantine and undergo a medical test. We believe this solution may be useful in particular in the current COVID-19 pandemic and moreover could be used to prevent similar events in the future.},
archivePrefix = {arXiv},
arxivId = {2003.10222},
author = {Faggian, Marco and Urbani, Michele and Zanotto, Luca},
eprint = {2003.10222},
file = {::},
month = {mar},
title = {{Proximity: a recipe to break the outbreak}},
url = {http://arxiv.org/abs/2003.10222},
year = {2020}
}
@manual{Bluecloud,
title = {{Blue-cloud}},
url = {https://www.blue-cloud.org/}
}
@article{Valera2015,
abstract = {We propose the infinite factorial dynamic model (iFDM), a general Bayesian nonparametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.},
author = {Valera, Isabel and Ruiz, Francisco and Svensson, Lennart and Perez-Cruz, Fernando},
file = {::},
journal = {Advances in Neural Information Processing Systems 28},
pages = {1666--1674},
title = {{Infinite Factorial Dynamical Model}},
url = {http://papers.nips.cc/paper/5667-infinite-factorial-dynamical-model.pdf},
year = {2015}
}
@book{Alex2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Alex, Graves},
booktitle = {Studies in computational intelligence},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
file = {::},
isbn = {9783642229091 9783642229091 3642229093 9783642229107},
issn = {18792782},
keywords = {Bayes-Netz,Bootstrap-Aggregation,Computational Intelligence,Ensembles in Machine Learning Applications,Fehlerkorrekturcode,Hardback,Klassifikator,Merkmalsextraktion,Research,Set theory,Soft Computing,Un{\"{u}}berwachtes Lernen,machine learning,{\"{U}}berwachtes Lernen},
pages = {252},
pmid = {23459267},
title = {{Supervised Sequence Labelling with RNN}},
url = {files/1074/bok{\%}3A978-3-642-24797-2.pdf},
year = {2011}
}

@Book{Bianconi2018,
  title =     {Multilayer Networks},
  publisher = {Oxford: Oxford University Press. 402 p.},
  year =      {2018},
  author =    {Ginestra Bianconi}
}

@manual{Elixir,
title = {{Elixir open data}},
url = {https://elixir-europe.org/}
}
@article{Voelkl2018,
abstract = {Single-laboratory studies conducted under highly standardized conditions are the gold standard in preclinical animal research. Using simulations based on 440 preclinical studies across 13 different interventions in animal models of stroke, myocardial infarction, and breast cancer, we compared the accuracy of effect size estimates between single-laboratory and multi-laboratory study designs. Single-laboratory studies generally failed to predict effect size accurately, and larger sample sizes rendered effect size estimates even less accurate. By contrast, multi-laboratory designs including as few as 2 to 4 laboratories increased coverage probability by up to 42 percentage points without a need for larger sample sizes. These findings demonstrate that within-study standardization is a major cause of poor reproducibility. More representative study samples are required to improve the external validity and reproducibility of preclinical animal research and to prevent wasting animals and resources for inconclusive research.},
author = {Voelkl, Bernhard and Vogt, Lucile and Sena, Emily S. and W{\"{u}}rbel, Hanno},
doi = {10.1371/journal.pbio.2003693},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Voelkl{\_}PLoSBiol2018{\_}All.pdf:pdf},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
number = {2},
pmid = {29470495},
title = {{Reproducibility of preclinical animal research improves with heterogeneity of study samples}},
volume = {16},
year = {2018}
}
@article{Ali,
author = {Ali, M.},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Blockstack.pdf:pdf},
title = {{Blockstack Technical}}
}
@article{DeDomenico2015,
abstract = {The determination of the most central agents in complex networks is important because they are responsible for a faster propagation of information, epidemics, failures and congestion, among others. A challenging problem is to identify them in networked systems characterized by different types of interactions, forming interconnected multilayer networks. Here we describe a mathematical framework that allows us to calculate centrality in such networks and rank nodes accordingly, finding the ones that play the most central roles in the cohesion of the whole structure, bridging together different types of relations. These nodes are the most versatile in the multilayer network. We investigate empirical interconnected multilayer networks and show that the approaches based on aggregating—or neglecting—the multilayer structure lead to a wrong identification of the most versatile nodes, overestimating the importance of more marginal agents and demonstrating the power of versatility in predicting their role in diffusive and congestion processes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{De Domenico}, Manlio and Sol{\'{e}}-Ribalta, Albert and Omodei, Elisa and G{\'{o}}mez, Sergio and Arenas, Alex},
doi = {10.1038/ncomms7868},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {2041-1723 (Electronic){\$}\backslash{\$}r2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pages = {1--6},
pmid = {25904405},
title = {{Ranking in interconnected multilayer networks reveals versatile nodes}},
volume = {6},
year = {2015}
}
@inproceedings{Staar2018,
abstract = {Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make the contained knowledge discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. In this paper, we present a modular, cloud-based platform to ingest documents at scale. This platform, called the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse and annotate documents (i.e. collect ground-truth), train machine-learning classification algorithms and ultimately convert any type of PDF or bitmap-documents to a structured content representation format. We will show that each of the modules is scalable due to an asynchronous microservice architecture and can therefore handle massive amounts of documents. Furthermore, we will show that our capability to gather ground-truth is accelerated by machine-learning algorithms by at least one order of magnitude. This allows us to both gather large amounts of ground-truth in very little time and obtain very good precision/recall metrics in the range of 99{\%} with regard to content conversion to structured output. The CCS platform is currently deployed on IBM internal infrastructure and serving more than 250 active users for knowledge-engineering project engagements.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1806.02284},
author = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
doi = {10.1145/3219819.3219834},
eprint = {1806.02284},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1805.09687.pdf:pdf},
isbn = {9781450355520},
month = {jul},
pages = {774--782},
publisher = {ACM},
title = {{Corpus Conversion Service}},
url = {https://dl.acm.org/doi/10.1145/3219819.3219834},
year = {2018}
}
@article{Iten2020,
abstract = {Despite the success of neural networks at solving concrete physics problems, their use as a general-purpose tool for scientific discovery is still in its infancy. Here, we approach this problem by modeling a neural network architecture after the human physical reasoning process, which has similarities to representation learning. This allows us to make progress towards the long-term goal of machine-assisted scientific discovery from experimental data without making prior assumptions about the system. We apply this method to toy examples and show that the network finds the physically relevant parameters, exploits conservation laws to make predictions, and can help to gain conceptual insights, e.g., Copernicus' conclusion that the solar system is heliocentric.},
archivePrefix = {arXiv},
arxivId = {1807.10300},
author = {Iten, Raban and Metger, Tony and Wilming, Henrik and {Del Rio}, L{\'{i}}dia and Renner, Renato},
doi = {10.1103/PhysRevLett.124.010508},
eprint = {1807.10300},
file = {::},
issn = {10797114},
journal = {Physical Review Letters},
keywords = {doi:10.1103/PhysRevLett.124.010508 url:https://doi},
number = {1},
pages = {10508},
publisher = {American Physical Society},
title = {{Discovering Physical Concepts with Neural Networks}},
url = {https://doi.org/10.1103/PhysRevLett.124.010508},
volume = {124},
year = {2020}
}
@techreport{,
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/CV{\_}200410{\_}Deloitte{\_}Thrive{\_}scenarios{\_}for{\_}resilient{\_}leaders.pdf:pdf},
number = {April},
title = {{Remarkable times call for remarkable collaboration .}},
year = {2020}
}
@article{Kissler2020,
author = {Kissler, Stephen M. and Tedijanto, Christine and Goldstein, Edward and Grad, Yonatan H. and Lipsitch, Marc},
doi = {10.1126/science.abb5793},
file = {::},
journal = {Science},
number = {April},
title = {{Projecting the transmission dynamics of SARS-CoV-2 through the postpandemic period}},
volume = {5793},
year = {2020}
}
@article{Gil2019,
abstract = {Decades of research in artificial intelligence (AI) have produced formidable technologies that are providing immense benefit to industry, government, and society. AI systems can now translate across multiple languages, identify objects in images and video, streamline manufacturing processes, and control cars. The deployment of AI systems has not only created a trillion-dollar industry that is projected to quadruple in three years, but has also exposed the need to make AI systems fair, explainable, trustworthy, and secure. Future AI systems will rightfully be expected to reason effectively about the world in which they (and people) operate, handling complex tasks and responsibilities effectively and ethically, engaging in meaningful communication, and improving their awareness through experience. Achieving the full potential of AI technologies poses research challenges that require a radical transformation of the AI research enterprise, facilitated by significant and sustained investment. These are the major recommendations of a recent community effort coordinated by the Computing Community Consortium and the Association for the Advancement of Artificial Intelligence to formulate a Roadmap for AI research and development over the next two decades.},
archivePrefix = {arXiv},
arxivId = {1908.02624},
author = {Gil, Yolanda and Selman, Bart},
eprint = {1908.02624},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/AIRoadmapDraftforCommunityMay2019.pdf:pdf},
month = {aug},
title = {{A 20-Year Community Roadmap for Artificial Intelligence Research in the US}},
url = {https://bit.ly/2ZNVBVb http://arxiv.org/abs/1908.02624},
year = {2019}
}
@article{Seoane2020,
author = {Seoane, F and Sol, Ricard},
doi = {10.20944/preprints202001.0007.v1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/preprints202001.0007.v1.pdf:pdf},
keywords = {bottleneck method,pareto-optimality,phase transitions,statistical mechanics,syntax},
number = {January},
pages = {1--11},
title = {{Criticality in Pareto Optimal Grammars ?}},
year = {2020}
}
@article{Ferguson2020,
author = {Ferguson, Neil M and Laydon, Daniel and Nedjati-gilani, Gemma and Imai, Natsuko and Ainslie, Kylie and Baguelin, Marc and Bhatia, Sangeeta and Boonyasiri, Adhiratha and Cucunub{\'{a}}, Zulma and Cuomo-dannenburg, Gina and Dighe, Amy and Fu, Han and Gaythorpe, Katy and Green, Will and Hamlet, Arran and Hinsley, Wes and Okell, Lucy C and Van, Sabine and Thompson, Hayley and Verity, Robert and Volz, Erik and Wang, Haowei and Wang, Yuanrong and Walker, Patrick G T and Walters, Caroline and Winskill, Peter and Whittaker, Charles and Donnelly, Christl A and Riley, Steven and Ghani, Azra C},
doi = {10.25561/77482},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Ferguson et al. 2020 Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf:pdf},
number = {March},
title = {{Impact of non-pharmaceutical interventions ( NPIs ) to reduce COVID- 19 mortality and healthcare demand}},
year = {2020}
}
@article{Taylor2010,
author = {Taylor, Did and Lindsay, Alistair C and Halcox, Julian P},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/nejmc2004973.pdf:pdf},
pages = {0--2},
title = {{correspondence Niacin Compared with Ezetimibe}},
year = {2010}
}
@manual{Openstreetmap,
title = {{Openstreet}},
url = {https://www.openstreetmap.org/{\#}map=5/51.509/-0.110}
}
@article{Tarantola:2006,
author = {Tarantola, A},
journal = {Nature physics},
pages = {492--494},
title = {{Popper, {\{}B{\}}ayes and the inverse problem}},
volume = {2},
year = {2006}
}
@article{Roberts2013,
abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
doi = {10.1098/rsta.2011.0550},
file = {::},
isbn = {1364-503X (Print) 1364-503X (Linking)},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Bayesian modelling,Gaussian processes,Time-series analysis},
number = {1984},
pages = {1--27},
pmid = {23277607},
title = {{Gaussian processes for time-series modelling}},
volume = {371},
year = {2013}
}
@article{Innes2018,
abstract = {This paper presents reverse-mode algorithmic differentiation (AD) based on source code transformation, in particular of the Static Single Assignment (SSA) form used by modern compilers. The approach can support control flow, nesting, mutation, recursion, data structures, higher-order functions, and other language constructs, and the output is given to an existing compiler to produce highly efficient differentiated code. Our implementation is a new AD tool for the Julia language, called Zygote, which presents high-level dynamic semantics while transparently compiling adjoint code under the hood. We discuss the benefits of this approach to both the usability and performance of AD tools.},
archivePrefix = {arXiv},
arxivId = {1810.07951},
author = {Innes, Michael},
eprint = {1810.07951},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1810.07951v3.pdf:pdf},
month = {oct},
number = {1},
title = {{Don't Unroll Adjoint: Differentiating SSA-Form Programs}},
url = {http://arxiv.org/abs/1810.07951},
year = {2018}
}
@manual{Modulos,
title = {{Modulos}},
url = {http://www.modulos.ai/}
}
@article{Wilson2018,
abstract = {Digital communication technologies play an increasingly prominent role in humanitarian operations and in response to international pandemics specifically. A burgeoning body of scholarship on the topic displays high expectations for such tools to increase the efficiency of pandemic response. This article reviews empirical uses of communications technology in humanitarian and pandemic response, and the 2014 Ebola response in particular, in order to propose a three-part conceptual model for the new informatics of pandemic response. This model distinguishes between the use of digital communication tools for diagnostic, risk communication, and coordination activities and highlights how the influx of novel actors and tendencies towards digital and operational convergence risks focusing humanitarian action and decision-making outside national authorities' spheres of influence in pandemic response. This risk exacerbates a fundamental tension between the humanitarian promise of new technologies and the fundamental norm that international humanitarian response should complement and give primacy to the role of national authorities when possible. The article closes with recommendations for ensuring the inclusion of roles and agency for national authorities in technology-supported communication processes for pandemic response.},
author = {Wilson, Christopher and Jumbert, Maria Gabrielsen},
doi = {10.1186/s41018-018-0036-5},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/s41018-018-0036-5.pdf:pdf},
isbn = {4101801800},
issn = {2364-3412},
journal = {Journal of International Humanitarian Action},
keywords = {Humanitarian,Pandemics,ICTs,New media,Informatics,,global networks,humanitarian,icts,informatics,new media,pandemics,risk communication},
number = {1},
publisher = {Journal of International Humanitarian Action},
title = {{The new informatics of pandemic response: humanitarian technology, efficiency, and the subtle retreat of national agency}},
volume = {3},
year = {2018}
}
@article{Chang2020,
abstract = {In this paper we develop an agent-based model for a fine-grained computational simulation of the ongoing COVID-19 pandemic in Australia. This model is calibrated to reproduce several characteristics of COVID-19 transmission, accounting for its reproductive number, the length of incubation and generation periods, age-dependent attack rates, and the growth rate of cumulative incidence during a sustained and unmitigated local transmission. An important calibration outcome is the age-dependent fraction of symptomatic cases, with this fraction for children found to be one-fifth of such fraction for adults. We then apply the model to compare several intervention strategies, including restrictions on international air travel, case isolation, social distancing with varying levels of compliance, and school closures. School closures are not found to bring decisive benefits. We report an important transition across the levels of social distancing compliance, in the range between 70{\%} and 80{\%} levels. This suggests that a compliance of below 70{\%} is unlikely to succeed for any duration of social distancing, while a compliance at the 90{\%} level is likely to control the disease within 13-14 weeks, when coupled with effective case isolation and international travel restrictions.},
archivePrefix = {arXiv},
arxivId = {2003.10218},
author = {Chang, Sheryl L. and Harding, Nathan and Zachreson, Cameron and Cliff, Oliver M. and Prokopenko, Mikhail},
eprint = {2003.10218},
file = {::},
month = {mar},
title = {{Modelling transmission and control of the COVID-19 pandemic in Australia}},
url = {http://arxiv.org/abs/2003.10218},
year = {2020}
}
@article{Beaumont:2010,
author = {Beaumont, M A},
journal = {Annual Review Ecology, Evolution and Systematics},
pages = {379--406},
title = {{Approximate Bayesian Computation in Evolution and Ecology}},
volume = {41},
year = {2010}
}
@article{Corominas-Murtra2018,
abstract = {A major problem for evolutionary theory is understanding the so-called open-ended nature of evolutionary change, from its definition to its origins. Open-ended evolution (OEE) refers to the unbounded increase in complexity that seems to characterize evolution on multiple scales. This property seems to be a characteristic feature of biological and technological evolution and is strongly tied to the generative potential associated with combinatorics, which allows the system to grow and expand their available state spaces. Interestingly, many complex systems presumably displaying OEE, from language to proteins, share a common statistical property: the presence of Zipf's Law. Given an inventory of basic items (such as words or protein domains) required to build more complex structures (sentences or proteins) Zipf's Law tells us that most of these elements are rare whereas a few of them are extremely common. Using algorithmic information theory, in this paper we provide a fundamental definition for open-endedness, which can be understood as postulates. Its statistical counterpart, based on standard Shannon information theory, has the structure of a variational problem which is shown to lead to Zipf's Law as the expected consequence of an evolutionary process displaying OEE. We further explore the problem of information conservation through an OEE process and we conclude that statistical information (standard Shannon information) is not conserved, resulting in the paradoxical situation in which the increase of information content has the effect of erasing itself. We prove that this paradox is solved if we consider non-statistical forms of information. This last result implies that standard information theory may not be a suitable theoretical framework to explore the persistence and increase of the information content in OEE systems.},
archivePrefix = {arXiv},
arxivId = {1612.01605},
author = {Corominas-Murtra, Bernat and Seoane, Lu{\'{i}}s F. and Sol{\'{e}}, Ricard},
doi = {10.1098/rsif.2018.0395},
eprint = {1612.01605},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/rsif.2018.0395.pdf:pdf},
issn = {17425662},
journal = {Journal of the Royal Society Interface},
keywords = {Zipf's Law,algorithmic complexity,complexity,open-ended evolution},
number = {149},
title = {{Zipf's Law, unbounded complexity and open-ended evolution}},
volume = {15},
year = {2018}
}
@article{Grelaudetal:2009,
author = {{Grelaud A.} and {Robert C. P.} and {Marin J-M.} and {Rodolphe F.} and Taly, J-F.},
journal = {Bayesian Analysis},
pages = {317--336},
title = {{{\{}ABC{\}} likelihood-free methods for model choice in {\{}G{\}}ibbs random fields}},
url = {https://projecteuclid.org/euclid.ba/1340370280},
volume = {4},
year = {2009}
}
@article{Dutta2018,
abstract = {Infectious diseases are studied to understand their spreading mechanisms, to evaluate control strategies and to predict the risk and course of future outbreaks. Because people only interact with few other individuals, and the structure of these interactions influence spreading processes, the pairwise relationships between individuals can be usefully represented by a network. Although the underlying transmission processes are different, the network approach can be used to study the spread of pathogens in a contact network or the spread of rumours in a social network. We study simulated simple and complex epidemics on synthetic networks and on two empirical networks, a social/contact network in an Indian village and an online social network. Our goal is to learn simultaneously the spreading process parameters and the first infected node, given a fixed network structure and the observed state of nodes at several time points. Our inference scheme is based on approximate Bayesian computation, a likelihood-free inference technique. Our method is agnostic about the network topology and the spreading process. It generally performs well and, somewhat counter-intuitively, the inference problem appears to be easier on more heterogeneous network topologies, which enhances its future applicability to real-world settings where few networks have homogeneous topologies.},
archivePrefix = {arXiv},
arxivId = {1709.08862},
author = {Dutta, Ritabrata and Mira, Antonietta and Onnela, Jukka Pekka},
doi = {10.1098/rspa.2018.0129},
eprint = {1709.08862},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1709.08862.pdf:pdf},
issn = {14712946},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Approximate Bayesian computation,Bayesian inference,Epidemics,Network,Spreading process},
number = {2215},
pages = {1--24},
title = {{Bayesian inference of spreading processes on networks}},
volume = {474},
year = {2018}
}
@manual{aito,
title = {{Aito}},
url = {https://aito.ai/}
}
@article{Schmidhuber:2015,
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
issn = {08936080},
journal = {Neural Networks},
month = {jan},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014002135},
volume = {61},
year = {2015}
}

@Article{RePEc,
  author={Maisonobe, Marion and Eckert, Denis and Grossetti, Michel and Jégou, Laurent and Milard, Béatrice},
  title={{The world network of scientific collaborations between cities: domestic or international dynamics?}},
  journal={Journal of Informetrics},
  year=2016,
  volume={10},
  number={4},
  pages={1025-1036},
  month={},
  keywords={Scientific publications; Co-authorship; Research collaboration; Network of cities; Geography of scie},
  doi={10.1016/j.joi.2016.06.002},
  abstract={An earlier publication (Grossetti et al., 2014) has established that we are attending a decreasing concentration of scientific activities within “world-cities”. Given that more and more cities and countries are contributing to the world production of knowledge, this article analyses the evolution of the world collaboration network both at the domestic and international levels during the 2000s. Using data from the Science Citation Index Expanded, scientific authors’ addresses are geo-localized and grouped by urban areas. Our data suggests that interurban collaborations within countries increased together with international linkages. In most countries, domestic collaborations increased faster than international collaborations. Even among the top collaborating cities, sometimes referred to as “world cities”, the share of domestic collaborations has gained momentum. Our results suggest that, contrary to common beliefs about the globalization process, national systems of research have been strengthening during the 2000s.},
  url={https://ideas.repec.org/a/eee/infome/v10y2016i4p1025-1036.html}
}


@article{Golem2016,
author = {Golem},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Golem-white-paper.pdf:pdf},
journal = {Golem.Network},
number = {November},
pages = {1--28},
title = {{The Golem Project Crowdfunding Whitepaper}},
url = {https://golem.network/crowdfunding/Golemwhitepaper.pdf},
year = {2016}
}
@article{Siegenfeld2020,
abstract = {We analyze the spread of COVID-19 by considering the transmission of the disease among individuals both within and between communities. A set of communities can be defined as any partition of a population such that travel/social contact within each community far exceeds that between them (e.g. the U.S. could be partitioned by state or commuting zone boundaries). COVID-19 can be eliminated if the community-to-community reproductive number---i.e. the expected/average number of other communities to which a single infected community will transmit the virus---is reduced to less than one. We find that this community-to-community reproductive number is proportional to the travel rate between communities and exponential in the length of the time-delay before community-level action is taken. Thus, reductions in travel and the speed at which communities take action can play decisive roles in stopping the outbreak. The analysis suggests that for the coronavirus to be eliminated, it is not necessary to impose aggressive social distancing measures all over the world at once, but rather only in communities in which active spreading is detected. The sooner such measures are imposed, the shorter the duration they must remain in place. If infected communities (including those that become re-infected in the future) are quick enough to act, the number of actively infected communities (and thus the number of communities in which such measures are required) will exponentially decrease over time.},
archivePrefix = {arXiv},
arxivId = {2003.10086},
author = {Siegenfeld, Alexander F. and Bar-Yam, Yaneer},
eprint = {2003.10086},
file = {::},
month = {mar},
title = {{Eliminating COVID-19: A Community-based Analysis}},
url = {http://arxiv.org/abs/2003.10086},
year = {2020}
}
@article{Inhaber1977,
abstract = {An important aspect of the ‘science of science' is its geography. One way of studying this aspect is to evaluate the degree of dispersion or centralization of scientists. The calculation has been performed for the 16 countries with the greatest number of scientists. Czechoslovakia and Sweden have the highest concentration in a few cities, while the United States and the Federal Republic of Germany have the lowest concentration. In general, the degree of concentration varies inversely with the total number of scientists in a country, although there are exceptions. Part of the ‘concentration effect' is due to a large number of scientists in one city, usually the capital. When the effect of the leading city is removed, the ranking of the countries changes sharply. Switzerland and Australia have the highest concentrations, and the United States and the United Kingdom have the lowest. Only a weak relationship is then shown between the total number of scientists and the degree of concentration. To remove the effect of population concentration, the cumulative proportion of scientists was divided by the cumulative proportion of population for the leading cities. This produced considerable change in the rankings by nation. The USSR and India have the highest proportions. Countries with low proportions included the United States, Canada, the United Kingdom and Australia. Almost all countries had a greater degree of dispersion in 1972 as compared with 1967. The greatest changes towards dispersion were seen in India and the USSR.},
author = {Inhaber, H.},
doi = {10.1016/0048-7333(77)90024-5},
issn = {0048-7333},
journal = {Research Policy},
month = {apr},
number = {2},
pages = {178--193},
publisher = {North-Holland},
title = {{Changes in centralization of science}},
url = {https://www.sciencedirect.com/science/article/abs/pii/0048733377900245},
volume = {6},
year = {1977}
}
@manual{datarobot,
title = {{Datarobot}},
url = {https://www.datarobot.com/}
}
@article{Clark2013,
abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this hierarchical prediction machine approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency. {\textcopyright} 2013 Cambridge University Press.},
author = {Clark, Andy},
doi = {10.1017/S0140525X12000477},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/whatever-next-predictive-brains-situated-agents-and-the-future-of-cognitive-science.pdf:pdf},
issn = {14691825},
journal = {Behavioral and Brain Sciences},
keywords = {Bayesian brain,action,attention,expectation,generative model,hierarchy,perception,precision,prediction,prediction error,predictive coding,top-down processing},
number = {3},
pages = {181--204},
pmid = {23663408},
title = {{Whatever next? Predictive brains, situated agents, and the future of cognitive science}},
volume = {36},
year = {2013}
}
@article{Bianconi2020,
abstract = {Here we focus on the data analysis of the growth of epidemic spreading of Covid-19 in countries where different policies of containment have been activated. It is known that the growth of the pandemic spreading at its threshold is exponential but it is not known how to quantify the success of different containment policies. We identify that a successful approach gives an arrested phase regime following the Ostwald growth, where in the course of time one phase transforms into another metastable phase with a similar free energy as observed in oxygen interstitials diffusion in quantum complex matter and in crystallization of proteins. We introduce the s factor which provides a quantitative measure of the efficiency and speed of the adopted containment policy, which is very helpful not only to monitor the Covid-19 pandemic spreading but also for other countries to choose the best containment policy. The results show that the policy based in joint confinement, targeted tests and tracking positive cases is the most rapid pandemic containment policy in fact we have found for China, South Korea, and Italy the values of the success s factor 9, 5, 32 respectively where the lowest s value indicates the best containment policy.},
archivePrefix = {arXiv},
arxivId = {2003.08868},
author = {Bianconi, Antonio and Marcelli, Augusto and Campi, Gaetano and Perali, Andrea},
eprint = {2003.08868},
file = {::},
month = {mar},
title = {{Ostwald growth rate in controlled Covid-19 epidemic spreading as in arrested growth in quantum complex matter}},
url = {http://arxiv.org/abs/2003.08868},
year = {2020}
}
@article{Dufresne2016,
abstract = {A hierarchy of timescales is ubiquitous in biological systems, where enzymatic reactions play an important role because they can hasten the relaxation to equilibrium. We introduced a statistical physics model of interacting spins that also incorporates enzymatic reactions to extend the classic model for allosteric regulation. Through Monte Carlo simulations and theoretical analysis, we found that the relaxation dynamics are much slower than the elementary reactions and are logarithmic in time with several plateaus, as is commonly observed for glasses. This is because of the kinetic constraints from the cooperativity via the competition between enzymes, which tend to bind to molecules with the limited structure. This inhibits the progress of the modification reaction. Our model showed symmetry breaking in the relaxation trajectories that led to inherently kinetic phase transitions without any correspondence to the equilibrium state. In this paper, we discuss the relevance of these results for diverse responses in biology.},
archivePrefix = {arXiv},
arxivId = {arXiv:2001.04385v1},
author = {Dufresne, Alexis and Labasque, Thierry and Bochet, Olivier and Borgne, Tanguy Le and Bethencourt, Lorine and Pedrot, Mathieu and Lavenant, Nicolas and Petton, Christophe and Aquilina, Luc},
doi = {10.1073/pnas.XXXXXXXXXX},
eprint = {arXiv:2001.04385v1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/2001.04385.pdf:pdf},
journal = {Manuscript},
keywords = {1,keyword 2,keyword 3},
number = {Xx},
pages = {1--8},
title = {{Test Short-circuiting of hydrological pathways by fractures triggers microbial hotspots in the subsurface}},
volume = {XXX},
year = {2016}
}
@article{Durov2017,
author = {Durov, Nikolai},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/TON Technology.pdf:pdf},
pages = {1--132},
title = {{Telegram Open Network}},
year = {2017}
}
@article{Kappel2015,
author = {Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
doi = {10.1371/journal.pcbi.1004485},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/journal.pcbi.1004485.PDF:PDF},
pages = {1--31},
title = {{Network Plasticity as Bayesian Inference}},
year = {2015}
}
@article{Cristian1989,
abstract = {Abstract A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock ... $\backslash$n},
author = {Cristian, Flaviu},
doi = {10.1007/BF01784024},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Clock synchronization,Communication,Distributed system,Fault-tolerance,Time service},
number = {3},
pages = {146--158},
title = {{Probabilistic clock synchronization}},
volume = {3},
year = {1989}
}
@article{Maass2014,
abstract = {We are used to viewing noise as a nuisance in computing systems. This is a pity, since noise will be abundantly available in energy-efficient future nanoscale devices and circuits. I propose here to learn from the way the brain deals with noise, and apparently even benefits from it. Recent theoretical results have provided insight into how this can be achieved: how noise enables networks of spiking neurons to carry out probabilistic inference through sampling and also enables creative problem solving. In addition, noise supports the self-organization of networks of spiking neurons, and learning from rewards. I will sketch here the main ideas and some consequences of these results. I will also describe why these results are paving the way for a qualitative jump in the computational capability and learning performance of neuromorphic networks of spiking neurons with noise, and for other future computing systems that are able to treat noise as a resource. {\textcopyright} 2014 IEEE.},
author = {Maass, Wolfgang},
doi = {10.1109/JPROC.2014.2310593},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/10.1.1.701.7037.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Computational power,Stochastic computing,neural networks,neuromorphic hardware,noise,self-organization,spiking neurons},
number = {5},
pages = {860--880},
title = {{Noise as a resource for computation and learning in networks of spiking neurons}},
volume = {102},
year = {2014}
}
@article{Meshulam2018,
abstract = {We develop a phenomenological coarse--graining procedure for activity in a large network of neurons, and apply this to recordings from a population of 1000+ cells in the hippocampus. Distributions of coarse--grained variables seem to approach a fixed non--Gaussian form, and we see evidence of scaling in both static and dynamic quantities. These results suggest that the collective behavior of the network is described by a non--trivial fixed point.},
archivePrefix = {arXiv},
arxivId = {1809.08461},
author = {Meshulam, Leenoy and Gauthier, Jeffrey L and Brody, Carlos D and Tank, David W and Bialek, William},
doi = {10.1103/PhysRevLett.123.178103},
eprint = {1809.08461},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1809.08461.pdf:pdf},
month = {sep},
pages = {1--5},
title = {{Coarse--graining, fixed points, and scaling in a large population of neurons}},
url = {http://arxiv.org/abs/1809.08461 http://dx.doi.org/10.1103/PhysRevLett.123.178103},
year = {2018}
}
@article{Enserink2020,
author = {Enserink, Martin},
doi = {10.1126/science.abb8814},
file = {::},
issn = {0036-8075},
journal = {Science},
month = {mar},
pages = {1--8},
title = {{Mathematics of life and death: How disease models shape national shutdowns and other pandemic policies}},
url = {https://www.sciencemag.org/news/2020/03/mathematics-life-and-death-how-disease-models-shape-national-shutdowns-and-other},
year = {2020}
}
@article{Bradde2017,
abstract = {A system with many degrees of freedom can be characterized by a covariance matrix; principal components analysis focuses on the eigenvalues of this matrix, hoping to find a lower dimensional description. But when the spectrum is nearly continuous, any distinction between components that we keep and those that we ignore becomes arbitrary; it then is natural to ask what happens as we vary this arbitrary cutoff. We argue that this problem is analogous to the momentum shell renormalization group. Following this analogy, we can define relevant and irrelevant operators, where the role of dimensionality is played by properties of the eigenvalue density. These results also suggest an approach to the analysis of real data. As an example, we study neural activity in the vertebrate retina as it responds to naturalistic movies, and find evidence of behavior controlled by a nontrivial fixed point. Applied to financial data, our analysis separates modes dominated by sampling noise from a smaller but still macroscopic number of modes described by a non-Gaussian distribution.},
archivePrefix = {arXiv},
arxivId = {1610.09733},
author = {Bradde, Serena and Bialek, William},
doi = {10.1007/s10955-017-1770-6},
eprint = {1610.09733},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Bradde-Bialek2017{\_}Article{\_}PCAMeetsRG.pdf:pdf},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Financial markets,Neural networks,Renormalization group},
number = {3-4},
pages = {462--475},
title = {{PCA Meets RG}},
volume = {167},
year = {2017}
}
@manual{APISGURU,
title = {{APIs}},
url = {https://apis.guru/openapi-directory/}
}
@article{Walters2011,
abstract = {In this paper we survey the literature on the Black-Litterman model. This survey is provided both as a chronology and a taxonomy as there are many claims on the model in the literature. We provide a complete description of the canonical model including full derivations from the underlying principles using both Theil's Mixed Estimation model and Bayes Theory. The various parameters of the model are considered, along with information on their computation or calibration. Further consideration is given to several of the key papers, with worked examples illustrating the concepts.},
author = {Walters, Jay},
doi = {10.2139/ssrn.1314585},
isbn = {1556-5068},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{The Black-Litterman Model in Detail}},
url = {http://www.ssrn.com/abstract=1314585},
year = {2011}
}
@article{Melniketal:2010,
author = {{Melnik S.} and {Gubarev A.} and {Long J. J.} and {Romer G.} and {Shivakumar S.} and {Tolton M.} and Vassilakis, T},
journal = {Proc. of the 36th International Conference on Very Large Data Bases (VLDB)},
pages = {159--176},
title = {{Dremel: Interactive Analysis of Web-Scale Datasets}},
year = {2010}
}
@article{Mastrangelo2019,
abstract = {Regional and global assessments periodically update what we know, and highlight what remains to be known, about the linkages between people and nature that both define and depend upon the state of the environment. To guide research that better informs policy and practice, we systematically synthesize knowledge gaps from recent assessments of four regions of the globe and three key themes by the Intergovernmental Science-Policy Platform for Biodiversity and Ecosystem Services. We assess their relevance to global sustainability goals and trace their evolution relative to those identified in the Millennium Ecosystem Assessment. We found that global sustainability goals cannot be achieved without improved knowledge on feedbacks between social and ecological systems, effectiveness of governance systems and the influence of institutions on the social distribution of ecosystem services. These top research priorities have persisted for the 14 years since the Millennium Ecosystem Assessment. Our analysis also reveals limited understanding of the role of indigenous and local knowledge in sustaining nature's benefits to people. Our findings contribute to a policy-relevant and solution-oriented agenda for global, long-term social-ecological research.},
author = {Mastr{\'{a}}ngelo, Mat{\'{i}}as E and P{\'{e}}rez-Harguindeguy, Natalia and Enrico, Lucas and Bennett, Elena and Lavorel, Sandra and Cumming, Graeme S and Abeygunawardane, Dilini and Amarilla, Leonardo D and Burkhard, Benjamin and Egoh, Benis N and Frishkoff, Luke and Galetto, Leonardo and Huber, Sibyl and Karp, Daniel S and Ke, Alison and Kowaljow, Esteban and Kronenburg-Garc{\'{i}}a, Angela and Locatelli, Bruno and Mart{\'{i}}n-L{\'{o}}pez, Berta and Meyfroidt, Patrick and Mwampamba, Tuyeni H and Nel, Jeanne and Nicholas, Kimberly A and Nicholson, Charles and Oteros-Rozas, Elisa and Rahlao, Sebataolo J and Raudsepp-Hearne, Ciara and Ricketts, Taylor and Shrestha, Uttam B and Torres, Carolina and Winkler, Klara J and Zoeller, Kim},
doi = {10.1038/s41893-019-0412-1},
issn = {2398-9629},
journal = {Nature Sustainability},
number = {12},
pages = {1115--1121},
title = {{Key knowledge gaps to achieve global sustainability goals}},
url = {https://doi.org/10.1038/s41893-019-0412-1},
volume = {2},
year = {2019}
}
@article{Hensman2015,
abstract = {In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.1605v2},
author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
doi = {10.1109/TPAMI.2014.2318711},
eprint = {arXiv:1401.1605v2},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {383--393},
title = {{Fast nonparametric clustering of structured time-series}},
volume = {37},
year = {2015}
}
@article{Ghahramani:2015,
author = {Ghahramani, Z},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Ghahramani 2015 Nature.pdf:pdf},
journal = {Nature},
pages = {452--459},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@article{Teles2020,
abstract = {The new coronavirus covid-19 has spread very quickly worldwide, leading the World Health Organization (WHO) to declare a state of pandemic. One of the questions many policy makers, and governments are asking themselves is how the spread is going to evolve in time. In this study, I applied an adapted SIR model previously used in South Korea to model the MERS outbreak, which is also caused by a coronavirus, to estimate the evolution of the curve of active cases in the case of the Portuguese situation, using Italian dara. I then construct five different scenarios for the evolution of covid-19 in Portugal. In the out of control scenario, the number of active cases could reach as much as {\~{}}40,000 people on the 5th of April (out-of-control scenario). If the self-protective and control are taken in the same level as what was considered for the South Korean model, this number could have be reduced to about 800 cases (scenario 1). Considering that this scenario is now unrealistic, three other scenarios were devised. In all these scenarios, the government measures had a 50$\backslash${\%} effectiveness when compared to the measures in Korea. But in scenario 2 the transmission rate {\$}\backslashbeta{\$} was effectively reduced to 50$\backslash${\%}, In this scenario active cases could reach circa 7,000 people. In scenario 3, the transmission rate {\$}\backslashbeta{\$} was reduced to 70$\backslash${\%} of its initial value, in which the number of cases would reach a peak of {\~{}}11,000 people. And finally in scenario 4, {\$}\backslashbeta{\$} was reduced to 80$\backslash${\%}. In this scenario, the peak would be reached at about {\~{}}13,000 cases. This study shows the importance of control and self-protecting measure to bring down the number of affected people by following the recommendations of the WHO and health authorities. With the appropriate measures, this number can be brought down to {\~{}}7,000-13,000 people. Hopefully that will be the case not just in Portugal, but in the rest of the World.},
archivePrefix = {arXiv},
arxivId = {2003.10047},
author = {Teles, Pedro},
eprint = {2003.10047},
file = {::},
month = {mar},
title = {{Predicting the evolution Of SARS-Covid-2 in Portugal using an adapted SIR Model previously used in South Korea for the MERS outbreak}},
url = {http://arxiv.org/abs/2003.10047},
year = {2020}
}
@article{Lopez2020,
archivePrefix = {arXiv},
arxivId = {2003.10359},
author = {Lopez, Christian E. and Vasu, Malolan and Gallemore, Caleb},
eprint = {2003.10359},
file = {::},
month = {mar},
title = {{Understanding the perception of COVID-19 policies by mining a multilanguage Twitter dataset}},
year = {2020}
}
@manual{eureqa,
title = {{Eureqa}},
url = {https://www.nutonian.com/products/eureqa/}
}
@article{Gael2018,
abstract = {We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.},
author = {Gael, Jurgen Van},
doi = {10.7551/mitpress/1120.003.0079},
file = {::},
isbn = {0-262-04208-8},
journal = {Advances in Neural Information Processing Systems 14},
pages = {1--8},
title = {{The Infinite Hidden Markov Model}},
year = {2018}
}
@book{Anderson,
author = {Anderson, Carl},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Creating a Data-Driven Organization.pdf:pdf},
isbn = {9781491916919},
title = {{Creating a Organization}}
}
@article{Gaoetal:2012,
author = {{Gao J.} and {Buldyrev S. V.} and {Stanley H. E.} and Havlin, S},
journal = {Nature physics},
pages = {40--48},
title = {{Networks formed from interdependent networks}},
volume = {8},
year = {2012}
}
@article{Gunther2018,
abstract = {Technology evolves faster than ever, with the pace picking up with every passing year. Unprecedented in history, we have the greatest and brightest minds driving unparalleled change. Progress isn't a trend that naturally happens on its own - it requires an ever growing number of researchers and experts. The scientific community of today doesn't get what it deserves, constantly struggling with obtaining funding and work- ing endless hours with little to no reimbursement. It is our stated mission objective to empower those who empower us all by establishing Science- root, the first blockchain-based scientific ecosystem to integrate a social media scientific network, a funding platform and a decentralized publish- ing framework for journals.},
author = {G{\"{u}}nther, Vlad and Alexandru Chirita},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/whitepaperScienceRoot.pdf:pdf},
title = {{" Scienceroot " Whitepaper}},
url = {https://www.scienceroot.com/},
year = {2018}
}
@article{Sheehan2016,
abstract = {Given genomic variation data from multiple individuals, computing the likelihood of complex population genetic models is often infeasible. To circumvent this problem, we introduce a novel likelihood-free inference framework by applying deep learning, a powerful modern technique in machine learning. Deep learning makes use of multilayer neural networks to learn a feature-based function from the input (e.g., hundreds of correlated summary statistics of data) to the output (e.g., population genetic parameters of interest). We demonstrate that deep learning can be effectively employed for population genetic inference and learning informative features of data. As a concrete application, we focus on the challenging problem of jointly inferring natural selection and demography (in the form of a population size change history). Our method is able to separate the global nature of demography from the local nature of selection, without sequential steps for these two factors. Studying demography and selection jointly is motivated by Drosophila, where pervasive selection confounds demographic analysis. We apply our method to 197 African Drosophila melanogaster genomes from Zambia to infer both their overall demography, and regions of their genome under selection. We find many regions of the genome that have experienced hard sweeps, and fewer under selection on standing variation (soft sweep) or balancing selection. Interestingly, we find that soft sweeps and balancing selection occur more frequently closer to the centromere of each chromosome. In addition, our demographic inference suggests that previously estimated bottlenecks for African Drosophila melanogaster are too extreme.},
author = {Sheehan, Sara and Song, Yun S},
doi = {10.1371/journal.pcbi.1004845},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Approximate Bayesian computation,Artificial intelligence,Balancing selection,Bioinformatics,Biology,Deep learning,Genetics,Inference,Natural selection,Population,Population genetics,Population size},
month = {mar},
number = {3},
pages = {e1004845},
pmid = {27018908},
title = {{Deep learning for population genetic inference}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27018908 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4809617},
volume = {12},
year = {2016}
}
@article{Schwarz2016,
author = {Schwarz, Max and Beul, Marius and Droeschel, David and Sch, Sebastian and Periyasamy, Selvam and Lenz, Christian and Schreiber, Michael and Behnke, Sven},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/479363{\_}De{\_}Silva{\_}Manuscript.PDF:PDF},
journal = {Frontiers in Robotics and AI},
keywords = {manipulation,mapping,mobile manipulation,navigation,perception for grasping and,space robotics and automation},
title = {{In w e i v re In w e i v re}},
year = {2016}
}
@article{Guimera2020,
abstract = {Closed-form, interpretable mathematical models have been instrumental for advancing our understanding of the world; with the data revolution, we may now be in a position to uncover new such models for many systems from physics to the social sciences. However, to deal with increasing amounts of data, we need “machine scientists” that are able to extract these models automatically from data. Here, we introduce a Bayesian machine scientist, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions. It explores the space of models using Markov chain Monte Carlo. We show that this approach uncovers accurate models for synthetic and real data and provides out-of-sample predictions that are more accurate than those of existing approaches and of other nonparametric methods.},
author = {Guimer{\`{a}}, Roger and Reichardt, Ignasi and Aguilar-Mogas, Antoni and Massucci, Francesco A. and Miranda, Manuel and Pallar{\`{e}}s, Jordi and Sales-Pardo, Marta},
doi = {10.1126/sciadv.aav6971},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/aav6971{\_}SM.pdf:pdf},
issn = {23752548},
journal = {Science Advances},
number = {5},
pages = {eaav6971},
title = {{A Bayesian machine scientist to aid in the solution of challenging scientific problems}},
volume = {6},
year = {2020}
}
@manual{easeml,
title = {{EaseML}},
url = {https://github.com/DS3Lab/easeml}
}
@unpublished{Bailey2013,
abstract = {We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.},
author = {Bailey, David H. and Borwein, Jonathan and {Lopez de Prado}, Marcos and Zhu, Qiji Jim},
booktitle = {SSRN},
doi = {10.2139/ssrn.2308659},
issn = {0002-9920},
keywords = {E44,G0,G1,G15,G2,G24,Sharpe ratio,backtest,historical simulation,investment strategy,minimum backtest length,optimization,performance degradation,probability of backtest over-fitting},
title = {{Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance}},
year = {2013}
}
@article{Cranmer2019,
abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving new momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound change these developments may have on science.},
archivePrefix = {arXiv},
arxivId = {1911.01429},
author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
eprint = {1911.01429},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1911.01429v2.pdf:pdf},
pages = {1--10},
title = {{The frontier of simulation-based inference}},
url = {http://arxiv.org/abs/1911.01429},
year = {2019}
}
@article{SobolevAlexeyandSchneider2019,
author = {{Sobolev, Alexey and Schneider}, Lisa},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Industry{\_}Marketplace{\_}Technical{\_}Documentation.pdf:pdf},
title = {{Industry Marketplace Technical Documentation}},
year = {2019}
}
@article{Reichstein,
abstract = {Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning. H umans have always striven to predict and understand the world, and the ability to make better predictions has given competitive advantages in diverse contexts (such as weather, diseases or financial markets). Yet the tools for prediction have substantially changed over time, from ancient Greek philosophical reasoning to non-scientific medieval methods such as soothsaying, towards modern scientific discourse , which has come to include hypothesis testing, theory development and computer modelling underpinned by statistical and physical relationships, that is, laws 1. A success story in the geosciences is weather prediction, which has greatly improved through the integration of better theory, increased computational power, and established observational systems, which allow for the assimilation of large amounts of data into the modelling system 2. Nevertheless, we can accurately predict the evolution of the weather on a timescale of days, not months. Seasonal meteorological predictions, forecasting extreme events such as flooding or fire, and long-term climate projections are still major challenges. This is especially true for predicting dynamics in the biosphere, which is dominated by biologically mediated processes such as growth or reproduction, and is strongly controlled by seemingly stochastic disturbances such as fires and landslides. Such predictive problems have not seen much progress in the past few decades 3. At the same time, a deluge of Earth system data has become available, with storage volumes already well beyond dozens of petabytes and rapidly increasing transmission rates exceeding hundreds of terabytes per day 4. These data come from a plethora of sensors measuring states, fluxes and intensive or time/space-integrated variables, representing fifteen or more orders of temporal and spatial magnitude. They include remote sensing from a few metres to hundreds of kilometres above Earth as well as in situ observations (increasingly from autonomous sensors) at and below the surface and in the atmosphere, many of which are further being complemented by citizen science observations. Model simulation output adds to this deluge; the CMIP-5 dataset of the Climate Model Intercomparison Project, used extensively for scientific groundwork towards periodic climate assessments, is over 3 petabytes in size, and the next generation, CMIP-6, is estimated to reach up to 30 petabytes 5. The data from models share many of the challenges and statistical properties of observational data, including many forms of uncertainty. In summary, Earth system data are exemplary of all four of the 'four Vs' of 'big data': volume, velocity, variety and veracity (see Fig. 1). One key challenge is to extract interpret-able information and knowledge from this big data, possibly almost in real time and integrating between disciplines. Taken together, our ability to collect and create data far outpaces our ability to sensibly assimilate it, let alone understand it. Predictive ability in the last few decades has not increased apace with data availability. To get the most out of the explosive growth and diversity of Earth system data, we face two major tasks in the coming years: (1) extracting knowledge from the data deluge, and (2) deriving models that learn much more from data than traditional data assimilation approaches can, while still respecting our evolving understanding of nature's laws. The combination of unprecedented data sources, increased computational power, and the recent advances in statistical modelling and machine learning offer exciting new opportunities for expanding our knowledge about the Earth system from data. In particular, many tools are available from the fields of machine learning and artificial intelligence, but they need to be further developed and adapted to geo-scientific analysis. Earth system science offers new opportunities, challenges and methodological demands, in particular for recent research lines focusing on spatio-temporal context and uncertainties (Box 1; see https://developers. google.com/machine-learning/glossary/ and http://www.wildml.com/ deep-learning-glossary/ for more complete glossaries). In the following sections we review the development of machine learning in the geoscientific context, and highlight how deep learning-that is, the automatic extraction of abstract (spatio-temporal) features-has the potential to overcome many of the limitations that have, until now, hindered a more widespread adoption of machine learning. We further lay out the most promising but also challenging approaches in combining machine learning with physical modelling. State-of-the-art geoscientific machine learning Machine learning is now a successful part of several research-driven and operational geoscientific processing schemes, addressing the atmosphere, the land surface and the ocean, and has co-evolved with data availability over the past decade. Early landmarks in classification of land cover and clouds emerged almost 30 years ago through the coincidence of high-resolution satellite data and the first revival of neural networks 6,7. Most major machine learning methodological},
author = {Reichstein, Markus and Camps-Valls, Gustau and Stevens, Bjorn and Jung, Martin and Denzler, Joachim and Carvalhais, Nuno and Prabhat, {\&}},
doi = {10.1038/s41586-019-0912-1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/s41586-019-0912-1.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
publisher = {Springer US},
title = {{Deep learning and process understanding for data-driven Earth system science}},
url = {www.nature.com/nature}
}
@article{Wang,
abstract = {BACKGROUND},
author = {Wang, Chaolong and Liu, Li and Hao, Xingjie and Guo, Huan and Wang, Qi and Huang, Jiao and He, Na and Yu, Hongjie and Lin, Xihong and Pan, An and Wei, Sheng and Wu, Tangchun},
doi = {10.1101/2020.03.03.20030593},
file = {::},
title = {{Evolving Epidemiology and Impact of Non-pharmaceutical Interventions on the Outbreak of Coronavirus Disease 2019 in Wuhan, China From the Department of Epidemiology}},
url = {https://doi.org/10.1101/2020.03.03.20030593}
}
@book{PERKINS2009,
author = {PERKINS, JOHN},
booktitle = {Berrett-Koehler},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/john{\_}perkins{\_}confessions{\_}of{\_}an{\_}economic{\_}hit{\_}man.pdf:pdf},
isbn = {1576753018},
number = {6},
pages = {14--21},
title = {{Confession of an Economic Hitman}},
volume = {67},
year = {2009}
}
@article{DeDomenico:2014,
author = {{De Domenico M.} and {Porter M. A.} and Arenas, A},
journal = {Journal of Complex Networks},
pages = {159--176},
title = {{MuxViz: a tool for multilayer analysis and visualization of networks}},
volume = {3},
year = {2014}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P a},
doi = {10.1371/journal.pmed.0020124},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16060722},
volume = {2},
year = {2005}
}
@article{Soto-Valero2019,
abstract = {Maven artifacts are immutable: an artifact that is uploaded on Maven Central cannot be removed nor modified. The only way for developers to upgrade their library is to release a new version. Consequently, Maven Central accumulates all the versions of all the libraries that are published there, and applications that declare a dependency towards a library can pick any version. In this work, we hypothesize that the immutability of Maven artifacts and the ability to choose any version naturally support the emergence of software diversity within Maven Central. We analyze 1,487,956 artifacts that represent all the versions of 73,653 libraries. We observe that more than 30{\%} of libraries have multiple versions that are actively used by latest artifacts. In the case of popular libraries, more than 50{\%} of their versions are used. We also observe that more than 17{\%} of libraries have several versions that are significantly more used than the other versions. Our results indicate that the immutability of artifacts in Maven Central does support a sustained level of diversity among versions of libraries in the repository.},
archivePrefix = {arXiv},
arxivId = {1903.05394},
author = {Soto-Valero, Cesar and Benelallam, Amine and Harrand, Nicolas and Barais, Olivier and Baudry, Benoit},
doi = {10.1109/MSR.2019.00059},
eprint = {1903.05394},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1903.05394.pdf:pdf},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Dataset,Development history graph,Digital preservation,Free software,Mining software repositories,Open source software,Source code},
pages = {333--343},
title = {{The emergence of software diversity in maven central}},
volume = {2019-May},
year = {2019}
}
@article{BigchainDBGmbH2018,
abstract = {BigchainDB is software that has blockchain properties (e.g. decentral-ization, immutability, owner-controlled assets) and database properties (e.g. high transaction rate, low latency, indexing {\&} querying of structured data). It was first released—open source—in February 2016 and has been improved continuously ever since. BigchainDB version 2.0 makes signifi-cant improvements over previous versions. In particular, it is now Byzan-tine fault tolerant (BFT), so up to a third of the nodes can fail in any way, and the system will continue to agree on how to proceed. BigchainDB 2.0 is also production-ready for many use cases. In this paper, we review the design goals of BigchainDB 2.0 and how they were achieved, we explore some use cases, we show how BigchainDB fits into the overall decentral-ization ecosystem, we follow the life of a transaction to understand how BigchainDB 2.0 works, we note ways to try BigchainDB, we outline how you can contribute, and we summarize future plans.},
author = {{BigchainDB GmbH}},
doi = {10.1111/j.1365-2958.2006.05434.x},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/bigchaindb-whitepaper.pdf:pdf},
journal = {BigchainDB. The blockchain database.},
number = {May},
pages = {1--14},
title = {{BigchainDB: The blockchain database}},
url = {https://www.bigchaindb.com/whitepaper/bigchaindb-whitepaper.pdf},
year = {2018}
}
@article{Maslov2020,
abstract = {We estimate the growth in demand for ICU beds in Chicago during the emerging COVID-19 epidemic, using state-of-the-art computer simulations calibrated for the SARS-CoV-2 virus. The questions we address are these: (1) Will the ICU capacity in Chicago be exceeded, and if so by how much? (2) Can strong mitigation strategies, such as lockdown or shelter in place order, prevent the overflow of capacity? (3) When should such strategies be implemented? Our answers are as follows: (1) The ICU capacity may be exceeded by a large amount, probably by a factor of ten. (2) Strong mitigation can avert this emergency situation potentially, but even that will not work if implemented too late. (3) If the strong mitigation precedes April 1st, then the growth of COVID-19 can be controlled and the ICU capacity could be adequate. The earlier the strong mitigation is implemented, the greater the probability that it will be successful. After around April 1 2020, any strong mitigation will not avert the emergency situation. In Italy, the lockdown occurred too late and the number of deaths is still doubling every 2.3 days. It is difficult to be sure about the precise dates for this window of opportunity, due to the inherent uncertainties in computer simulation. But there is high confidence in the main conclusion that it exists and will soon be closed. Our conclusion is that, being fully cognizant of the societal trade-offs, there is a rapidly closing window of opportunity to avert a worst-case scenario in Chicago, but only with strong mitigation/lockdown implemented in the next week at the latest. If this window is missed, the epidemic will get worse and then strong mitigation/lockdown will be required after all, but it will be too late.},
archivePrefix = {arXiv},
arxivId = {2003.09564},
author = {Maslov, Sergei and Goldenfeld, Nigel},
eprint = {2003.09564},
file = {::},
month = {mar},
title = {{Window of Opportunity for Mitigation to Prevent Overflow of ICU capacity in Chicago by COVID-19}},
url = {http://arxiv.org/abs/2003.09564},
year = {2020}
}
@article{Hardwicke2018,
abstract = {{\textcopyright} 2018 The Authors. Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data ('analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25{\%} pre-policy to 136/174, 78{\%} post-policy), although not all data appeared reusable (23/104, 22{\%} pre-policy to 85/136, 62{\%}, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10{\%} margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
annote = {doi: 10.1098/rsos.180448},
author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Mohr, Alicia Hofelich and Clayton, Elizabeth and Yoon, Erica J. and Tessler, Michael Henry and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
doi = {10.1098/rsos.180448},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {Interrupted time series,Journal policy,Meta-science,Open data,Open science,Reproducibility},
month = {sep},
number = {8},
pages = {180448},
publisher = {Royal Society},
title = {{Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal Cognition}},
url = {https://doi.org/10.1098/rsos.180448},
volume = {5},
year = {2018}
}
@article{Seoane2016,
author = {Seoane, F},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/tlfs.pdf:pdf},
pages = {1--312},
title = {{Multiobjective Optimization in Models of Synthetic and Natural Living Systems Lu ´}},
year = {2016}
}
@article{Mehrabi2019,
abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
archivePrefix = {arXiv},
arxivId = {1908.09635},
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
eprint = {1908.09635},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1908.09635.pdf:pdf},
title = {{A Survey on Bias and Fairness in Machine Learning}},
url = {http://arxiv.org/abs/1908.09635},
year = {2019}
}
@article{Araujo2020,
author = {Ara{\'{u}}jo, Miguel B and Naimi, Babak},
doi = {10.1101/2020.03.12.20034728},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/2020.03.12.20034728v1.full.pdf:pdf},
pages = {1--15},
title = {{Spread of SARS-CoV-2 Coronavirus likely to be constrained by climate}},
url = {https://www.medrxiv.org/content/10.1101/2020.03.12.20034728v1},
year = {2020}
}
@manual{OpenKnowledgeFoundation,
title = {{Open Knowledge Foundation}},
url = {https://discuss.okfn.org/}
}
@article{Wynants2020,
abstract = {Objective: To review and critically appraise published and preprint reports of models that aim to predict either (i) presence of existing COVID-19 infection, or (ii) future complications in individuals already diagnosed with COVID-19. Any models to identify subjects at risk for COVID-19 in the general population were also included. Design: Rapid systematic review and critical appraisal of prediction models for diagnosis or prognosis of COVID-19 infection. Data sources: PubMed, EMBASE via Ovid, Arxiv, medRxiv and bioRxiv until 13th March 2020. Study selection: Studies that developed or validated a multivariable COVID-19 related prediction model. Two authors independently screened titles and abstracts. Data extraction: Data from included studies were extracted independently by at least two authors based on the CHARMS checklist, and risk of bias was assessed using PROBAST. Data were extracted on various domains including the participants, predictors, outcomes, data analysis, and prediction model performance. Results: 1916 titles were screened. Of these, 15 studies describing 19 prediction models were included for data extraction and critical appraisal. We identified three models to predict hospital admission from pneumonia and other events (as a proxy for covid-19 pneumonia) in the general population; nine diagnostic models to detect COVID-19 infection in symptomatic individuals (seven of which were deep learning models for COVID-19 diagnosis utilising computed tomography (CT) results); and seven prognostic models for predicting mortality risk, or length of hospital stay. None of the 15 studies used data on COVID-19 cases outside of China. Predictors included in more than one of the 19 models were: age, sex, comorbidities, C-reactive protein, lymphocyte markers (percentage or neutrophil-to-lymphocyte ratio), lactate dehydrogenase, and features derived from CT images. Reported C-index estimates for the prediction models ranged from 0.73 to 0.81 in those for the general population (reported for all 3 general population models), from 0.81 to {\textgreater} 0.99 in those for diagnosis (reported for 5 of the 9 diagnostic models), and from 0.90 to 0.98 in those for prognosis (reported for 4 of the 7 prognostic models). All studies were rated at high risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, and poor statistical analysis, including high risk of model overfitting. Reporting quality varied substantially between studies. A description of the study population and intended use of the models was absent in almost all reports, and calibration of predictions was rarely assessed. Conclusion: COVID-19 related prediction models for diagnosis and prognosis are quickly entering the academic literature through publications and preprint reports, aiming to support medical decision making in a time where this is needed urgently. Many models were poorly reported and all appraised as high risk of bias. We call for immediate sharing of the individual participant data from COVID-19 studies worldwide to support collaborative efforts in building more rigously developed and validated COVID-19 related prediction models. The predictors identified in current studies should be considered for potential inclusion in new models. We also stress the need to adhere to methodological standards when developing and evaluating COVID-19 related predictions models, as unreliable predictions may cause more harm than benefit when used to guide clinical decisions about COVID-19 in the current pandemic.

{\#}{\#}{\#} Competing Interest Statement

The authors have declared no competing interest.

{\#}{\#}{\#} Funding Statement

LW is a post-doctoral fellow of Research Foundation  Flanders (FWO). BVC received support from FWO (grant G0B4716N) and Internal Funds KU Leuven (grant C24/15/037). TD acknowledges financial support from the Netherlands Organization for Health Research and Development (Grant Numbers: 91617050). KGMM gratefully acknowledges financial support from Cochrane Collaboration (SMF 2018). KIES is funded by the National Institute for Health Research School for Primary Care Research (NIHR SPCR). The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care. GSC was supported by the NIHR Biomedical Research Centre, Oxford, and Cancer Research UK (programme grant: C49297/A27294). The funders played no role in study design, data collection, data analysis, data interpretation, or reporting. The guarantors had full access to all the data in the study, take responsibility for the integrity of the data and the accuracy of the data analysis, and had final responsibility for the decision to submit for publication.

{\#}{\#}{\#} Author Declarations

All relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.

Yes

All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.

Yes

I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).

Yes

I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.

Yes

Extracted data is partially reported in Tables 1-2 and Supplementary Tables 1-2. The original manuscripts are published open access (links provided in the reference list). The protocol for this systematic review was registered on OSF (osf.io/ehc47/).},
author = {Wynants, Laure and Calster, Ben Van and Bonten, Marc MJ and Collins, Gary S and Debray, Thomas PA and Vos, Maarten De and Haller, Maria C and Heinze, Georg and Moons, Karel GM and Riley, Richard D and Schuit, Ewoud and Smits, Luc and Snell, Kym IE and Steyerberg, Ewout W and Wallisch, Christine and van Smeden, Maarten},
doi = {10.1101/2020.03.24.20041020},
file = {::},
journal = {medRxiv},
pages = {2020.03.24.20041020},
title = {{Systematic review and critical appraisal of prediction models for diagnosis and prognosis of COVID-19 infection}},
year = {2020}
}
@article{Diez-fuertes2020,
author = {D{\'{i}}ez-fuertes, Francisco and Iglesias-caballero, Mar{\'{i}}a and Monz{\'{o}}n, Sara and Jim{\'{e}}nez, Pilar and P{\'{e}}rez, Javier Garc{\'{i}}a and Pozo, Francisco and P{\'{e}}rez-olmeda, Mayte},
doi = {10.1101/2020.04.20.050039},
file = {::},
title = {{Phylodynamics of SARS-CoV-2 transmission in Spain}},
year = {2020}
}
@article{Haleem2018,
abstract = {The Internet of Things is an {\$}800 billion industry, with over 8.4 billion connected devices online, and spending predicted to reach nearly {\$}1.4 trillion by 2021 [1]. Most of these devices need to connect to the Internet to function. However, current solutions such as cellular, WiFi, and Bluetooth are suboptimal: they are too expensive, too power hungry, or too limited in range. Helium is a decentralized machine network that enables ma-chines anywhere in the world to wirelessly connect to the Internet and geolocate themselves without the need for power-hungry satellite location hardware or expensive cellular plans. Powering the network is a blockchain with a native protocol token incentivizing a two-sided marketplace between cover-age providers and coverage consumers. With the introduction of a blockchain, Helium injects decentralization into an in-dustry currently controlled by monopolies. The result is that wireless network coverage becomes a commodity, fueled by competition, available anywhere in the world, at a fraction of current costs. Helium's secure and open-source primitives enable develop-ers to build low-power, Internet-connected machines quickly and cost-effectively. Helium has a wide variety of applica-tions across industries and is the first decentralized machine network of its kind.},
author = {Haleem, Amir and Allen, Andrew and Thompson, Andrew and Nijdam, Marc and Garg, Rahul},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Helium.pdf:pdf},
title = {{Helium A Decentralized Machine Network}},
url = {http://whitepaper.helium.com/},
year = {2018}
}
@article{OceanProtocolFoundation2018,
abstract = {This paper presents Ocean Protocol. Ocean is a decentralized protocol and network of artificial intelligence (AI) data/services. It incentivizes for a vast supply of relevant AI data/services. This network helps to power AI data/service marketplaces, as well as public commons data. The heart of Ocean's network is a new construction: a Curated Proofs Market. CPMs bridge predicted relevance with actual relevance of each AI service, by having curation markets for cryptographic proofs (e.g. proof of data availability).},
author = {{Ocean Protocol Foundation} and {BigchainDB GmbH} and {DEX Pte. Ltd}},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Ocean Protocol Technical Whitepaper.pdf:pdf},
pages = {1--51},
title = {{Ocean Protocol: A Decentralized Substrate for AI Data {\&} Services Technical Whitepaper}},
url = {https://oceanprotocol.com/},
year = {2018}
}
@article{Granozio2020,
abstract = {The sudden spread of Covid-19 outside China has pushed on March 11 the World Health Organization to acknowledge the ongoing outbreak as a pandemic. It is crucial in this phase to understand what should countries which presently lag behind in the spread of the infection learn from countries where the infection spread earlier. The choice of this work is to prefer timeliness to comprehensiveness. By adopting a purely empirical approach, we will limit ourselves to identifying different phases in the plots of different countries, based on their different functional behaviour, and to make a comparative analysis. The comparative analysis of the registered cases curves highlights remarkable similarities, especially among Western countries, together with some minor but crucial differences. We highlight how timeliness can largely reduce the size of the individual national outbreaks, ultimately limiting the final death toll. Our data suggest that Western governments have not unfortunately shown the capability to anticipate their decisions, based on the experience of countries hit earlier by the outbreak.},
archivePrefix = {arXiv},
arxivId = {2003.08661},
author = {Granozio, Fabio Miletto},
eprint = {2003.08661},
file = {::},
month = {mar},
title = {{Comparative analysis of the diffusion of Covid-19 infection in different countries}},
url = {http://arxiv.org/abs/2003.08661},
year = {2020}
}
@article{Sardar2020,
abstract = {Middle East Respiratory Syndrome Coronavirus (MERS-CoV) causes severe acute respiratory illness with a case fatality rate (CFR) of 35,5{\%}. The highest number of MERS-CoV cases are from Saudi-Arabia, the major worldwide hotspot for this disease. In the absence of neither effective treatment nor a ready-to-use vaccine and with yet an incomplete understanding of its epidemiological cycle, prevention and containment measures can be derived from mathematical models of disease epidemiology. We constructed 2-strain models to predict past outbreaks in the interval 2012-2016 and derive key epidemiological information for Macca, Madina and Riyadh. We approached variability in infection through three different disease incidence functions capturing social behavior in response to an epidemic (e.g. Bilinear, BL; Non-monotone, NM; and Saturated, SAT models). The best model combination successfully anticipated the total number of MERS-CoV clinical cases for the 2015-2016 season and accurately predicted both the number of cases at the peak of seasonal incidence and the overall shape of the epidemic cycle. The evolution in the basic reproduction number (R0) warns that MERS-CoV may easily take an epidemic form. The best model correctly captures this feature, indicating a high epidemic risk (1≤R0≤2,5) in Riyadh and Macca and confirming the alleged co-circulation of more than one strain. Accurate predictions of the future MERS-CoV peak week, as well as the number of cases at the peak are now possible. These results indicate public health agencies should be aware that measures for strict containment are urgently needed before new epidemics take off in the region.},
author = {Sardar, Tridip and Ghosh, Indrajit and Rod{\'{o}}, Xavier and Chattopadhyay, Joydev},
doi = {10.1371/journal.pntd.0008065},
editor = {Althouse, Benjamin},
file = {::},
issn = {1935-2735},
journal = {PLOS Neglected Tropical Diseases},
month = {feb},
number = {2},
pages = {e0008065},
publisher = {NLM (Medline)},
title = {{A realistic two-strain model for MERS-CoV infection uncovers the high risk for epidemic propagation}},
url = {https://dx.plos.org/10.1371/journal.pntd.0008065},
volume = {14},
year = {2020}
}
@article{Castro2020,
abstract = {No, they can't. Exponentially growing dynamics are intrinsically unpredictable. The time at which the number of infected individuals starts decreasing cannot be reliably calculated before it is actually attained. A standard SIR model with confinement shows that infection spread is inhibited only above a threshold. Confinement induces a slow-down in the expansion phase that does not guarantee an eventual control of the epidemic. A Bayesian fit to the on-going COVID-19 pandemic in Spain shows that we can infer neither its peaking time nor whether there is a peak at all. The dispersion of possible trajectories grows extremely fast, yielding a short horizon for reliable prediction. As unpredictability is intrinsic, not due to incomplete or wrong data, our study advocates for a scenario of probabilistic forecasting.},
archivePrefix = {arXiv},
arxivId = {2004.08842},
author = {Castro, Mario and Ares, Sa{\'{u}}l and Cuesta, Jos{\'{e}} A. and Manrubia, Susanna},
eprint = {2004.08842},
file = {::},
pages = {1--21},
title = {{Predictability: Can the turning point and end of an expanding epidemic be precisely forecast?}},
url = {http://arxiv.org/abs/2004.08842},
year = {2020}
}
@article{Wang2020,
abstract = {BACKGROUND We described the epidemiological features of the coronavirus disease 2019 (Covid-19) outbreak, and evaluated the impact of non-pharmaceutical interventions on the epidemic in Wuhan, China. METHODS Individual-level data on 25,961 laboratory-confirmed Covid-19 cases reported through February 18, 2020 were extracted from the municipal Notifiable Disease Report System. Based on key events and interventions, we divided the epidemic into four periods: before January 11, January 11-22, January 23 - February 1, and February 2-18. We compared epidemiological characteristics across periods and different demographic groups. We developed a susceptible-exposed-infectious-recovered model to study the epidemic and evaluate the impact of interventions. RESULTS The median age of the cases was 57 years and 50.3{\%} were women. The attack rate peaked in the third period and substantially declined afterwards across geographic regions, sex and age groups, except for children (age {\textless}20) whose attack rate continued to increase. Healthcare workers and elderly people had higher attack rates and severity risk increased with age. The effective reproductive number dropped from 3.86 (95{\%} credible interval 3.74 to 3.97) before interventions to 0.32 (0.28 to 0.37) post interventions. The interventions were estimated to prevent 94.5{\%} (93.7 to 95.2{\%}) infections till February 18. We found that at least 59{\%} of infected cases were unascertained in Wuhan, potentially including asymptomatic and mild-symptomatic cases. CONCLUSIONS Considerable countermeasures have effectively controlled the Covid-19 outbreak in Wuhan. Special efforts are needed to protect vulnerable populations, including healthcare workers, elderly and children. Estimation of unascertained cases has important implications on continuing surveillance and interventions. {\#}{\#}{\#} Competing Interest Statement The authors have declared no competing interest. {\#}{\#}{\#} Funding Statement This study was partly supported by the Fundamental Research Funds for the Central Universities (2019kfyXMBZ015), the 111 Project (C.W., L.L., X.H., H.G., Q.W., J.H., A.P., S.W., T.W.). X.L. is supported by Harvard University. {\#}{\#}{\#} Author Declarations All relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript. Yes All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived. Yes I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance). Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable. Yes Not available},
author = {Wang, Chaolong and Liu, Li and Hao, Xingjie and Guo, Huan and Wang, Qi and Huang, Jiao and He, Na and Yu, Hongjie and Lin, Xihong and Pan, An and Wei, Sheng and Wu, Tangchun},
doi = {10.101/2020.03.03.20030593},
file = {::},
journal = {MedArxiv},
pages = {2020.03.03.20030593},
title = {{Evolving Epidemiology and Impact of Non-pharmaceutical Interventions on the Outbreak of Coronavirus Disease 2019 in Wuhan, China}},
url = {https://doi.org/10.1101/2020.03.03.20030593{\%}0Ahttps://www.medrxiv.org/content/10.1101/2020.03.03.20030593v1},
year = {2020}
}

@manual{KGcovid19,
title = {{Knowledge Graph COVID-19}},
url = {http://www.odbms.org/2020/03/we-build-a-knowledge-graph-on-covid-19/}
}

@manual{nsf,
title = {{U.S. National Science Foundation\'s proposed CyberInfrastructure}},
url = {https://www.nsf.gov/div/index.jsp?div=OAC}
}

@article{EUcommission,
author=European Commission,
title="Towards a Sustainable Europe by 2030",
journal="ICES Scientific Reports",
year="2019",
volume=,
number=,
pages="",
doi="",
}

@article{ICES,
author=ICES,
title="International Bottom Trawl Survey Working Group (IBTSWG)",
journal="ICES Scientific Reports",
year="2019",
volume=,
number=,
pages="1--60",
doi="10.17895/ices.pub.5596",
}

@article{Maass2015,
abstract = {Both the brain and digital computers process information, but they do this in completely different ways. Neurons in the brain transmit information not through bits, but through spikes. Spikes are short voltage increases that are generated near the cell body of a neuron, with average spike rates below 10 Hz. These spikes are transmitted via fine axonal fibers and synapses to about 10 000 other neurons. Neurons also differ in another fundamental aspect from processors in a digital computer: they produce spikes according to stochastic rather than deterministic rules. This article discusses recent progress in understanding how complex computations can be carried out with such stochastically spiking neurons. Other recent developments suggest that spike-based neural networks can be emulated by neuromorphic hardware at a fraction of the energy consumed by current digital computing hardware. Can both developments be merged to provide a blueprint for substantially more energy-efficient computing devices? Explores these issues and examines the viability of such a merger.},
author = {Maass, Wolfgang},
doi = {10.1109/JPROC.2015.2496679},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/07329679.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Biological neural networks,Brain models,Digital computers,Energy efficiency,Neural networks,Neurons,Optical fiber devices,Optical fiber networks,Program processors,Voltage measurement},
number = {12},
pages = {2219--2224},
publisher = {IEEE},
title = {{To Spike or Not to Spike: That Is the Question}},
volume = {103},
year = {2015}
}
@article{Dilley2016,
abstract = {Bitcoin, the first peer-to-peer electronic cash system, opened the door to permissionless, private, and trustless transactions. Attempts to repurpose Bitcoin's underlying blockchain technology have run up against fundamental limitations to privacy, faithful execution, and transaction finality. We introduce $\backslash$emph{\{}Strong Federations{\}}: publicly verifiable, Byzantine-robust transaction networks that facilitate movement of any asset between disparate markets, without requiring third-party trust. $\backslash$emph{\{}Strong Federations{\}} enable commercial privacy, with support for transactions where asset types and amounts are opaque, while remaining publicly verifiable. As in Bitcoin, execution fidelity is cryptographically enforced; however, $\backslash$emph{\{}Strong Federations{\}} significantly lower capital requirements for market participants by reducing transaction latency and improving interoperability. To show how this innovative solution can be applied today, we describe $\backslash$emph{\{}$\backslash$liquid{\}}: the first implementation of $\backslash$emph{\{}Strong Federations{\}} deployed in a Financial Market.},
archivePrefix = {arXiv},
arxivId = {1612.05491},
author = {Dilley, Johnny and Poelstra, Andrew and Wilkins, Jonathan and Piekarska, Marta and Gorlick, Ben and Friedenbach, Mark},
eprint = {1612.05491},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/4{\_}5920008888731567415.pdf:pdf},
title = {{Strong Federations: An Interoperable Blockchain Solution to Centralized Third-Party Risks}},
url = {http://arxiv.org/abs/1612.05491},
year = {2016}
}
@article{Iten2020a,
abstract = {Despite the success of neural networks at solving concrete physics problems, their use as a general-purpose tool for scientific discovery is still in its infancy. Here, we approach this problem by modeling a neural network architecture after the human physical reasoning process, which has similarities to representation learning. This allows us to make progress towards the long-term goal of machine-assisted scientific discovery from experimental data without making prior assumptions about the system. We apply this method to toy examples and show that the network finds the physically relevant parameters, exploits conservation laws to make predictions, and can help to gain conceptual insights, e.g., Copernicus' conclusion that the solar system is heliocentric.},
archivePrefix = {arXiv},
arxivId = {1807.10300},
author = {Iten, Raban and Metger, Tony and Wilming, Henrik and {Del Rio}, L{\'{i}}dia and Renner, Renato},
doi = {10.1103/PhysRevLett.124.010508},
eprint = {1807.10300},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1807.10300.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {1},
pages = {1--18},
title = {{Discovering Physical Concepts with Neural Networks}},
volume = {124},
year = {2020}
}
@article{Real2020,
abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
archivePrefix = {arXiv},
arxivId = {2003.03384},
author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
eprint = {2003.03384},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/2003.03384.pdf:pdf},
title = {{AutoML-Zero: Evolving Machine Learning Algorithms From Scratch}},
url = {http://arxiv.org/abs/2003.03384},
year = {2020}
}
@article{Li2017,
abstract = {We present ease.ml, a declarative machine learning service platform we built to support more than ten research groups outside the computer science departments at ETH Zurich for their machine learning needs. With ease.ml, a user defines the high-level schema of a machine learning application and submits the task via a Web interface. The system automatically deals with the rest, such as model selection and data movement. In this paper, we describe the ease.ml architecture and focus on a novel technical problem introduced by ease.ml regarding resource allocation. We ask, as a "service provider" that manages a shared cluster of machines among all our users running machine learning workloads, what is the resource allocation strategy that maximizes the global satisfaction of all our users? Resource allocation is a critical yet subtle issue in this multi-tenant scenario, as we have to balance between efficiency and fairness. We first formalize the problem that we call multi-tenant model selection, aiming for minimizing the total regret of all users running automatic model selection tasks. We then develop a novel algorithm that combines multi-armed bandits with Bayesian optimization and prove a regret bound under the multi-tenant setting. Finally, we report our evaluation of ease.ml on synthetic data and on one service we are providing to our users, namely, image classification with deep neural networks. Our experimental evaluation results show that our proposed solution can be up to 9.8x faster in achieving the same global quality for all users as the two popular heuristics used by our users before ease.ml.},
archivePrefix = {arXiv},
arxivId = {1708.07308},
author = {Li, Tian and Zhong, Jie and Liu, Ji and Wu, Wentao and Zhang, Ce},
eprint = {1708.07308},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/LietalEaseML.pdf:pdf},
number = {5},
pages = {607--620},
title = {{Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning Workloads}},
url = {http://arxiv.org/abs/1708.07308},
volume = {11},
year = {2017}
}
@article{Futia2020,
abstract = {Deep learning models contributed to reaching unprecedented results in prediction and classification tasks of Artificial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a specific result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is-or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artificial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI-while being less flexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three specific challenges for future research-knowledge matching, cross-disciplinary explanations and interactive explanations.},
author = {Futia, Giuseppe and Vetr{\`{o}}, Antonio},
doi = {10.3390/info11020122},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/information-11-00122.pdf:pdf},
isbn = {3901109072},
issn = {20782489},
journal = {Information (Switzerland)},
keywords = {Deep learning,EXplainable artificial intelligence,Knowledge graphs},
number = {2},
title = {{On the integration of knowledge graphs into deep learning models for a more comprehensible AI-Three challenges for future research}},
volume = {11},
year = {2020}
}
@article{Feurer,
author = {Feurer, Matthias},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/5872-efficient-and-robust-automated-machine-learning.pdf:pdf},
title = {{Efficient and Robust Automated Machine Learning}}
}
@article{Androulaki2018,
abstract = {Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains and one of the Hyperledger projects hosted by the Linux Foundation (www.hyperledger.org). Fabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in standard, general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing blockchain platforms that require "smart-contracts" to be written in domain-specific languages or rely on a cryptocurrency. Fabric realizes the permissioned model using a portable notion of membership, which may be integrated with industry-standard identity management. To support such flexibility, Fabric introduces an entirely novel blockchain design and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks. This paper describes Fabric, its architecture, the rationale behind various design decisions, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency, scaling well to over 100 peers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.10228v2},
author = {Androulaki, Elli and Barger, Artem and Bortnikov, Vita and Muralidharan, Srinivasan and Cachin, Christian and Christidis, Konstantinos and {De Caro}, Angelo and Enyeart, David and Murthy, Chet and Ferris, Christopher and Laventman, Gennady and Manevich, Yacov and Nguyen, Binh and Sethi, Manish and Singh, Gari and Smith, Keith and Sorniotti, Alessandro and Stathakopoulou, Chrysoula and Vukoli{\'{c}}, Marko and Cocco, Sharon Weed and Yellick, Jason},
doi = {10.1145/3190508.3190538},
eprint = {arXiv:1801.10228v2},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1801.10228.pdf:pdf},
isbn = {9781450355841},
journal = {Proceedings of the 13th EuroSys Conference, EuroSys 2018},
title = {{Hyperledger Fabric: A Distributed Operating System for Permissioned Blockchains}},
volume = {2018-Janua},
year = {2018}
}
