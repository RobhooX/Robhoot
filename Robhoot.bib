@article{,
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/3XnQSMw9QiG20cUyfjRF{\_}TON.pdf:pdf},
title = {{Table of Contents}}
}
@article{DeDomenico2015,
abstract = {The determination of the most central agents in complex networks is important because they are responsible for a faster propagation of information, epidemics, failures and congestion, among others. A challenging problem is to identify them in networked systems characterized by different types of interactions, forming interconnected multilayer networks. Here we describe a mathematical framework that allows us to calculate centrality in such networks and rank nodes accordingly, finding the ones that play the most central roles in the cohesion of the whole structure, bridging together different types of relations. These nodes are the most versatile in the multilayer network. We investigate empirical interconnected multilayer networks and show that the approaches based on aggregating—or neglecting—the multilayer structure lead to a wrong identification of the most versatile nodes, overestimating the importance of more marginal agents and demonstrating the power of versatility in predicting their role in diffusive and congestion processes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{De Domenico}, Manlio and Sol{\'{e}}-Ribalta, Albert and Omodei, Elisa and G{\'{o}}mez, Sergio and Arenas, Alex},
doi = {10.1038/ncomms7868},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {2041-1723 (Electronic){\$}\backslash{\$}r2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pages = {1--6},
pmid = {25904405},
title = {{Ranking in interconnected multilayer networks reveals versatile nodes}},
volume = {6},
year = {2015}
}
@article{Feurer,
author = {Feurer, Matthias},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/5872-efficient-and-robust-automated-machine-learning.pdf:pdf},
title = {{Efficient and Robust Automated Machine Learning}}
}
@article{Gunther2018,
abstract = {Technology evolves faster than ever, with the pace picking up with every passing year. Unprecedented in history, we have the greatest and brightest minds driving unparalleled change. Progress isn't a trend that naturally happens on its own - it requires an ever growing number of researchers and experts. The scientific community of today doesn't get what it deserves, constantly struggling with obtaining funding and work- ing endless hours with little to no reimbursement. It is our stated mission objective to empower those who empower us all by establishing Science- root, the first blockchain-based scientific ecosystem to integrate a social media scientific network, a funding platform and a decentralized publish- ing framework for journals.},
author = {G{\"{u}}nther, Vlad and Alexandru Chirita},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/whitepaperScienceRoot.pdf:pdf},
title = {{" Scienceroot " Whitepaper}},
url = {https://www.scienceroot.com/},
year = {2018}
}
@article{OceanProtocolFoundation2018,
abstract = {This paper presents Ocean Protocol. Ocean is a decentralized protocol and network of artificial intelligence (AI) data/services. It incentivizes for a vast supply of relevant AI data/services. This network helps to power AI data/service marketplaces, as well as public commons data. The heart of Ocean's network is a new construction: a Curated Proofs Market. CPMs bridge predicted relevance with actual relevance of each AI service, by having curation markets for cryptographic proofs (e.g. proof of data availability).},
author = {{Ocean Protocol Foundation} and {BigchainDB GmbH} and {DEX Pte. Ltd}},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Ocean Protocol Technical Whitepaper.pdf:pdf},
pages = {1--51},
title = {{Ocean Protocol: A Decentralized Substrate for AI Data {\&} Services Technical Whitepaper}},
url = {https://oceanprotocol.com/},
year = {2018}
}
@article{DeDomenico:2014,
author = {{De Domenico M.} and {Porter M. A.} and Arenas, A},
journal = {Journal of Complex Networks},
pages = {159--176},
title = {{MuxViz: a tool for multilayer analysis and visualization of networks}},
volume = {3},
year = {2014}
}
@article{Maass2015,
abstract = {Both the brain and digital computers process information, but they do this in completely different ways. Neurons in the brain transmit information not through bits, but through spikes. Spikes are short voltage increases that are generated near the cell body of a neuron, with average spike rates below 10 Hz. These spikes are transmitted via fine axonal fibers and synapses to about 10 000 other neurons. Neurons also differ in another fundamental aspect from processors in a digital computer: they produce spikes according to stochastic rather than deterministic rules. This article discusses recent progress in understanding how complex computations can be carried out with such stochastically spiking neurons. Other recent developments suggest that spike-based neural networks can be emulated by neuromorphic hardware at a fraction of the energy consumed by current digital computing hardware. Can both developments be merged to provide a blueprint for substantially more energy-efficient computing devices? Explores these issues and examines the viability of such a merger.},
author = {Maass, Wolfgang},
doi = {10.1109/JPROC.2015.2496679},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/07329679.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Biological neural networks,Brain models,Digital computers,Energy efficiency,Neural networks,Neurons,Optical fiber devices,Optical fiber networks,Program processors,Voltage measurement},
number = {12},
pages = {2219--2224},
publisher = {IEEE},
title = {{To Spike or Not to Spike: That Is the Question}},
volume = {103},
year = {2015}
}
@article{Voelkl2018,
abstract = {Single-laboratory studies conducted under highly standardized conditions are the gold standard in preclinical animal research. Using simulations based on 440 preclinical studies across 13 different interventions in animal models of stroke, myocardial infarction, and breast cancer, we compared the accuracy of effect size estimates between single-laboratory and multi-laboratory study designs. Single-laboratory studies generally failed to predict effect size accurately, and larger sample sizes rendered effect size estimates even less accurate. By contrast, multi-laboratory designs including as few as 2 to 4 laboratories increased coverage probability by up to 42 percentage points without a need for larger sample sizes. These findings demonstrate that within-study standardization is a major cause of poor reproducibility. More representative study samples are required to improve the external validity and reproducibility of preclinical animal research and to prevent wasting animals and resources for inconclusive research.},
author = {Voelkl, Bernhard and Vogt, Lucile and Sena, Emily S. and W{\"{u}}rbel, Hanno},
doi = {10.1371/journal.pbio.2003693},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Voelkl{\_}PLoSBiol2018{\_}All.pdf:pdf},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
number = {2},
pmid = {29470495},
title = {{Reproducibility of preclinical animal research improves with heterogeneity of study samples}},
volume = {16},
year = {2018}
}
@article{Gael2018,
abstract = {We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.},
author = {Gael, Jurgen Van},
doi = {10.7551/mitpress/1120.003.0079},
file = {::},
isbn = {0-262-04208-8},
journal = {Advances in Neural Information Processing Systems 14},
pages = {1--8},
title = {{The Infinite Hidden Markov Model}},
year = {2018}
}
@article{Roberts2013,
abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
doi = {10.1098/rsta.2011.0550},
file = {::},
isbn = {1364-503X (Print) 1364-503X (Linking)},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Bayesian modelling,Gaussian processes,Time-series analysis},
number = {1984},
pages = {1--27},
pmid = {23277607},
title = {{Gaussian processes for time-series modelling}},
volume = {371},
year = {2013}
}
@article{Li2017,
abstract = {We present ease.ml, a declarative machine learning service platform we built to support more than ten research groups outside the computer science departments at ETH Zurich for their machine learning needs. With ease.ml, a user defines the high-level schema of a machine learning application and submits the task via a Web interface. The system automatically deals with the rest, such as model selection and data movement. In this paper, we describe the ease.ml architecture and focus on a novel technical problem introduced by ease.ml regarding resource allocation. We ask, as a "service provider" that manages a shared cluster of machines among all our users running machine learning workloads, what is the resource allocation strategy that maximizes the global satisfaction of all our users? Resource allocation is a critical yet subtle issue in this multi-tenant scenario, as we have to balance between efficiency and fairness. We first formalize the problem that we call multi-tenant model selection, aiming for minimizing the total regret of all users running automatic model selection tasks. We then develop a novel algorithm that combines multi-armed bandits with Bayesian optimization and prove a regret bound under the multi-tenant setting. Finally, we report our evaluation of ease.ml on synthetic data and on one service we are providing to our users, namely, image classification with deep neural networks. Our experimental evaluation results show that our proposed solution can be up to 9.8x faster in achieving the same global quality for all users as the two popular heuristics used by our users before ease.ml.},
archivePrefix = {arXiv},
arxivId = {1708.07308},
author = {Li, Tian and Zhong, Jie and Liu, Ji and Wu, Wentao and Zhang, Ce},
eprint = {1708.07308},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/LietalEaseML.pdf:pdf},
number = {5},
pages = {607--620},
title = {{Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning Workloads}},
url = {http://arxiv.org/abs/1708.07308},
volume = {11},
year = {2017}
}
@article{Reichstein,
abstract = {Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning. H umans have always striven to predict and understand the world, and the ability to make better predictions has given competitive advantages in diverse contexts (such as weather, diseases or financial markets). Yet the tools for prediction have substantially changed over time, from ancient Greek philosophical reasoning to non-scientific medieval methods such as soothsaying, towards modern scientific discourse , which has come to include hypothesis testing, theory development and computer modelling underpinned by statistical and physical relationships, that is, laws 1. A success story in the geosciences is weather prediction, which has greatly improved through the integration of better theory, increased computational power, and established observational systems, which allow for the assimilation of large amounts of data into the modelling system 2. Nevertheless, we can accurately predict the evolution of the weather on a timescale of days, not months. Seasonal meteorological predictions, forecasting extreme events such as flooding or fire, and long-term climate projections are still major challenges. This is especially true for predicting dynamics in the biosphere, which is dominated by biologically mediated processes such as growth or reproduction, and is strongly controlled by seemingly stochastic disturbances such as fires and landslides. Such predictive problems have not seen much progress in the past few decades 3. At the same time, a deluge of Earth system data has become available, with storage volumes already well beyond dozens of petabytes and rapidly increasing transmission rates exceeding hundreds of terabytes per day 4. These data come from a plethora of sensors measuring states, fluxes and intensive or time/space-integrated variables, representing fifteen or more orders of temporal and spatial magnitude. They include remote sensing from a few metres to hundreds of kilometres above Earth as well as in situ observations (increasingly from autonomous sensors) at and below the surface and in the atmosphere, many of which are further being complemented by citizen science observations. Model simulation output adds to this deluge; the CMIP-5 dataset of the Climate Model Intercomparison Project, used extensively for scientific groundwork towards periodic climate assessments, is over 3 petabytes in size, and the next generation, CMIP-6, is estimated to reach up to 30 petabytes 5. The data from models share many of the challenges and statistical properties of observational data, including many forms of uncertainty. In summary, Earth system data are exemplary of all four of the 'four Vs' of 'big data': volume, velocity, variety and veracity (see Fig. 1). One key challenge is to extract interpret-able information and knowledge from this big data, possibly almost in real time and integrating between disciplines. Taken together, our ability to collect and create data far outpaces our ability to sensibly assimilate it, let alone understand it. Predictive ability in the last few decades has not increased apace with data availability. To get the most out of the explosive growth and diversity of Earth system data, we face two major tasks in the coming years: (1) extracting knowledge from the data deluge, and (2) deriving models that learn much more from data than traditional data assimilation approaches can, while still respecting our evolving understanding of nature's laws. The combination of unprecedented data sources, increased computational power, and the recent advances in statistical modelling and machine learning offer exciting new opportunities for expanding our knowledge about the Earth system from data. In particular, many tools are available from the fields of machine learning and artificial intelligence, but they need to be further developed and adapted to geo-scientific analysis. Earth system science offers new opportunities, challenges and methodological demands, in particular for recent research lines focusing on spatio-temporal context and uncertainties (Box 1; see https://developers. google.com/machine-learning/glossary/ and http://www.wildml.com/ deep-learning-glossary/ for more complete glossaries). In the following sections we review the development of machine learning in the geoscientific context, and highlight how deep learning-that is, the automatic extraction of abstract (spatio-temporal) features-has the potential to overcome many of the limitations that have, until now, hindered a more widespread adoption of machine learning. We further lay out the most promising but also challenging approaches in combining machine learning with physical modelling. State-of-the-art geoscientific machine learning Machine learning is now a successful part of several research-driven and operational geoscientific processing schemes, addressing the atmosphere, the land surface and the ocean, and has co-evolved with data availability over the past decade. Early landmarks in classification of land cover and clouds emerged almost 30 years ago through the coincidence of high-resolution satellite data and the first revival of neural networks 6,7. Most major machine learning methodological},
author = {Reichstein, Markus and Camps-Valls, Gustau and Stevens, Bjorn and Jung, Martin and Denzler, Joachim and Carvalhais, Nuno and Prabhat, {\&}},
doi = {10.1038/s41586-019-0912-1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/s41586-019-0912-1.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
publisher = {Springer US},
title = {{Deep learning and process understanding for data-driven Earth system science}},
url = {www.nature.com/nature}
}
@article{Valera2015,
abstract = {We propose the infinite factorial dynamic model (iFDM), a general Bayesian nonparametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.},
author = {Valera, Isabel and Ruiz, Francisco and Svensson, Lennart and Perez-Cruz, Fernando},
file = {::},
journal = {Advances in Neural Information Processing Systems 28},
pages = {1666--1674},
title = {{Infinite Factorial Dynamical Model}},
url = {http://papers.nips.cc/paper/5667-infinite-factorial-dynamical-model.pdf},
year = {2015}
}
@article{Hensman2015,
abstract = {In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.1605v2},
author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
doi = {10.1109/TPAMI.2014.2318711},
eprint = {arXiv:1401.1605v2},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {383--393},
title = {{Fast nonparametric clustering of structured time-series}},
volume = {37},
year = {2015}
}
@article{Kivela:2013,
author = {Kivela, M},
journal = {http://www.plexmath.eu/?page{\_}id=327},
title = {{Multilayer networks library}},
year = {2013}
}
@article{Clark2013,
abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this hierarchical prediction machine approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency. {\textcopyright} 2013 Cambridge University Press.},
author = {Clark, Andy},
doi = {10.1017/S0140525X12000477},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/whatever-next-predictive-brains-situated-agents-and-the-future-of-cognitive-science.pdf:pdf},
issn = {14691825},
journal = {Behavioral and Brain Sciences},
keywords = {Bayesian brain,action,attention,expectation,generative model,hierarchy,perception,precision,prediction,prediction error,predictive coding,top-down processing},
number = {3},
pages = {181--204},
pmid = {23663408},
title = {{Whatever next? Predictive brains, situated agents, and the future of cognitive science}},
volume = {36},
year = {2013}
}
@techreport{Gil2019,
author = {Gil, Yolanda and Selman, Bart and Desjardins, Marie and Forbus, Ken and Mckeown, Kathy and Weld, Dan and Dietterich, Tom and Li, Fei Fei and Bradley, Liz and Lopresti, Daniel and Mishra, Nina and Parkes, David and Drobnis, Ann Schwartz},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/AIRoadmapDraftforCommunityMay2019.pdf:pdf},
title = {{A 20-Year Community Roadmap for Artificial Intelligence Research in the US Roadmap Co-chairs: Workshop Chairs: Steering Committee}},
url = {https://bit.ly/2ZNVBVb},
year = {2019}
}
@article{Haleem2018,
abstract = {The Internet of Things is an {\$}800 billion industry, with over 8.4 billion connected devices online, and spending predicted to reach nearly {\$}1.4 trillion by 2021 [1]. Most of these devices need to connect to the Internet to function. However, current solutions such as cellular, WiFi, and Bluetooth are suboptimal: they are too expensive, too power hungry, or too limited in range. Helium is a decentralized machine network that enables ma-chines anywhere in the world to wirelessly connect to the Internet and geolocate themselves without the need for power-hungry satellite location hardware or expensive cellular plans. Powering the network is a blockchain with a native protocol token incentivizing a two-sided marketplace between cover-age providers and coverage consumers. With the introduction of a blockchain, Helium injects decentralization into an in-dustry currently controlled by monopolies. The result is that wireless network coverage becomes a commodity, fueled by competition, available anywhere in the world, at a fraction of current costs. Helium's secure and open-source primitives enable develop-ers to build low-power, Internet-connected machines quickly and cost-effectively. Helium has a wide variety of applica-tions across industries and is the first decentralized machine network of its kind.},
author = {Haleem, Amir and Allen, Andrew and Thompson, Andrew and Nijdam, Marc and Garg, Rahul},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Helium.pdf:pdf},
title = {{Helium A Decentralized Machine Network}},
url = {http://whitepaper.helium.com/},
year = {2018}
}
@article{Cristian1989,
abstract = {Abstract A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock ... $\backslash$n},
author = {Cristian, Flaviu},
doi = {10.1007/BF01784024},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Clock synchronization,Communication,Distributed system,Fault-tolerance,Time service},
number = {3},
pages = {146--158},
title = {{Probabilistic clock synchronization}},
volume = {3},
year = {1989}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P a},
doi = {10.1371/journal.pmed.0020124},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16060722},
volume = {2},
year = {2005}
}
@article{BigchainDBGmbH2018,
abstract = {BigchainDB is software that has blockchain properties (e.g. decentral-ization, immutability, owner-controlled assets) and database properties (e.g. high transaction rate, low latency, indexing {\&} querying of structured data). It was first released—open source—in February 2016 and has been improved continuously ever since. BigchainDB version 2.0 makes signifi-cant improvements over previous versions. In particular, it is now Byzan-tine fault tolerant (BFT), so up to a third of the nodes can fail in any way, and the system will continue to agree on how to proceed. BigchainDB 2.0 is also production-ready for many use cases. In this paper, we review the design goals of BigchainDB 2.0 and how they were achieved, we explore some use cases, we show how BigchainDB fits into the overall decentral-ization ecosystem, we follow the life of a transaction to understand how BigchainDB 2.0 works, we note ways to try BigchainDB, we outline how you can contribute, and we summarize future plans.},
author = {{BigchainDB GmbH}},
doi = {10.1111/j.1365-2958.2006.05434.x},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/bigchaindb-whitepaper.pdf:pdf},
journal = {BigchainDB. The blockchain database.},
number = {May},
pages = {1--14},
title = {{BigchainDB: The blockchain database}},
url = {https://www.bigchaindb.com/whitepaper/bigchaindb-whitepaper.pdf},
year = {2018}
}
@article{Vats2015,
abstract = {Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations with respect to a target distribution. A fundamental question is when should sampling stop so that we have good estimates of the desired quantities? The key to answering this question lies in assessing the Monte Carlo error through a multivariate Markov chain central limit theorem (CLT). The multivariate nature of this Monte Carlo error largely has been ignored in the MCMC literature. We present a multivariate framework for terminating simulation in MCMC. We define a multivariate effective sample size, estimating which requires strongly consistent estimators of the covariance matrix in the Markov chain CLT; a property we show for the multivariate batch means estimator. We then provide a lower bound on the number of minimum effective samples required for a desired level of precision. This lower bound depends on the problem only in the dimension of the expectation being estimated, and not on the underlying stochastic process. This result is obtained by drawing a connection between terminating simulation via effective sample size and terminating simulation using a relative standard deviation fixed-volume sequential stopping rule; which we demonstrate is an asymptotically valid procedure. The finite sample properties of the proposed method are demonstrated in a variety of examples.},
archivePrefix = {arXiv},
arxivId = {1512.07713},
author = {Vats, Dootika and Flegal, James M. and Jones, Galin L.},
doi = {10.1016/j.jaad.2011.01.035},
eprint = {1512.07713},
file = {::},
isbn = {1512.07713},
issn = {0190-9622},
title = {{Multivariate Output Analysis for Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1512.07713},
year = {2015}
}
@article{Walters2011,
abstract = {In this paper we survey the literature on the Black-Litterman model. This survey is provided both as a chronology and a taxonomy as there are many claims on the model in the literature. We provide a complete description of the canonical model including full derivations from the underlying principles using both Theil's Mixed Estimation model and Bayes Theory. The various parameters of the model are considered, along with information on their computation or calibration. Further consideration is given to several of the key papers, with worked examples illustrating the concepts.},
author = {Walters, Jay},
doi = {10.2139/ssrn.1314585},
isbn = {1556-5068},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{The Black-Litterman Model in Detail}},
url = {http://www.ssrn.com/abstract=1314585},
year = {2011}
}
@article{Uzzi2018,
author = {Uzzi, Brian and Vespignani, Alessandro and B{\"{o}}rner, Katy and Radicchi, Filippo and Sinatra, Roberta and Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Waltman, Ludo and Bergstrom, Carl T. and Milojevi{\'{c}}, Sta{\v{s}}a and Helbing, Dirk and Petersen, Alexander M. and Fortunato, Santo and Wang, Dashun and Evans, James A.},
doi = {10.1126/science.aao0185},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/eaao0185.full.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6379},
pages = {eaao0185},
title = {{Science of science}},
volume = {359},
year = {2018}
}
@article{Golem2016,
author = {Golem},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Golem-white-paper.pdf:pdf},
journal = {Golem.Network},
number = {November},
pages = {1--28},
title = {{The Golem Project Crowdfunding Whitepaper}},
url = {https://golem.network/crowdfunding/Golemwhitepaper.pdf},
year = {2016}
}
@article{Meshulam2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.08461v1},
author = {Meshulam, Leenoy and Gauthier, Jeffrey L and Brody, Carlos D and Tank, David W and Bialek, William},
eprint = {arXiv:1809.08461v1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1809.08461.pdf:pdf},
pages = {1--5},
title = {{Coarse–graining, fixed points, and scaling in a large population of neurons}},
year = {2018}
}
@article{Steinruecken,
author = {Steinruecken, Christian and Smith, Emma and Janz, David and Lloyd, James and Ghahramani, Zoubin},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Steinruecken2019{\_}Chapter{\_}TheAutomaticStatistician.pdf:pdf},
isbn = {9783030053185},
pages = {161--173},
title = {{The Automatic Statistician}}
}
@article{Gaoetal:2012,
author = {{Gao J.} and {Buldyrev S. V.} and {Stanley H. E.} and Havlin, S},
journal = {Nature physics},
pages = {40--48},
title = {{Networks formed from interdependent networks}},
volume = {8},
year = {2012}
}
@article{Bradde2017,
abstract = {A system with many degrees of freedom can be characterized by a covariance matrix; principal components analysis focuses on the eigenvalues of this matrix, hoping to find a lower dimensional description. But when the spectrum is nearly continuous, any distinction between components that we keep and those that we ignore becomes arbitrary; it then is natural to ask what happens as we vary this arbitrary cutoff. We argue that this problem is analogous to the momentum shell renormalization group. Following this analogy, we can define relevant and irrelevant operators, where the role of dimensionality is played by properties of the eigenvalue density. These results also suggest an approach to the analysis of real data. As an example, we study neural activity in the vertebrate retina as it responds to naturalistic movies, and find evidence of behavior controlled by a nontrivial fixed point. Applied to financial data, our analysis separates modes dominated by sampling noise from a smaller but still macroscopic number of modes described by a non-Gaussian distribution.},
archivePrefix = {arXiv},
arxivId = {1610.09733},
author = {Bradde, Serena and Bialek, William},
doi = {10.1007/s10955-017-1770-6},
eprint = {1610.09733},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Bradde-Bialek2017{\_}Article{\_}PCAMeetsRG.pdf:pdf},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Financial markets,Neural networks,Renormalization group},
number = {3-4},
pages = {462--475},
title = {{PCA Meets RG}},
volume = {167},
year = {2017}
}
@article{Butler1999,
author = {Butler, Charles},
doi = {10.2307/40153772},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/GPMB{\_}Annual{\_}Report{\_}Exec{\_}Summary{\_}Foreword{\_}and{\_}About{\_}English.pdf:pdf},
issn = {01637517},
journal = {Sales and Marketing Management},
number = {9 PART 1},
pages = {44},
title = {{A World}},
volume = {151},
year = {1999}
}
@article{Bradde2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.09733v1},
author = {Bradde, Serena and Bialek, William},
eprint = {arXiv:1610.09733v1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/1610.09733.pdf:pdf},
pages = {1--8},
title = {{PCA meets RG}},
year = {2016}
}
@article{Inhaber1977,
abstract = {An important aspect of the ‘science of science' is its geography. One way of studying this aspect is to evaluate the degree of dispersion or centralization of scientists. The calculation has been performed for the 16 countries with the greatest number of scientists. Czechoslovakia and Sweden have the highest concentration in a few cities, while the United States and the Federal Republic of Germany have the lowest concentration. In general, the degree of concentration varies inversely with the total number of scientists in a country, although there are exceptions. Part of the ‘concentration effect' is due to a large number of scientists in one city, usually the capital. When the effect of the leading city is removed, the ranking of the countries changes sharply. Switzerland and Australia have the highest concentrations, and the United States and the United Kingdom have the lowest. Only a weak relationship is then shown between the total number of scientists and the degree of concentration. To remove the effect of population concentration, the cumulative proportion of scientists was divided by the cumulative proportion of population for the leading cities. This produced considerable change in the rankings by nation. The USSR and India have the highest proportions. Countries with low proportions included the United States, Canada, the United Kingdom and Australia. Almost all countries had a greater degree of dispersion in 1972 as compared with 1967. The greatest changes towards dispersion were seen in India and the USSR.},
author = {Inhaber, H.},
doi = {10.1016/0048-7333(77)90024-5},
issn = {0048-7333},
journal = {Research Policy},
month = {apr},
number = {2},
pages = {178--193},
publisher = {North-Holland},
title = {{Changes in centralization of science}},
url = {https://www.sciencedirect.com/science/article/abs/pii/0048733377900245},
volume = {6},
year = {1977}
}
@article{DeDomenicoetal:2015,
author = {{De Domenico M.} and {Nicosia V.} and Latora, V},
journal = {Nature Communications},
pages = {6864},
title = {{Structural reducibility of multilayer networks}},
volume = {6},
year = {2015}
}
@unpublished{Bailey2013,
abstract = {We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.},
author = {Bailey, David H. and Borwein, Jonathan and {Lopez de Prado}, Marcos and Zhu, Qiji Jim},
booktitle = {SSRN},
doi = {10.2139/ssrn.2308659},
issn = {0002-9920},
keywords = {E44,G0,G1,G15,G2,G24,Sharpe ratio,backtest,historical simulation,investment strategy,minimum backtest length,optimization,performance degradation,probability of backtest over-fitting},
title = {{Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance}},
year = {2013}
}
@article{Fang2011,
abstract = {Articles may be retracted when their findings are no longer considered trustworthy due to scientific misconduct or error, they plagiarize previously published work, or they are found to violate ethical guidelines. Using a novel measure that we call the “retraction index,” we found that the frequency of retraction varies among journals and shows a strong correlation with the journal impact factor. Although retractions are relatively rare, the retraction process is essential for correcting the literature and maintaining trust in the scientific process.},
author = {Fang, Ferric C and Casadevall, Arturo},
doi = {10.1128/IAI.05661-11},
editor = {Morrison, R P},
journal = {Infection and Immunity},
month = {oct},
number = {10},
pages = {3855 LP  -- 3859},
title = {{Retracted Science and the Retraction Index}},
url = {http://iai.asm.org/content/79/10/3855.abstract},
volume = {79},
year = {2011}
}
@article{Hardwicke2018,
abstract = {{\textcopyright} 2018 The Authors. Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data ('analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25{\%} pre-policy to 136/174, 78{\%} post-policy), although not all data appeared reusable (23/104, 22{\%} pre-policy to 85/136, 62{\%}, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10{\%} margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
annote = {doi: 10.1098/rsos.180448},
author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Mohr, Alicia Hofelich and Clayton, Elizabeth and Yoon, Erica J. and Tessler, Michael Henry and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
doi = {10.1098/rsos.180448},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {Interrupted time series,Journal policy,Meta-science,Open data,Open science,Reproducibility},
month = {sep},
number = {8},
pages = {180448},
publisher = {Royal Society},
title = {{Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal Cognition}},
url = {https://doi.org/10.1098/rsos.180448},
volume = {5},
year = {2018}
}
@article{Beaumont:2010,
author = {Beaumont, M A},
journal = {Annual Review Ecology, Evolution and Systematics},
pages = {379--406},
title = {{Approximate {\{}B{\}}ayesian Computation in Evolution and Ecology}},
volume = {41},
year = {2010}
}
@article{Schmidhuber:2015,
author = {Schmidhuber, J},
journal = {Neural Networks},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@book{Alex2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Alex, Graves},
booktitle = {Studies in computational intelligence},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
file = {::},
isbn = {9783642229091 9783642229091 3642229093 9783642229107},
issn = {18792782},
keywords = {Bayes-Netz,Bootstrap-Aggregation,Computational Intelligence,Ensembles in Machine Learning Applications,Fehlerkorrekturcode,Hardback,Klassifikator,Merkmalsextraktion,Research,Set theory,Soft Computing,Un{\"{u}}berwachtes Lernen,machine learning,{\"{U}}berwachtes Lernen},
pages = {252},
pmid = {23459267},
title = {{Supervised Sequence Labelling with RNN}},
url = {files/1074/bok{\%}3A978-3-642-24797-2.pdf},
year = {2011}
}
@article{Corominas-Murtra2018,
abstract = {A major problem for evolutionary theory is understanding the so-called open-ended nature of evolutionary change, from its definition to its origins. Open-ended evolution (OEE) refers to the unbounded increase in complexity that seems to characterize evolution on multiple scales. This property seems to be a characteristic feature of biological and technological evolution and is strongly tied to the generative potential associated with combinatorics, which allows the system to grow and expand their available state spaces. Interestingly, many complex systems presumably displaying OEE, from language to proteins, share a common statistical property: the presence of Zipf's Law. Given an inventory of basic items (such as words or protein domains) required to build more complex structures (sentences or proteins) Zipf's Law tells us that most of these elements are rare whereas a few of them are extremely common. Using algorithmic information theory, in this paper we provide a fundamental definition for open-endedness, which can be understood as postulates. Its statistical counterpart, based on standard Shannon information theory, has the structure of a variational problem which is shown to lead to Zipf's Law as the expected consequence of an evolutionary process displaying OEE. We further explore the problem of information conservation through an OEE process and we conclude that statistical information (standard Shannon information) is not conserved, resulting in the paradoxical situation in which the increase of information content has the effect of erasing itself. We prove that this paradox is solved if we consider non-statistical forms of information. This last result implies that standard information theory may not be a suitable theoretical framework to explore the persistence and increase of the information content in OEE systems.},
archivePrefix = {arXiv},
arxivId = {1612.01605},
author = {Corominas-Murtra, Bernat and Seoane, Lu{\'{i}}s F. and Sol{\'{e}}, Ricard},
doi = {10.1098/rsif.2018.0395},
eprint = {1612.01605},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/rsif.2018.0395.pdf:pdf},
issn = {17425662},
journal = {Journal of the Royal Society Interface},
keywords = {Zipf's Law,algorithmic complexity,complexity,open-ended evolution},
number = {149},
title = {{Zipf's Law, unbounded complexity and open-ended evolution}},
volume = {15},
year = {2018}
}
@article{Maass2014,
abstract = {We are used to viewing noise as a nuisance in computing systems. This is a pity, since noise will be abundantly available in energy-efficient future nanoscale devices and circuits. I propose here to learn from the way the brain deals with noise, and apparently even benefits from it. Recent theoretical results have provided insight into how this can be achieved: how noise enables networks of spiking neurons to carry out probabilistic inference through sampling and also enables creative problem solving. In addition, noise supports the self-organization of networks of spiking neurons, and learning from rewards. I will sketch here the main ideas and some consequences of these results. I will also describe why these results are paving the way for a qualitative jump in the computational capability and learning performance of neuromorphic networks of spiking neurons with noise, and for other future computing systems that are able to treat noise as a resource. {\textcopyright} 2014 IEEE.},
author = {Maass, Wolfgang},
doi = {10.1109/JPROC.2014.2310593},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/10.1.1.701.7037.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Computational power,Stochastic computing,neural networks,neuromorphic hardware,noise,self-organization,spiking neurons},
number = {5},
pages = {860--880},
title = {{Noise as a resource for computation and learning in networks of spiking neurons}},
volume = {102},
year = {2014}
}
@article{Schwarz2016,
author = {Schwarz, Max and Beul, Marius and Droeschel, David and Sch, Sebastian and Periyasamy, Selvam and Lenz, Christian and Schreiber, Michael and Behnke, Sven},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/479363{\_}De{\_}Silva{\_}Manuscript.PDF:PDF},
journal = {Frontiers in Robotics and AI},
keywords = {manipulation,mapping,mobile manipulation,navigation,perception for grasping and,space robotics and automation},
title = {{In w e i v re In w e i v re}},
year = {2016}
}
@article{Kappel2015,
author = {Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
doi = {10.1371/journal.pcbi.1004485},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/journal.pcbi.1004485.PDF:PDF},
pages = {1--31},
title = {{Network Plasticity as Bayesian Inference}},
year = {2015}
}
@article{Seoane2016,
author = {Seoane, F},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/tlfs.pdf:pdf},
pages = {1--312},
title = {{Multiobjective Optimization in Models of Synthetic and Natural Living Systems Lu ´}},
year = {2016}
}
@article{OHare2015,
abstract = {Model parameter inference has become increasingly popular in recent years in the field of computational epidemiology, especially for models with a large number of parameters. Techniques such as Approximate Bayesian Computation (ABC) or maximum/partial likelihoods are commonly used to infer parameters in phenomenological models that best describe some set of data. These techniques rely on efficient exploration of the underlying parameter space, which is difficult in high dimensions, especially if there are correlations between the parameters in the model that may not be known a priori. The aim of this article is to demonstrate the use of the recently invented Adaptive Metropolis algorithm for exploring parameter space in a practical way through the use of a simple epidemiological model.},
author = {O'Hare, Anthony},
doi = {10.1089/cmb.2015.0086},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {1999,also,antigen,bola,bola-drb3,bovine,called the bovine leucocyte,complex,genotyping,includes many immune-related genes,lewin et al,mhc,of cattle,the major histocompatibility complex},
number = {11},
pages = {997--1004},
pmid = {26176624},
title = {{Inference in High-Dimensional Parameter Space}},
url = {http://online.liebertpub.com/doi/10.1089/cmb.2015.0086},
volume = {22},
year = {2015}
}
@article{Durov2017,
author = {Durov, Nikolai},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/TON Technology.pdf:pdf},
pages = {1--132},
title = {{Telegram Open Network}},
year = {2017}
}
@article{Cozzo2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.1759v4},
author = {Cozzo, Emanuele and Moreno, Yamir},
eprint = {arXiv:1311.1759v4},
file = {:home/melian/Downloads/1311.1759.pdf:pdf},
title = {{Dimensionality reduction and spectral properties of multilayer networks}},
year = {2018}
}
@article{Tarantola:2006,
author = {Tarantola, A},
journal = {Nature physics},
pages = {492--494},
title = {{Popper, {\{}B{\}}ayes and the inverse problem}},
volume = {2},
year = {2006}
}
@book{PERKINS2009,
author = {PERKINS, JOHN},
booktitle = {Berrett-Koehler},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/john{\_}perkins{\_}confessions{\_}of{\_}an{\_}economic{\_}hit{\_}man.pdf:pdf},
isbn = {1576753018},
number = {6},
pages = {14--21},
title = {{Confession of an Economic Hitman}},
volume = {67},
year = {2009}
}
@article{Grelaudetal:2009,
author = {{Grelaud A.} and {Robert C. P.} and {Marin J-M.} and {Rodolphe F.} and Taly, J-F.},
journal = {Bayesian Analysis},
pages = {317--336},
title = {{{\{}ABC{\}} likelihood-free methods for model choice in {\{}G{\}}ibbs random fields}},
volume = {4},
year = {2009}
}
@article{Kivelaetal:2014,
author = {{Kivela M.} and {Arenas A.} and {Barthelemy M.} and {Gleeson J. P.} and {Moreno Y.} and Porter, M A},
journal = {Journal of Complex Networks},
pages = {203--271},
title = {{Multilayer networks}},
volume = {3},
year = {2014}
}
@article{Seoane2020,
author = {Seoane, F and Sol, Ricard},
doi = {10.20944/preprints202001.0007.v1},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/preprints202001.0007.v1.pdf:pdf},
keywords = {bottleneck method,pareto-optimality,phase transitions,statistical mechanics,syntax},
number = {January},
pages = {1--11},
title = {{Criticality in Pareto Optimal Grammars ?}},
year = {2020}
}
@article{Ghahramani:2015,
author = {Ghahramani, Z},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Ghahramani 2015 Nature.pdf:pdf},
journal = {Nature},
pages = {452--459},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@book{MarkowitzBook,
author = {Markowitz, H},
publisher = {Blackwell Publishing, MA},
title = {{Portfolio selection}},
year = {1991}
}
@article{Ali,
author = {Ali, M.},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Blockstack.pdf:pdf},
title = {{Blockstack Technical}}
}
@article{SobolevAlexeyandSchneider2019,
author = {{Sobolev, Alexey and Schneider}, Lisa},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Industry{\_}Marketplace{\_}Technical{\_}Documentation.pdf:pdf},
title = {{Industry Marketplace Technical Documentation}},
year = {2019}
}
@article{DeDomenico2015a,
abstract = {Many complex systems can be represented as networks consisting of distinct types of interactions, which can be categorized as links belonging to different layers. For example, a good description of the full protein-protein interactome requires, for some organisms, up to seven distinct network layers, accounting for different genetic and physical interactions, each containing thousands of protein-protein relationships. A fundamental open question is then how many layers are indeed necessary to accurately represent the structure of a multilayered complex system. Here we introduce a method based on quantum theory to reduce the number of layers to a minimum while maximizing the distinguishability between the multilayer network and the corresponding aggregated graph. We validate our approach on synthetic benchmarks and we show that the number of informative layers in some real multilayer networks of protein-genetic interactions, social, economical and transportation systems can be reduced by up to 75{\%}.},
archivePrefix = {arXiv},
arxivId = {1405.0425},
author = {{De Domenico}, Manlio and Nicosia, Vincenzo and Arenas, Alexandre and Latora, Vito},
doi = {10.1038/ncomms7864},
eprint = {1405.0425},
file = {::},
isbn = {2041-1723 (Electronic)$\backslash$r2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pages = {1--9},
pmid = {25904309},
publisher = {Nature Publishing Group},
title = {{Structural reducibility of multilayer networks}},
url = {http://dx.doi.org/10.1038/ncomms7864},
volume = {6},
year = {2015}
}
@book{Anderson,
author = {Anderson, Carl},
file = {:home/melian/Documents/mendeleydesktop-1.17.11-linux-x86{\_}64/Refs/Creating a Data-Driven Organization.pdf:pdf},
isbn = {9781491916919},
title = {{Creating a Organization}}
}

@article{Mastrangelo2019,
abstract = {Regional and global assessments periodically update what we know, and highlight what remains to be known, about the linkages between people and nature that both define and depend upon the state of the environment. To guide research that better informs policy and practice, we systematically synthesize knowledge gaps from recent assessments of four regions of the globe and three key themes by the Intergovernmental Science-Policy Platform for Biodiversity and Ecosystem Services. We assess their relevance to global sustainability goals and trace their evolution relative to those identified in the Millennium Ecosystem Assessment. We found that global sustainability goals cannot be achieved without improved knowledge on feedbacks between social and ecological systems, effectiveness of governance systems and the influence of institutions on the social distribution of ecosystem services. These top research priorities have persisted for the 14 years since the Millennium Ecosystem Assessment. Our analysis also reveals limited understanding of the role of indigenous and local knowledge in sustaining nature's benefits to people. Our findings contribute to a policy-relevant and solution-oriented agenda for global, long-term social-ecological research.},
author = {Mastr\'angelo, Mat\'ias E and P\'erez-Harguindeguy, Natalia and Enrico, Lucas and Bennett, Elena and Lavorel, Sandra and Cumming, Graeme S and Abeygunawardane, Dilini and Amarilla, Leonardo D and Burkhard, Benjamin and Egoh, Benis N and Frishkoff, Luke and Galetto, Leonardo and Huber, Sibyl and Karp, Daniel S and Ke, Alison and Kowaljow, Esteban and Kronenburg-Garc\'ia, Angela and Locatelli, Bruno and Mart\'in-L\'opez, Berta and Meyfroidt, Patrick and Mwampamba, Tuyeni H and Nel, Jeanne and Nicholas, Kimberly A and Nicholson, Charles and Oteros-Rozas, Elisa and Rahlao, Sebataolo J and Raudsepp-Hearne, Ciara and Ricketts, Taylor and Shrestha, Uttam B and Torres, Carolina and Winkler, Klara J and Zoeller, Kim},
doi = {10.1038/s41893-019-0412-1},
issn = {2398-9629},
journal = {Nature Sustainability},
number = {12},
pages = {1115--1121},
title = {{Key knowledge gaps to achieve global sustainability goals}},
url = {https://doi.org/10.1038/s41893-019-0412-1},
volume = {2},
year = {2019}
}


@article{Androulaki2018,
abstract = {Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains and one of the Hyperledger projects hosted by the Linux Foundation (www.hyperledger.org). Fabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in standard, general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing blockchain platforms that require "smart-contracts" to be written in domain-specific languages or rely on a cryptocurrency. Fabric realizes the permissioned model using a portable notion of membership, which may be integrated with industry-standard identity management. To support such flexibility, Fabric introduces an entirely novel blockchain design and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks. This paper describes Fabric, its architecture, the rationale behind various design decisions, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency, scaling well to over 100 peers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.10228v2},
author = {Androulaki, Elli and Barger, Artem and Bortnikov, Vita and Muralidharan, Srinivasan and Cachin, Christian and Christidis, Konstantinos and {De Caro}, Angelo and Enyeart, David and Murthy, Chet and Ferris, Christopher and Laventman, Gennady and Manevich, Yacov and Nguyen, Binh and Sethi, Manish and Singh, Gari and Smith, Keith and Sorniotti, Alessandro and Stathakopoulou, Chrysoula and Vukoli{\'{c}}, Marko and Cocco, Sharon Weed and Yellick, Jason},
doi = {10.1145/3190508.3190538},
eprint = {arXiv:1801.10228v2},
isbn = {9781450355841},
journal = {Proceedings of the 13th EuroSys Conference, EuroSys 2018},
title = {{Hyperledger Fabric: A Distributed Operating System for Permissioned Blockchains}},
volume = {2018-Janua},
year = {2018}
}
