Robhoot aims to fully automate the research cycle in a open decentralized network. Research automation with reporting generation might help to contrast informed decisions when solving complex social, environmental and technological problems. Current technologies for scientific inquiry and decision-making are highly fragmented and thus only increase robustness, reproducibility, open-access and the interactions with the public marginally. The goal of Robhoot is to propose a hybrid-neutral-technology to lay out the foundation for an open-science research ecosystem aiming to strengthen the robustness and reproducibility of science. Robhoot is not set out to deliver a finished research open network in the science ecosystem, but to provide a science-enabled technology in establishing a prototype proof-of-principle to connect automated, decentralized and neutral-knowledge generation with knowledge-inspired societies.

Robhoot will be developed in four different stages following standard version protocols (Robhoot 1.0 to 4.0). The most advanced version is to provide real-time open-access reporting by a decentralized neutral-knowledge network to gain informed decisions when solving complex social, environmental and technological problems. Automating the research cycle in a open research network ultimately aims to contrast human-produced science with machine-produced science to enrich the human-knowledge graphs in a neutral-knowledge-inspired society. The following is a brief description of each four Robhoot versions:


Robhoot 1.0: Automated Research Cycle
  
  Develop, deploy and integrate open-source algorithms to fully automate the research cycle.
  
  Exploration of robustness within and between layers from data integration, complexity reduction, inference, and validation to visualization and automated reporting.
  
  Robhoot 1.0 testnet to explore a Open Research Network in Biodiversity and Global Change Research to connect open science (i.e., citizen and other data-driven models) to real-time
  open-access knowledge generation to gain informed decisions when solving local and global environmental problems.
   
  Tools and Methods: Multilayer networks, Bayesian Networks, Network metrics, Julia computing language, Open-source software protocols, Gitchain, ETLs algorithms, Kafka, Clickhouse.
   

Robhoot 1.0 will automate algorithms within and between layers to explore the robustness of the scientific process:
______________________________________________________________________________________________________________________________________________________________________________________
## 1. Data Integration

Data come from many sources. Yet, inferring insightful patterns and drivers integrating heterogeneous datasets to fully automated reporting remain challenging. 
Robhoot aims to develop approaches to integrate data, statistical Learning, AI algorithms and process-based models to take better informed decisions when solving complex social, environmental and technological problems. A generalized algorithm will be developed to extract, transform and load data (ETLs) from many (heterogeneous) sources for analysis. The integrated data will then be available to analyze in python-Julia or other computer languages. ETLs algorithms include "real time data" -- for example using GraphQL or similar. Robhoot v.1.0 aims to have a generalized funcitonal open-source package to let the user automatically get the integrated data in their desired format.

## 2. Complexity Reduction (CR)

Robhoot will reduce data dimensionality using a variety of complexity reduction methods. Robhoot will combine classical PCA, correlation methods and information theory metrics in multilayer networks to contrast the robutness of the different data complexity reduction methods.

## 3. Pattern Process Inference (PPI)

Many patterns can be extracted from data. Robhoot will exmplore a suite of methods from classic variance-covariance matrix methods to AI, machine learning, and process-based stochastic and deterministic modeling to understand the processes underlying the patterns.

## 4. Validation (VA)

Robhoot will use different model selection criteria accounting for uncertainty in the parameter estimations using information theory methods, Bayesian Inference and Approximate Bayesian Computation methods. 

## 5. Visualization (VI)

Robhoot will integrate automated plotting and visualization tools (e.g., [Plotly](https://plot.ly/), [JoVE](https://www.jove.com/visualize)) to visualize the empirical patterns and the models that best predict the empirical patterns. Robhoot will summarize the patterns and processes in compact and clean animation tools to help to improve the multilayer protocol and in making decisions in research, management and investment landscapes.

## 6. Reporting (RE)

Robhoot will produce fully automated standard reporting accounting for the population of explored paths (i.e., each path connects the different layers).
_______________________________________________________________________________________________________________________________________________________________________________________



  Robhoot 2.0: Knowledge Graphs
  
  Implementation of algorithms to reproduce paths of the research cycle with Knowledge Graphs (KGs).
 
  Robustness and stability exploring a suite of open-source lineage client-tracker algorithms.
  
  Tools and Methods: Knowlegde graph algorithms and packages (i.e., Renku and others).
  


  Robhoot 3.0: Deep learning networks

  Deploy automated deep learning algorithms to sample paths of the research cycle to produce populations of Knowledge Graphs (KGs).
  
  Exploration of the robustness of automated research cycle combining optimization algorithms and the population of Knowledge Graphs.

  Tools and Methods: Multilayer networks, Neural Biological Networks, Bayesian Networks, Deep learning networks. Optimization algorithms.


    
  Robhoot 4.0: Distributed ledger network
  
  Deploy a permissioned-permissionless distributed ledger technology to guarantee decentralization, open-access, neutral-knowledge-based network and prior confidenciality/posterior   	  reproducibility of the KGs populations.
   
  Exploration of a suite of consensus algorithms and smart contracts among trusted-untrusted peer-to-peer interactions to infer macroscopic metrics of the open research network.

  Quantification of metrics to study the scalability-security-decentralization trade-offs when storing KGs in the research network.
  
  Testnet case study to explore the interaction between consensus protocols and the scalability-security-decentralization trade-offs when committing the KGs to the distributed ledger.
  
  Mainnet to cryptographically link each population of KGs to previous KGs-ledger to create an historical KGs-ledger chain that goes back to the genesis ledger in the open research network. The mainnet aims to connect multiple database with real-time open-access citizen data science to knowledge-inspired societies.

   Tools and Methods: Distributed computing algorithms, Blockchain and consensus algorithms, BighainDB, Gitchain. Telegram open network, Golem.


