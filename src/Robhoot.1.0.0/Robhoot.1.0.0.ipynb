{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "# Robhoot 1.0.0\n",
    "# Julia Install packages \n",
    "# Version 1.7.1 (2021-12-22)\n",
    "# Official https://julialang.org/ release\n",
    "\n",
    "\"\"\"\"\"\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Query\n",
    "using JDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3dbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Filtering DATRAS database.\n",
    "https://www.ices.dk/data/data-portals/Pages/DATRAS.aspx\n",
    "\n",
    "Returns a DataFrame that is the merge of haul and lengths. T\n",
    "axa is already incorporated into the length file.\n",
    "\n",
    "Ali Vahdati 29 January 2021\n",
    "\n",
    "# Inputs\n",
    "* HHd.csv: haul data\n",
    "* HLd.csv: lenght-based data\n",
    "\"\"\"\n",
    "function merge_haul_length(haul_file= \"HLd_first10000.csv\", length_file=\"HLd.csv\", lengths_file_small = \"HLd_first10000.csv\")\n",
    "  classes = [\"Myxini\", \"Petromyzonti\", \"Elasmobranchii\", \"Holocephali\", \"Actinopterygii\"]\n",
    "  surveys = [\"BITS\", \"NS-IBTS\", \"SP-PORC\", \"SCOWCGFS\", \"ROCKALL\", \"SCOROC\", \"DWS\", \"IE-IGFS\", \"IE-IAMS\", \"NIGFS\", \"SP-NORTH\", \"FR-CGFS\", \"EVHOE\", \"SP-ARSA\", \"PT-IBTS\", \"SNS\"]\n",
    "  ranks = [\"Species\", \"Subspecies\"]\n",
    "  common_names = [\"Survey\", \"Year\", \"Quarter\", \"Country\", \"Gear\", \"HaulNo\"]\n",
    "  haul = CSV.read(haul_file, DataFrame);\n",
    "  lengths_small = CSV.read(lengths_file_small, DataFrame);\n",
    "  \n",
    "  lengths_small = lengths_small[union(findall(x-> in(x, ranks), lengths_small.Rank), findall(x-> in(x, classes), lengths_small.Class), findall(x-> in(x, surveys), lengths_small.Survey)), :];\n",
    "  \n",
    "  ## Get the common set between Hld and HHd\n",
    "  merged = @from i in lengths_small begin\n",
    "    @let it = join([i.Survey, i.Year, i.Quarter, i.Country, i.Gear, i.HaulNo], \",\")\n",
    "      @join j in haul on it equals join([j.Survey, j.Year, j.Quarter, j.Country, j.Gear, j.HaulNo], \",\")\n",
    "      @select {i.Survey, i.Year, i.Quarter, i.Country, i.Gear,\n",
    "      i.HaulNo, i.Ship, i.GearExp, j.DoorType, i.SpecCodeType,\n",
    "      i.SpecCode, i.SpecVal, i.Sex, i.TotalNo, i.CatIdentifier, i.NoMeas, i.SubFactor, i.SubWgt,\n",
    "      i.CatCatchWgt, i.LngtClass, i.LngtCode, i.HLNoAtLngt, i.DateofCalculation, i.Valid_Aphia,\n",
    "      i.AphiaID, i.Scientificname, i.Status, i.Rank, i.Valid_name, i.Genus, i.Family, i.Order,\n",
    "      i.Class, i.Phylum,\n",
    "      j.Month, j.Day, j.Date, j.TimeShot, j.HaulDur, j.ShootLat, j.ShootLong}\n",
    "      @collect DataFrame                               \n",
    "  end\n",
    "  \n",
    "  return merged\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data = merge_haul_length()\n",
    "\n",
    "# merged data is in DBa.csv\n",
    "db = CSV.read(\"DBa.csv\", DataFrame) \n",
    "#db = CSV.read(\"data/large/DBa.csv\", DataFrame) @CM Feb 4 2023\n",
    "\n",
    "# describe(db)\n",
    "db = db[!, 9:end]\n",
    "\n",
    "excluded_vars = [:RecordType, :Status, :SweepLngt, :GearExp, :DoorType, :StNo, :HaulNo, :SpecCode,   :SpecCodeType, :SpecVal, :CatIdentifier, :TotalNo, :NoMeas, :SubFactor, :SubWgt, :CatCatchWgt, :LngtCode, :HLNoAtLngt, :DateofCalculation, :Valid_Aphia, :Valid_name, :Rank, :Phylum, :ID, :TimeShot, :HaulVal, :StdSpecRecCode, :BycSpecRecCode, :DataType, :DateTime, :Sex, :AphiaID, :Date]\n",
    "\n",
    "remained_vars = [i for i in names(db) if !in(Symbol(i), excluded_vars)]\n",
    "\n",
    "db_final = db[!, remained_vars]\n",
    "des = describe(db_final)\n",
    "\n",
    "# remove rows with NA.\n",
    "\n",
    "# for col in 1:size(db_final, 2)\n",
    "#   if !isnothing(findfirst(x-> x == \"NA\", db_final[:, col]))\n",
    "#     println(col)\n",
    "#   end\n",
    "# end\n",
    "# # 7, 19\n",
    "# names(db_final)[[7,19]]  # \"LngtClass\", \"Depth\"\n",
    "# count(x-> x==\"NA\", db_final.Depth) # 13105\n",
    "# count(x-> x==\"NA\", db_final.LngtClass) # 18928\n",
    "\n",
    "j = findall(x-> x==\"NA\", db_final.Depth);\n",
    "j2 = findall(x-> x==\"NA\", db_final.LngtClass);\n",
    "j3 = union(j, j2);\n",
    "newrows = setdiff(1:size(db_final, 1), j3)\n",
    "\n",
    "db_final = db_final[newrows, :];\n",
    "\n",
    "# # remove rows with HaulDur == 0\n",
    "# db_final = db_final[db_final.HaulDur .!= 0, :]\n",
    "\n",
    "## Convert LngtClass and Depth to integers\n",
    "# db_final[!, :LngtClass2] = parse.(Int, db_final.LngtClass)\n",
    "# newnames = names(db_final)\n",
    "# splice!(newnames, findfirst(x -> x==\"LngtClass\", newnames))\n",
    "# db_final = db_final[!, newnames]\n",
    "# rename!(db_final, :LngtClass2 => :LngtClass)\n",
    "\n",
    "db_final[!, :Depth2] = parse.(Int, db_final.Depth)\n",
    "newnames = names(db_final)\n",
    "splice!(newnames, findfirst(x -> x==\"Depth\", newnames))\n",
    "db_final = db_final[!, newnames]\n",
    "rename!(db_final, :Depth2 => :Depth)\n",
    "\n",
    "# # remove Infs from CPUE_number_per_hour\n",
    "# keeprows = findall(x-> x != Inf, db_final.CPUE_number_per_hour)\n",
    "# db_final = db_final[keeprows, :]\n",
    "\n",
    "# Save with JDF for compressed saving and fast loading\n",
    "#JDF.save(\"data/large/DB_cleaned.jdf\", db_final) @CM Feb 4 2023\n",
    "JDF.save(\"DB_cleaned.jdf\", db_final)\n",
    "#Load it with the command below:\n",
    "#df = DataFrame(JDF.load(\"data/large/DB_cleaned.jdf\")) @CM Feb 4 2023\n",
    "df = DataFrame(JDF.load(\"DB_cleaned.jdf\"))\n",
    "\n",
    "## TODO: Descretize the data\n",
    "using Discretizers\n",
    "\n",
    "dfd = DataFrame()\n",
    "# discretizers\n",
    "surv_disc = CategoricalDiscretizer(df.Survey)\n",
    "country_disc = CategoricalDiscretizer(df.Country)\n",
    "ship_disc = CategoricalDiscretizer(df.Ship)\n",
    "gear_disc = CategoricalDiscretizer(df.Gear)\n",
    "year_disc = CategoricalDiscretizer(df.Year)\n",
    "name_disc = CategoricalDiscretizer(df.Scientificname)\n",
    "genus_disc = CategoricalDiscretizer(df.Genus)\n",
    "family_disc = CategoricalDiscretizer(df.Family)\n",
    "order_disc = CategoricalDiscretizer(df.Order)\n",
    "class_disc = CategoricalDiscretizer(df.Class)\n",
    "hauldur_disc = LinearDiscretizer(binedges(DiscretizeUniformWidth(10), df.HaulDur))\n",
    "daynight_disc = CategoricalDiscretizer(df.DayNight)\n",
    "lat_disc = LinearDiscretizer(binedges(DiscretizeUniformWidth(10), df.ShootLat))\n",
    "lon_disc = LinearDiscretizer(binedges(DiscretizeUniformWidth(10), df.ShootLong))\n",
    "count_disc = LinearDiscretizer(binedges(DiscretizeUniformWidth(20), df.ShootLong))\n",
    "length_disc = LinearDiscretizer(binedges(DiscretizeUniformWidth(50), df.LngtClass))\n",
    "depth_disc = LinearDiscretizer(binedges(DiscretizeUniformWidth(10), df.Depth))\n",
    "\n",
    "# discretized df\n",
    "dfd[!, :survey] = df.Survey #encode(surv_disc, df.Survey)\n",
    "dfd[!, :quarter] = df.Quarter\n",
    "dfd[!, :country] = df.Country # encode(country_disc, df.Country)\n",
    "dfd[!, :ship] = df.Ship # encode(ship_disc, df.Ship)\n",
    "dfd[!, :gear] = df.Gear # encode(gear_disc, df.Gear)\n",
    "dfd[!, :year] = df.Year # encode(year_disc, df.Year)\n",
    "dfd[!, :name] = df.Scientificname  # encode(name_disc, df.Scientificname)\n",
    "dfd[!, :genus] = df.Genus  # encode(genus_disc, df.Genus)\n",
    "dfd[!, :family] = df.Family  # encode(family_disc, df.Family)\n",
    "dfd[!, :order] = df.Order  # encode(order_disc, df.Order);\n",
    "dfd[!, :class] = df.Class  # encode(class_disc, df.Class);\n",
    "dfd[!, :month] = df.Month;\n",
    "dfd[!, :day] = df.Day;\n",
    "dfd[!, :hauldur] = encode(hauldur_disc, df.HaulDur);\n",
    "dfd[!, :daynight] = df. DayNight  # encode(daynight_disc, df.DayNight);\n",
    "dfd[!, :lat] = encode(lat_disc, df.ShootLat);\n",
    "dfd[!, :lon] = encode(lon_disc, df.ShootLong);\n",
    "dfd[!, :count] = encode(count_disc, df.CPUE_number_per_hour);\n",
    "dfd[!, :length] = encode(length_disc, df.LngtClass);\n",
    "dfd[!, :depth] = encode(depth_disc, df.Depth);\n",
    "\n",
    "CSV.write(\"DB_cleaned_discretized.csv\", dfd)\n",
    "\n",
    "\n",
    "# Discretie all\n",
    "# discretized df\n",
    "dfd[!, :survey] = encode(surv_disc, df.Survey)\n",
    "dfd[!, :quarter] = df.Quarter\n",
    "dfd[!, :country] = encode(country_disc, df.Country)\n",
    "dfd[!, :ship] = encode(ship_disc, df.Ship)\n",
    "dfd[!, :gear] = encode(gear_disc, df.Gear)\n",
    "dfd[!, :year] = encode(year_disc, df.Year)\n",
    "dfd[!, :name] = encode(name_disc, df.Scientificname)\n",
    "dfd[!, :genus] = encode(genus_disc, df.Genus)\n",
    "dfd[!, :family] = encode(family_disc, df.Family)\n",
    "dfd[!, :order] = encode(order_disc, df.Order);\n",
    "dfd[!, :class] = encode(class_disc, df.Class);\n",
    "dfd[!, :month] = df.Month;\n",
    "dfd[!, :day] = df.Day;\n",
    "dfd[!, :hauldur] = encode(hauldur_disc, df.HaulDur);\n",
    "dfd[!, :daynight] = encode(daynight_disc, df.DayNight);\n",
    "dfd[!, :lat] = encode(lat_disc, df.ShootLat);\n",
    "dfd[!, :lon] = encode(lon_disc, df.ShootLong);\n",
    "dfd[!, :count] = encode(count_disc, df.CPUE_number_per_hour);\n",
    "dfd[!, :length] = encode(length_disc, df.LngtClass);\n",
    "dfd[!, :depth] = encode(depth_disc, df.Depth);\n",
    "\n",
    "CSV.write(\"DB_cleaned_discretized_all.csv\", dfd)\n",
    "\n",
    "\n",
    "# Structure learning using greedy hill climbing\n",
    "using BayesNets\n",
    "#df = CSV.read(\"large/DB_cleaned_discretized_all.csv\", DataFrame) @CM Feb 4 2023 \n",
    "df = CSV.read(\"DB_cleaned_discretized_all.csv\", DataFrame)\n",
    "\n",
    "parameters = GreedyHillClimbing(ScoreComponentCache(df), max_n_parents=15, prior=UniformPrior())\n",
    "bn = fit(DiscreteBayesNet, df, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4dae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------------------------------------------\n",
    "## Choose species in a specific region\n",
    "##-------------------------------------------\n",
    "using Statistics: mean\n",
    "using BayesNets\n",
    "using Discretizers\n",
    "using GraphPlot\n",
    "using Compose, Cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a table where columns are species and rows are unique sampling events (same date). \n",
    "Values are species abundance. This is called converting from long format to wide format.\n",
    "\"\"\"\n",
    "function survey_species(df, survey)\n",
    "  df2 = df[df.Survey .== survey, :]\n",
    "  # First, combine all length classes\n",
    "  # TODO: also combine smaller sampling regions. Need data from Paco\n",
    "  df2[!, :Date] = [join([a,b,c], \"-\") for (a,b,c) in zip(df2.Day, df2.Month, df2.Year)];\n",
    "  grouped = DataFrames.groupby(df2, [:Scientificname, :Date])\n",
    "  cc = combine(grouped, :CPUE_number_per_hour => sum)\n",
    "  # Now unstack\n",
    "  df2_wide = unstack(cc, :Scientificname, :CPUE_number_per_hour_sum, allowduplicates=false)\n",
    "  # Replace missings with zeros\n",
    "  df2_wide = coalesce.(df2_wide, 0)\n",
    "  # Remove species that are rare\n",
    "  colsums = [count(x -> x>0, df2_wide[:, i]) for i in 2:size(df2_wide, 2)]\n",
    "  # using Plots\n",
    "  # histogram(colsums)\n",
    "  common_ids = findall(x-> x>100, colsums)\n",
    "\n",
    "  outdf = df2_wide[:, common_ids .+ 1]\n",
    "\n",
    "  #conver to Integer\n",
    "  for col in 1:size(outdf,2)\n",
    "    outdf[!, col] = round.(Int64, outdf[:, col])\n",
    "  end\n",
    "  return outdf\n",
    "end\n",
    "\n",
    "#df = DataFrame(JDF.load(\"data/large/DB_cleaned.jdf\")) @CM Feb 4 2023\n",
    "df = DataFrame(JDF.load(\"DB_cleaned.jdf\"))\n",
    "\n",
    "surveys = levels(df.Survey)\n",
    "surveys = surveys[8] # limiting to NS_IBTS as it is the oldest time-series.\n",
    "\n",
    "outfile = \"data/small/interacting_pairs_per_survey.csv\"\n",
    "ff = open(outfile, \"w\")\n",
    "header = join([\"child\", \"parent\", \"survey\"], \",\")\n",
    "println(header, ff)\n",
    "close(ff)\n",
    "\n",
    "open(outfile, \"a+\") do ff\n",
    "  for survey in surveys\n",
    "    speciesdf = survey_species(df, survey)\n",
    "\n",
    "    # Discretize counts\n",
    "    colnames = names(speciesdf)\n",
    "    dfd = DataFrame()\n",
    "    for col in 1:size(speciesdf, 2)\n",
    "      cc = LinearDiscretizer(binedges(DiscretizeUniformWidth(100), speciesdf[!, col]))\n",
    "      dfd[!, colnames[col]] = encode(cc, speciesdf[!, col]);\n",
    "    end\n",
    "\n",
    "    # CSV.write(\"data/large/NS_IBTS_species.csv\", dfd)\n",
    "\n",
    "    # Structure learning using greedy hill climbing\n",
    "    parameters = GreedyHillClimbing(ScoreComponentCache(dfd), max_n_parents=7, prior=UniformPrior())\n",
    "    bn = fit(DiscreteBayesNet, dfd, parameters)\n",
    "\n",
    "    # draw(PNG(\"plots/NS_IBTS_species.png\"), gplot(bn.dag))\n",
    "\n",
    "    have_parents = Symbol[]\n",
    "    for (k, v) in bn.name_to_index\n",
    "      cpd = bn.cpds[v]\n",
    "      if length(cpd.parents) > 0\n",
    "        push!(have_parents, k)\n",
    "      end\n",
    "    end\n",
    "\n",
    "    have_parents_ids = [bn.name_to_index[i] for i in have_parents]\n",
    "\n",
    "    for name in have_parents\n",
    "      id = bn.name_to_index[name]\n",
    "      prnts = String.(bn.cpds[id].parents)\n",
    "      for prnt in prnts\n",
    "        entry = join([String(name), prnt, survey], \",\")\n",
    "        println(ff, entry)\n",
    "      end\n",
    "      # println(name, \": \", join(prnts, \", \"))\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# julia> include(\"preprocessing.jl\")\n",
    "# Output\n",
    " Warning: infer_number_of_instantiations assumes values in 1:N, lowest value is 8!\n",
    "â”” @ BayesNets.CPDs ~/.julia/packages/BayesNets/hZr0W/src/CPDs/utils.jl:64\n",
    "Killed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7316cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes networks bnlearn.jl\n",
    "\"\"\"\n",
    "Julia\n",
    "(@v1.7) pkg> add RCall\n",
    "https://www.bnlearn.com/\n",
    "\n",
    "R\n",
    "install.packages(\"bnlearn\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc926ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(bnlearn)\n",
    "\n",
    "# df = read.csv(\"data/large/DB_cleaned_discretized.csv\") @CM Feb 4 2023\n",
    "df = read.csv(\"DB_cleaned_discretized.csv\")\n",
    "\n",
    "## change data type to factor because bnlearn needs that\n",
    "df[, 1:20] <- lapply(df[,1:20], as.factor)\n",
    "\n",
    "df2 = df[, -c(1,13)]\n",
    "\n",
    "# dim(df)\n",
    "# data(learning.test)\n",
    "\n",
    "# Tutorials: https://www.bnlearn.com/examples/\n",
    "\n",
    "## Get a score of bn (learn.net) given the data (learning.test). Other types: aic, bde, etc. (see `score` help)\n",
    "# score(learn.net, learning.test, type = \"bic\")\n",
    "\n",
    "## Whitelists and blacklists in structure learning\n",
    "\"\"\"\n",
    "* Arcs in the whitelist are always included in the network.\n",
    "* Arcs in the blacklist are never included in the network.\n",
    "* Any arc whitelisted and blacklisted at the same time is assumed to be whitelisted, and is thus removed from the blacklist. In other words, the whitelist has precedence over the blacklist.\n",
    "\"\"\"\n",
    "\n",
    "#wl = read.csv(\"data/small/node_whitelist.csv\") @CM Feb 4 2023\n",
    "#bl = read.csv(\"data/small/node_blacklist.csv\") @CM Feb 4 2023\n",
    "#bl2 = read.csv(\"data/small/node_blacklist_noSurvey.csv\") @CM Feb 4 2023\n",
    "\n",
    "wl = read.csv(\"Data/small/node_whitelist.csv\")\n",
    "bl = read.csv(\"Data/small/node_blacklist.csv\")\n",
    "bl2 = read.csv(\"Data/small/node_blacklist_noSurvey.csv\")\n",
    "\n",
    "\n",
    "## Structure learning with PC algorithm. Does not result in any reasonable network\n",
    "bn_pc = pc.stable(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_pc)\n",
    "# GS algorithm: Error in if (a <= alpha) { : missing value where TRUE/FALSE needed\n",
    "bn_gs = gs(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_gs)\n",
    "# IAMB algorithm: Error in if (a <= alpha) { : missing value where TRUE/FALSE needed\n",
    "bn_iamb = iamb(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_iamb)\n",
    "# inter-IAMB algorithm\n",
    "bn_interiamb = inter.iamb(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_interiamb)\n",
    "# IAMB-FDR algorithm: Error in if (a <= alpha) { : missing value where TRUE/FALSE needed\n",
    "bn_iambfdr = iamb.FDR(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_iambfdr)\n",
    "# MMPC algorithm\n",
    "bn_mmpc = mmpc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_mmpc)\n",
    "# SI.HITON-PC algorithm\n",
    "bn_sihitonpc = si.hiton.pc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_sihitonpc)\n",
    "# HC algorithm (score-based): The most reasonable so far\n",
    "bn_hc = hc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_hc)\n",
    "bn_hc_fitted = bn.fit(bn_hc, df)\n",
    "write.net(\"data/small/bn_hc.net\", bn_hc_fitted)\n",
    "\n",
    "bn_hc2 = hc(df2, whitelist=wl, blacklist=bl2)\n",
    "plot(bn_hc2)\n",
    "bn_hc_fitted2 = bn.fit(bn_hc2, df2)\n",
    "write.net(\"data/small/bn_hc_noDaySurvey.net\", bn_hc_fitted2)\n",
    "\n",
    "# To read again\n",
    "bn_hc_fitted2 = read.net(\"data/small/bn_hc_noDaySurvey.net\")\n",
    "# To plot\n",
    "# Install with: install.packages(\"BiocManager\"); BiocManager::install(\"Rgraphviz\")\n",
    "library(Rgraphviz)\n",
    "pdf(\"plots/network_structure.pdf\")\n",
    "graphviz.plot(bn_hc_fitted2)\n",
    "dev.off()\n",
    "\n",
    "# # HC algorithm (score-based) with restars\n",
    "# bn_hc_restart = hc(df, whitelist=wl, blacklist=bl, restart=100)\n",
    "# plot(bn_hc_restart)\n",
    "# bn_hc_fitted_restart = bn.fit(bn_hc_restart, df)\n",
    "# write.net(\"bn_hc_restart.net\", bn_hc_fitted_restart)\n",
    "# MMHC algorithm (hybrid)\n",
    "bn_mmhc = mmhc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_mmhc)\n",
    "# RSMAX2 algorithm (hybrid)\n",
    "bn_rsmax2 = rsmax2(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_rsmax2)\n",
    "# ARACNE (local discovery)\n",
    "bn_aracne = aracne(df, whitelist=wl, blacklist=bl, mi=\"mi\")\n",
    "plot(bn_aracne)\n",
    "# Chow-Liu (local discovery)\n",
    "bn_chowLiu = chow.liu(df, whitelist=wl, blacklist=bl, mi=\"mi\")\n",
    "plot(bn_chowLiu)\n",
    "\n",
    "##-----------------------------------------------\n",
    "## Species interaction networks NB: none of the networks have any edges\n",
    "##-----------------------------------------------\n",
    "\n",
    "# See preprocessing.jl for producing a CSV for a specific country.\n",
    "df = read.csv(\"data/large/DEN_species.csv\")\n",
    "\n",
    "## change data type to factor because bnlearn needs that\n",
    "df[, 1:ncol(df)] <- lapply(df[,1:ncol(df)], as.factor)\n",
    "\n",
    "\n",
    "# HC algorithm (score-based) with restars\n",
    "bn_hc_restart = hc(df, restart=100)\n",
    "plot(bn_hc_restart)\n",
    "# bn_hc_fitted_restart = bn.fit(bn_hc_restart, df)\n",
    "# write.net(\"DEN_bn_hc_restart.net\", bn_hc_fitted_restart)\n",
    "\n",
    "# To plot\n",
    "# Install with: install.packages(\"BiocManager\"); BiocManager::install(\"Rgraphviz\")\n",
    "library(Rgraphviz)\n",
    "pdf(\"plots/network_structure_DEN.pdf\")\n",
    "graphviz.plot(bn_hc_restart)\n",
    "dev.off()\n",
    "\n",
    "## Structure learning with PC algorithm. \n",
    "bn_pc = pc.stable(df)\n",
    "plot(bn_pc)\n",
    "# GS algorithm\n",
    "bn_gs = gs(df)\n",
    "plot(bn_gs)\n",
    "# IAMB algorithm\n",
    "bn_iamb = iamb(df)\n",
    "plot(bn_iamb)\n",
    "# inter-IAMB algorithm\n",
    "bn_interiamb = inter.iamb(df)\n",
    "plot(bn_interiamb)\n",
    "# IAMB-FDR algorithm\n",
    "bn_iambfdr = iamb.fdr(df)\n",
    "plot(bn_iambfdr)\n",
    "# MMPC algorithm\n",
    "bn_mmpc = mmpc(df)\n",
    "plot(bn_mmpc)\n",
    "# SI.HITON-PC algorithm\n",
    "bn_sihitonpc = si.hiton.pc(df)\n",
    "plot(bn_sihitonpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian network\n",
    "\n",
    "\"\"\"\n",
    "R\n",
    "https://cran.r-project.org/web/packages/bnlearn/index.html\n",
    "https://www.bnlearn.com/\n",
    "\"\"\"\n",
    "\n",
    "# To plot\n",
    "# Install with: install.packages(\"BiocManager\"); BiocManager::install(\"Rgraphviz\")\n",
    "\n",
    "\n",
    "library(bnlearn)\n",
    "\n",
    "# df = read.csv(\"data/large/DB_cleaned_discretized.csv\") @CM Feb 4 2023\n",
    "df = read.csv(\"DB_cleaned_discretized.csv\")\n",
    "\n",
    "## change data type to factor because bnlearn needs that\n",
    "df[, 1:20] <- lapply(df[,1:20], as.factor)\n",
    "\n",
    "df2 = df[, -c(1,13)]\n",
    "\n",
    "# dim(df)\n",
    "# data(learning.test)\n",
    "\n",
    "# Tutorials: https://www.bnlearn.com/examples/\n",
    "\n",
    "## Get a score of bn (learn.net) given the data (learning.test). Other types: aic, bde, etc. (see `score` help)\n",
    "# score(learn.net, learning.test, type = \"bic\")\n",
    "\n",
    "## Whitelists and blacklists in structure learning\n",
    "\"\"\"\n",
    "* Arcs in the whitelist are always included in the network.\n",
    "* Arcs in the blacklist are never included in the network.\n",
    "* Any arc whitelisted and blacklisted at the same time is assumed to be whitelisted, and is thus removed from the blacklist. In other words, the whitelist has precedence over the blacklist.\n",
    "\"\"\"\n",
    "\n",
    "#wl = read.csv(\"data/small/node_whitelist.csv\") @CM Feb 4 2023\n",
    "#bl = read.csv(\"data/small/node_blacklist.csv\") @CM Feb 4 2023\n",
    "#bl2 = read.csv(\"data/small/node_blacklist_noSurvey.csv\") @CM Feb 4 2023\n",
    "\n",
    "wl = read.csv(\"node_whitelist.csv\")\n",
    "bl = read.csv(\"node_blacklist.csv\")\n",
    "bl2 = read.csv(\"node_blacklist_noSurvey.csv\")\n",
    "\n",
    "\n",
    "## Structure learning with PC algorithm. Does not result in any reasonable network\n",
    "bn_pc = pc.stable(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_pc)\n",
    "# GS algorithm: Error in if (a <= alpha) { : missing value where TRUE/FALSE needed\n",
    "bn_gs = gs(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_gs)\n",
    "# IAMB algorithm: Error in if (a <= alpha) { : missing value where TRUE/FALSE needed\n",
    "bn_iamb = iamb(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_iamb)\n",
    "# inter-IAMB algorithm\n",
    "bn_interiamb = inter.iamb(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_interiamb)\n",
    "# IAMB-FDR algorithm: Error in if (a <= alpha) { : missing value where TRUE/FALSE needed\n",
    "bn_iambfdr = iamb.FDR(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_iambfdr)\n",
    "# MMPC algorithm\n",
    "bn_mmpc = mmpc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_mmpc)\n",
    "# SI.HITON-PC algorithm\n",
    "bn_sihitonpc = si.hiton.pc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_sihitonpc)\n",
    "# HC algorithm (score-based): The most reasonable so far\n",
    "bn_hc = hc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_hc)\n",
    "bn_hc_fitted = bn.fit(bn_hc, df)\n",
    "write.net(\"data/small/bn_hc.net\", bn_hc_fitted)\n",
    "\n",
    "bn_hc2 = hc(df2, whitelist=wl, blacklist=bl2)\n",
    "plot(bn_hc2)\n",
    "bn_hc_fitted2 = bn.fit(bn_hc2, df2)\n",
    "write.net(\"data/small/bn_hc_noDaySurvey.net\", bn_hc_fitted2)\n",
    "\n",
    "# To read again\n",
    "bn_hc_fitted2 = read.net(\"data/small/bn_hc_noDaySurvey.net\")\n",
    "# To plot\n",
    "# Install with: install.packages(\"BiocManager\"); BiocManager::install(\"Rgraphviz\")\n",
    "library(Rgraphviz)\n",
    "pdf(\"plots/network_structure.pdf\")\n",
    "graphviz.plot(bn_hc_fitted2)\n",
    "dev.off()\n",
    "\n",
    "# # HC algorithm (score-based) with restars\n",
    "# bn_hc_restart = hc(df, whitelist=wl, blacklist=bl, restart=100)\n",
    "# plot(bn_hc_restart)\n",
    "# bn_hc_fitted_restart = bn.fit(bn_hc_restart, df)\n",
    "# write.net(\"bn_hc_restart.net\", bn_hc_fitted_restart)\n",
    "# MMHC algorithm (hybrid)\n",
    "bn_mmhc = mmhc(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_mmhc)\n",
    "# RSMAX2 algorithm (hybrid)\n",
    "bn_rsmax2 = rsmax2(df, whitelist=wl, blacklist=bl)\n",
    "plot(bn_rsmax2)\n",
    "# ARACNE (local discovery)\n",
    "bn_aracne = aracne(df, whitelist=wl, blacklist=bl, mi=\"mi\")\n",
    "plot(bn_aracne)\n",
    "# Chow-Liu (local discovery)\n",
    "bn_chowLiu = chow.liu(df, whitelist=wl, blacklist=bl, mi=\"mi\")\n",
    "plot(bn_chowLiu)\n",
    "\n",
    "##-----------------------------------------------\n",
    "## Species interaction networks NB: none of the networks have any edges\n",
    "##-----------------------------------------------\n",
    "\n",
    "# See preprocessing.jl for producing a CSV for a specific country.\n",
    "df = read.csv(\"data/large/DEN_species.csv\")\n",
    "\n",
    "## change data type to factor because bnlearn needs that\n",
    "df[, 1:ncol(df)] <- lapply(df[,1:ncol(df)], as.factor)\n",
    "\n",
    "\n",
    "# HC algorithm (score-based) with restars\n",
    "bn_hc_restart = hc(df, restart=100)\n",
    "plot(bn_hc_restart)\n",
    "# bn_hc_fitted_restart = bn.fit(bn_hc_restart, df)\n",
    "# write.net(\"DEN_bn_hc_restart.net\", bn_hc_fitted_restart)\n",
    "\n",
    "# To plot\n",
    "# Install with: install.packages(\"BiocManager\"); BiocManager::install(\"Rgraphviz\")\n",
    "library(Rgraphviz)\n",
    "pdf(\"plots/network_structure_DEN.pdf\")\n",
    "graphviz.plot(bn_hc_restart)\n",
    "dev.off()\n",
    "\n",
    "## Structure learning with PC algorithm. \n",
    "bn_pc = pc.stable(df)\n",
    "plot(bn_pc)\n",
    "# GS algorithm\n",
    "bn_gs = gs(df)\n",
    "plot(bn_gs)\n",
    "# IAMB algorithm\n",
    "bn_iamb = iamb(df)\n",
    "plot(bn_iamb)\n",
    "# inter-IAMB algorithm\n",
    "bn_interiamb = inter.iamb(df)\n",
    "plot(bn_interiamb)\n",
    "# IAMB-FDR algorithm\n",
    "bn_iambfdr = iamb.fdr(df)\n",
    "plot(bn_iambfdr)\n",
    "# MMPC algorithm\n",
    "bn_mmpc = mmpc(df)\n",
    "plot(bn_mmpc)\n",
    "# SI.HITON-PC algorithm\n",
    "bn_sihitonpc = si.hiton.pc(df)\n",
    "plot(bn_sihitonpc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERROR \n",
    "\n",
    "bn_hc_fitted = bn.fit(bn_hc, df)\n",
    "write.net(\"data/small/bn_hc.net\", bn_hc_fitted)\n",
    "Error in iamb.FDR(df, whitelist = wl, blacklist = bl) : \n",
    "  could not find function \"iamb.FDR\"\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomization Bayes network: finding and comparing paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics maturation age: run simus without and with maturation age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "# Robhoot 1.1.0\n",
    "# Integrate Renku in Robhoot 1.0.0\n",
    "# https://renkulab.io/\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "# Robhoot 1.2.0\n",
    "# XAI: Dynamics with deep learning network \n",
    "# Connect Dynamics to deep learning \n",
    "# EXAMPLE: modeling trait-maturation age to overexploitation\n",
    "\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
